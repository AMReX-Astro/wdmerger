\documentclass[12pt]{book} 

\usepackage[margin=1.0in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{epsfig}

\usepackage{natbib}

% chapter title styles
\usepackage[Sonny]{fncychap}
\ChNameVar{\LARGE}
\ChTitleVar{\LARGE\sl}

% part page style see
% http://tex.stackexchange.com/questions/6609/problems-with-part-labels-using-titlesec
\usepackage{titlesec}

\titleformat{\part}[display]
   {\Huge\filcenter}
   {{\partname{}} \thepart}
   {0em}
   {\hrule}


% hyperlinks -- load after fncychap
\usepackage{hyperref}

% color package
\usepackage[usenames]{color}

% number subsubsections and put them in the TOC
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

% custom hrule for title page
\newcommand{\HRule}{\rule{\linewidth}{0.125mm}}


% short table of contents
\usepackage{shorttoc}

% spacing in the table of contents
\usepackage[titles]{tocloft}

\setlength{\cftbeforechapskip}{2ex}
\setlength{\cftbeforesecskip}{0.25ex}

% don't put a header on blank pages, see
% http://www.latex-community.org/forum/viewtopic.php?f=4&p=51559
% change ``plain'' to ``empty'' to eliminate the page number
\makeatletter
\renewcommand*\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else
\hbox{}
\thispagestyle{empty}
\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother


% don't make the chapter/section headings uppercase.  See the fancyhdr
% documentation (section 9)
\usepackage{fancyhdr}
\renewcommand{\chaptermark}[1]{%
 \markboth{\chaptername
\ \thechapter.\ #1}{}}

\renewcommand{\sectionmark}[1]{\markright{\thesection---#1}}


% skip a bit of space between paragraphs, to enhance readability
\usepackage{parskip}



% special fraction
\newcommand{\sfrac}[2]{\mathchoice
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
   \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
   \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptscriptfont0 #1}\kern-.2em/
   \kern-.15em\lower.25ex\hbox{\the\scriptscriptfont0 #2}}
  {#1\!/#2}}

% codes
\newcommand{\castro}{{\sf Castro}}
\newcommand{\boxlib}{{\sf BoxLib}}
\newcommand{\yt}{{\sf yt}}


%------------------------------------------------------------------------------
\begin{document}

\frontmatter

\begin{titlepage}
\begin{center}
\ \\[3in]
{\sf \Huge WDMERGER} 

\begin{minipage}{5.5in}
\HRule\\[2mm]
\centering
{\Large \em A software package for simulating white dwarf mergers with CASTRO}

\HRule
\end{minipage}

\ \\[1 in]
{\sf \huge User's Guide}

\vfill

{\large \today}
\end{center}

\end{titlepage}


%\shorttoc{Chapter Listing}{0}

%\setcounter{tocdepth}{2}
\tableofcontents

\clearpage

\mainmatter

\chapter{Getting Started}

Welcome to the user's guide for the wdmerger software package. This software is designed to
simulate binary white dwarf systems, and is intended to provide useful information on the 
viability of mergers of white dwarfs as a progenitor for Type Ia supernovae. It is a set of 
source files designed to use the CASTRO code \citep{castro}.

This software requires a few other software packages to be installed before it can be used.
The science software includes \href{https://ccse.lbl.gov/BoxLib/}{BoxLib} and 
\href{https://ccse.lbl.gov/Downloads/downloadCASTRO.html}{CASTRO}. This software is written 
in C++ and Fortran, and compilers for each are needed. The gcc compilers are sufficient.
In addition, it relies on several utilities that are 
commonly distributed on Linux systems: \texttt{bash}, \texttt{GNU bc}, \texttt{GNU make}, 
\texttt{batch}, \texttt{GNU sed}, and \texttt{GNU awk}. Analysis is mainly performed in 
\href{https://www.python.org/}{python}, with the \href{http://matplotlib.org/}{matplotlib} 
library and the \href{http://yt-project.org/}{yt} code being used for visualization. 
If python and yt are not installed on your system, you will still be able to compile and 
run this code, but you will need some other mechanism for analyzing the BoxLib output data.
We have some routines for this purpose available in Fortran and may be able to provide them
upon request.

This software assumes that you have set several environment variables:
\begin{itemize}
  \item \texttt{BOXLIB\_HOME}, the location of the BoxLib code's top-level directory
  \item \texttt{CASTRO\_DIR}, the location of the CASTRO code's top-level directory
  \item \texttt{WDMERGER\_HOME}, the location of this software's top-level directory
\end{itemize}

In addition, you should include in your \texttt{PYTHONPATH} the location of the 
\texttt{analysis} subdirectory. In \texttt{bash}, include the following in your 
\texttt{.bashrc} profile file:

\texttt{export PYTHONPATH=\$PYTHONPATH:\$WDMERGER\_HOME/analysis}

\chapter{Code Structure}

There are several top-level directories in the \texttt{wdmerger} project. Here we will describe
the contents of each. There are also a \texttt{LICENSE.txt} file containing the software license,
and a \texttt{README.md} file describing how to get started.

The \texttt{documentation} directory contains this user's guide and is intended to provide information
on how to use and run this software.

The \texttt{source} directory contains all of the Fortran routines needed to build a CASTRO 
executable file. It is not normally built directly, but you can type \texttt{make} in that directory
to build a vanilla executable built with the default settings.

The \texttt{analysis} directory contains the Python analysis scripts used for analyzing output
data (both in BoxLib and plaintext format). 

The \texttt{tests} directory includes problem setups that are used for testing 
the hydrodynamics code CASTRO and exercises a wide range of its capabilities.

The \texttt{job\_scripts} directory contains utilities that are used for building and running
problem setups on various machine architectures.

The \texttt{papers} directory includes the LaTeX source code for the journal articles
that are being submitted as a result of this project.

\chapter{Test Problems}

The \texttt{tests} directory contains a number of example problems that we use
to assess the reliability of CASTRO in performing hydrodynamics simulations 
with self-gravity. This section includes a list of the existing problems 
as well as the instructions for building and running a problem.

\begin{itemize}
  \item The \texttt{gravity} subdirectory includes problems that test the Poisson 
self-gravity module in CASTRO.
  \begin{itemize}
    \item The \texttt{boundary\_condition\_comparison} test checks the validity of our 
multipole approximation for the boundary conditions on the Poisson equation. It runs
the gravity solve for several different choices of the number of expansion coefficients
and compares the resulting gravitational potential to the exact result.
    \item The \texttt{uniform\_cube\_sphere} test is designed to check the convergence
properties of the Poisson solver. It loads either a sphere or cube of uniform density
onto the computational domain and then calculates the potential at several different
spatial resolutions to check if the potential converges.
  \end{itemize}
  \item The \texttt{hydro} subdirectory tests includes tests that use the hydrodynamics 
module, including many with self-gravity.
  \begin{itemize}
    \item The \texttt{circular\_orbit} test loads two white dwarfs on a grid and then orbits
them for a large number of periods, to test conservation of linear momentum,
angular momentum and energy convergence.
    \item The \texttt{evrard\_collapse} test runs the test problem described originally by 
\cite{evrard:1988} and whose results for a grid-based code can be seen in \cite{arepo}. A spherical
distribution of gas obeying a gamma-law equation of state is loaded onto a grid with negligible 
pressure. The gas collapses, rebounds with a shock, and then settles into hydrodynamic equilibrium.
It is a good test of the energy conservation properties of the code.
    \item The \texttt{freefall} test loads two white dwarfs onto a grid and allows them to fall towards
each other under their mutual gravitational influence. We check to see whether the timescale for them
colliding matches the analytical expectation for two point masses.
    \item The \texttt{kelvin\_helmoltz} test runs several varieties of a 2D shearing fluid that exhibits the 
Kelvin-Helmholtz instability. The relevant papers are \cite{robertson:2010}, \cite{arepo}, and \cite{mcnally:2012}.
    \item The \texttt{single\_star\_hse} test loads a single white dwarf on a grid and checks how well 
hydrostatic equilibrium is maintained.
  \end{itemize}
  \item The \texttt{performance} subdirectory examines the scaling and performance properties of CASTRO.
  \begin{itemize}
    \item The \texttt{strong\_scaling} test runs the same problem with a varying number of processors and 
checks the scaling properties.
    \item The \texttt{weak\_scaling} test scales the size of the problem with the number of processors and
checks the scaling properties.
  \end{itemize}
\end{itemize}

Each test directory is set up with a uniform pattern. The \texttt{source/} subdirectory includes all of the source code
needed to build the test. The \texttt{compile/} directory contains the \texttt{makefile}, which builds the executable and creates
the inputs files needed for the simulation. The \texttt{run\_test.sh} script executes the test by creating a subdirectory
for each parameter set and then submitting it with a batch submission script (for a vanilla Linux installation,
this will use the \texttt{batch} utility; for a limited number of clusters and supercomputers, 
there will be recommended options and specialized submission scripts). The \texttt{analysis.py} Python script
is used to analyze the output and generature figures. In summary, to run a test, perform the following series of commands:
\begin{itemize}
  \item \texttt{cd compile}
  \item \texttt{make}
  \item \texttt{cd ..}
  \item \texttt{bash run\_test.sh}
  \item \texttt{python analysis.py} (after the execution has completed)
\end{itemize}



\chapter{System Configuration}

Running the wdmerger software will require a little setup in terms of configuring the 
scripts to work with the system you are currently using. The software is currently actively 
maintained only for a small number of supercomputing resources (Blue Waters at NCSA 
and Titan at OLCF, in particular). If you are using one of these machines, then 
modifying the code for your purposes is straightforward. If you are not, you'll need to do a 
small bit of ground work to get everything set up, but once you do the automated scripts 
will handle everything else for you. Feel free to contact us for advice on how to do so 
(see Section \ref{sec:contributors} for contact information), but be aware that it will be 
hard for us to debug on systems that we do not have access to. 

It is important to note that although we have attempted to 
make sure that this software builds and runs on a generic Linux workstation, most of the test 
problems are simply too large to run on anything smaller than a medium-sized cluster. To get a flavor 
of what is going on, though, you can comment out the larger runs from the various loops in the 
\texttt{run\_test.sh} file and then run the smaller problem setup.

\section{Account Details and System Options}

All of the machine-dependent functionality exists in the file \texttt{job\_scripts/run\_utils.sh}.
In particular, near the end of the file you can find a bash conditional that tests on the name of 
certain machines. Each machine definition includes a number of variables that are needed to 
properly configure all of the helper functions. If you are using a system already known to us,
then the only thing you need to change is the \text{allocation} variable: this is the account 
that jobs will be charged to on the machine. 

If you are establishing a new machine, then there are a few steps you will need to take. First, 
add a section to this conditional that tests on what you want to call the name of the machine. 
Then, make entries for the necessary variables:
\begin{itemize}
  \item \texttt{allocation}: the account jobs are charged to.
  \item \texttt{exec}: the function used to submit batch scripts. For PBS-style submission 
systems such as TORQUE, this will be \texttt{qsub}.
  \item \texttt{ppn}: the number of processors per node on the system. Be careful with how 
you define this. Many systems have nodes that have two integer cores for every floating point
unit and will report the number of integer cores as the number of processors per node, 
and for floating point-intensive applications like hydrodynamics adding as many threads/MPI tasks
as there are integer cores may actually slow you down. You will have to test to find 
the best setting for you.
  \item \texttt{run\_ext}: the file extension of the standard output file when a job is in operation. 
This is used by the run scripts to ensure that we don't submit a job in a directory where one is 
already running. For many systems this will be ``.OU''.
  \item \texttt{batch\_system}: the job submission system used on the machine. For many supercomputers 
``PBS'' will be what you should insert here, which is a catchall for any derivative such as TORQUE 
that uses normal PBS options for accepting jobs.
  \item \texttt{archive\_method} (optional): the method used by the machine for saving files to 
long-term storage. See Section \ref{sec:archiving}. This is optional, and if you don't set it then
the scripts won't attempt to copy your data off the machine.
\end{itemize}
When you submit a job through a \texttt{run\_test.sh} file using the \texttt{run} command, 
a batch script will be automatically created for you. For PBS systems you should be mostly 
good to go as-is, but some systems vary particularly in the \texttt{-l nodes} option, so 
you should make sure that the job script construction is in sync with how batch jobs 
on that machine should appear. Locate the \texttt{create\_job\_script} function and adjust it 
accordingly by changing the relevant \texttt{echo} statements. If you are using a different submission
system, create a new section of the conditional in that function, and model it off what we do for PBS
so that the relevant information is output to the newly-created job script (called \texttt{run\_script} 
by default; you can change this in the definitions section as well).

The last step is to tell the script how to know that you're using the machine you just defined. 
Locate the \texttt{get\_machine} command and use the output of \texttt{uname -n} to uniquely determine 
which system you are using.

\section{Archiving}
\label{sec:archiving}

This software has some capability to automatically handle archiving of data to long-term storage,
at least for systems we already know. Each time we run a job and we are continuing from an old checkpoint,
we move all new plotfiles and checkpoints to the \texttt{output/} subdirectory of the job submission 
location. We also make copies of the diagnostics files, and the standard output files. By default this is 
all we do. But on systems where archiving is supported, we copy these files to the long-term storage.

There are currently two methods we know about: \texttt{htar} for Titan, and Globus Online for Blue Waters.
\texttt{htar} is a command that both creates a tar file and then copies it to the high-performance storage system 
(HPSS) using the \texttt{hsi} command. It works on a file-by-file basis, so we loop over all of the files to 
archive and run \texttt{htar} on them individually. If your system uses \texttt{htar}, then you should be all set.

\href{https://www.globus.org/}{Globus Online} is an internet service that handles data transfer requests 
between certain known endpoints. This is the choice used by Blue Waters for copying and retrieving data
to their storage system. You will need to set up your account and authenticate through Globus before using it.
Once you know your Globus username, you can adjust the \texttt{globus\_username} in \texttt{run\_utils.sh}. 
We note that the Globus archive strategy is different. There is a cap on the number of simultaneous transfers 
one can have, so submitting transfers for every plotfile and checkpoint will not work. Instead, when we 
run the archiving script, we first move all the files to the output directory, and then we do a transfer 
on the entire \texttt{output/} subdirectory. Globus will automatically sync the source and destination 
directories, so no extra work is required of us. We choose the \texttt{sync\_level = 2} option, which 
overwrites existing files if the source has a newer date than the destination. This allows us to 
continually update the diagnostic files, which always have the same name.

If your system uses a different archive strategy, you will need to update the \texttt{archive\_all} 
and \texttt{archive} functions.

\chapter{Contributors}
\label{sec:contributors}

This project was started as the Ph.D. dissertation project for Max Katz at Stony Brook University. 
Michael Zingale is Max's primary thesis advisor. Correspondence and requests for information 
about this software should be directed to them; their e-mail addresses are maximilian.katz@stonybrook.edu
and michael.zingale@stonybrook.edu, respectively. Alan Calder and Doug Swesty at Stony Brook are 
also co-investigators on this project. The CASTRO code was developed primarily at the 
Lawrence Berkeley National Laboratory. Staff there that have contributed directly and 
indirectly to this project include Ann Almgren, John Bell, Weiqun Zhang, Andy Nonaka, and Vince Beckner.

We also gratefully acknowledge the assistance of Noel Scudder and Platon Karpov, 
Stony Brook University undergraduates who helped with running simulations and 
developing the visualization software based on yt. Adam Jacobs at Stony Brook also provided 
assistance in various aspects of deploying our codes on supercomputing resources.

%------------------------------------------------------------------------------
\backmatter

\bibliographystyle{../papers/apj}
\bibliography{../papers/refs}

\end{document}
