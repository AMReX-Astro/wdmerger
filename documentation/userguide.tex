\documentclass[12pt]{book} 

\usepackage[margin=1.0in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{epsfig}

\usepackage{natbib}

\usepackage[usenames]{color}

% For inserting code snippets;
% use the lstlisting environment to access.
% emacs doesn't parse $'s in code sections 
% propertly, so that is why I have sprinkled 
% some %$'s through the code. The coloring 
% on the environment itself is still broken though.
\usepackage{listings}

\definecolor{gray}{rgb}       {0.8,0.8,0.8}
\definecolor{light-blue}{rgb} {0.8,0.8,1.0}
\definecolor{light-green}{rgb}{0.8,1.0,0.8}
\definecolor{light-red}{rgb}  {1.0,0.9,0.9}

\lstset{
  basicstyle=\small\ttfamily,
  frame=shadowbox, 
  rulesepcolor=\color{gray},                                                             
  backgroundcolor=\color{white}
}

% chapter title styles
\usepackage[Sonny]{fncychap}
\ChNameVar{\LARGE}
\ChTitleVar{\LARGE\sl}

% part page style see
% http://tex.stackexchange.com/questions/6609/problems-with-part-labels-using-titlesec
\usepackage{titlesec}

\titleformat{\part}[display]
   {\Huge\filcenter}
   {{\partname{}} \thepart}
   {0em}
   {\hrule}


% hyperlinks -- load after fncychap
\usepackage{hyperref}

% color package
\usepackage[usenames]{color}

% number subsubsections and put them in the TOC
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

% custom hrule for title page
\newcommand{\HRule}{\rule{\linewidth}{0.125mm}}


% short table of contents
\usepackage{shorttoc}

% spacing in the table of contents
\usepackage[titles]{tocloft}

\setlength{\cftbeforechapskip}{2ex}
\setlength{\cftbeforesecskip}{0.25ex}



% don't put a header on blank pages, see
% http://www.latex-community.org/forum/viewtopic.php?f=4&p=51559
% change ``plain'' to ``empty'' to eliminate the page number
\makeatletter
\renewcommand*\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else
\hbox{}
\thispagestyle{empty}
\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother


% don't make the chapter/section headings uppercase.  See the fancyhdr
% documentation (section 9)
\usepackage{fancyhdr}
\renewcommand{\chaptermark}[1]{%
 \markboth{\chaptername
\ \thechapter.\ #1}{}}

\renewcommand{\sectionmark}[1]{\markright{\thesection---#1}}


% skip a bit of space between paragraphs, to enhance readability
\usepackage{parskip}



% special fraction
\newcommand{\sfrac}[2]{\mathchoice
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
   \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptfont0 #1}\kern-.15em/
   \kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}
  {\kern0em\raise.5ex\hbox{\the\scriptscriptfont0 #1}\kern-.2em/
   \kern-.15em\lower.25ex\hbox{\the\scriptscriptfont0 #2}}
  {#1\!/#2}}

% codes
\newcommand{\castro}{{\sf Castro}}
\newcommand{\amrex}{{\sf AMReX}}
\newcommand{\yt}{{\sf yt}}


%------------------------------------------------------------------------------
\begin{document}

% For interpreting the journal names in the bibilography; this is copied and 
% amended from emulateapj.cls.

\newcommand\aj{AJ}%        % Astronomical Journal 
\newcommand\araa{ARA\&A}%  % Annual Review of Astron and Astrophys 
\newcommand\apj{ApJ}%    % Astrophysical Journal 
\newcommand\apjl{ApJ}%     % Astrophysical Journal, Letters 
\newcommand\apjs{ApJS}%    % Astrophysical Journal, Supplement 
\newcommand\ao{Appl.~Opt.}%   % Applied Optics 
\newcommand\apss{Ap\&SS}%  % Astrophysics and Space Science 
\newcommand\aap{A\&A}%     % Astronomy and Astrophysics 
\newcommand\aapr{A\&A~Rev.}%  % Astronomy and Astrophysics Reviews 
\newcommand\aaps{A\&AS}%    % Astronomy and Astrophysics, Supplement 
\newcommand\azh{AZh}%       % Astronomicheskii Zhurnal 
\newcommand\baas{BAAS}%     % Bulletin of the AAS 
\newcommand\icarus{Icarus}% % Icarus
\newcommand\jrasc{JRASC}%   % Journal of the RAS of Canada 
\newcommand\memras{MmRAS}%  % Memoirs of the RAS 
\newcommand\mnras{MNRAS}%   % Monthly Notices of the RAS 
\newcommand\pra{Phys.~Rev.~A}% % Physical Review A: General Physics 
\newcommand\prb{Phys.~Rev.~B}% % Physical Review B: Solid State 
\newcommand\prc{Phys.~Rev.~C}% % Physical Review C 
\newcommand\prd{Phys.~Rev.~D}% % Physical Review D 
\newcommand\pre{Phys.~Rev.~E}% % Physical Review E 
\newcommand\prl{Phys.~Rev.~Lett.}% % Physical Review Letters 
\newcommand\pasp{PASP}%     % Publications of the ASP 
\newcommand\pasj{PASJ}%     % Publications of the ASJ 
\newcommand\qjras{QJRAS}%   % Quarterly Journal of the RAS 
\newcommand\skytel{S\&T}%   % Sky and Telescope 
\newcommand\solphys{Sol.~Phys.}% % Solar Physics 
\newcommand\sovast{Soviet~Ast.}% % Soviet Astronomy 
\newcommand\ssr{Space~Sci.~Rev.}% % Space Science Reviews 
\newcommand\zap{ZAp}%       % Zeitschrift fuer Astrophysik 
\newcommand\nat{Nature}%  % Nature 
\newcommand\iaucirc{IAU~Circ.}% % IAU Cirulars 
\newcommand\aplett{Astrophys.~Lett.}%  % Astrophysics Letters 
\newcommand\apspr{Astrophys.~Space~Phys.~Res.}% % Astrophysics Space Physics Research 
\newcommand\bain{Bull.~Astron.~Inst.~Netherlands}% % Bulletin Astronomical Institute of the Netherlands 
\newcommand\fcp{Fund.~Cosmic~Phys.}%   % Fundamental Cosmic Physics 
\newcommand\gca{Geochim.~Cosmochim.~Acta}% % Geochimica Cosmochimica Acta 
\newcommand\grl{Geophys.~Res.~Lett.}%  % Geophysics Research Letters 
\newcommand\jcp{J.~Chem.~Phys.}%     % Journal of Chemical Physics 
\newcommand\jgr{J.~Geophys.~Res.}%     % Journal of Geophysics Research 
\newcommand\jqsrt{J.~Quant.~Spec.~Radiat.~Transf.}%   % Journal of Quantitiative Spectroscopy and Radiative Trasfer 
\newcommand\memsai{Mem.~Soc.~Astron.~Italiana}% % Mem. Societa Astronomica Italiana 
\newcommand\nphysa{Nucl.~Phys.~A}%     % Nuclear Physics A 
\newcommand\physrep{Phys.~Rep.}%       % Physics Reports 
\newcommand\physscr{Phys.~Scr}%        % Physica Scripta 
\newcommand\planss{Planet.~Space~Sci.}%  % Planetary Space Science 
\newcommand\procspie{Proc.~SPIE}%      % Proceedings of the SPIE 

\newcommand\actaa{Acta Astron.}%  % Acta Astronomica
\newcommand\caa{Chinese Astron. Astrophys.}%  % Chinese Astronomy and Astrophysics
\newcommand\cjaa{Chinese J. Astron. Astrophys.}%  % Chinese Journal of Astronomy and Astrophysics
\newcommand\jcap{J. Cosmology Astropart. Phys.}%  % Journal of Cosmology and Astroparticle Physics
\newcommand\na{New A}%  % New Astronomy
\newcommand\nar{New A Rev.}%  % New Astronomy Review
\newcommand\pasa{PASA}%  % Publications of the Astron. Soc. of Australia
\newcommand\rmxaa{Rev. Mexicana Astron. Astrofis.}%  % Revista Mexicana de Astronomia y Astrofisica


\frontmatter

\begin{titlepage}
\begin{center}
\ \\[3in]
{\sf \Huge WDMERGER} 

\begin{minipage}{5.5in}
\HRule\\[2mm]
\centering
{\Large \em A software package for simulating white dwarf mergers with CASTRO}

\HRule
\end{minipage}

\ \\[1 in]
{\sf \huge User's Guide}

\vfill

{\large \today}
\end{center}

\end{titlepage}


%\shorttoc{Chapter Listing}{0}

%\setcounter{tocdepth}{2}
\tableofcontents

\mainmatter

\chapter{Getting Started}

Welcome to the user's guide for the \texttt{wdmerger} software package. This software is designed to
simulate binary white dwarf systems, and is intended to provide useful information on the 
viability of mergers of white dwarfs as a progenitor for Type Ia supernovae. It is a set of 
driver routines and analysis scripts designed for use with the \castro\ code \citep{castro}.

This software requires a few other software packages to be installed before it can be used.
The science software includes \href{https://github.com/AMReX-Codes/amrex}{AMReX} and 
\href{https://github.com/AMReX-Astro/Castro}{CASTRO}. \castro\ will also need
the \href{https://github.com/BoxLib-Codes/Microphysics}{Microphysics} repository, 
which stores the equation of state and nuclear network we will use. Instructions for
building and installing \castro\ can be found in its documentation.
This package relies on several utilities that are
commonly distributed on Linux systems: \texttt{bash}, \texttt{GNU bc}, \texttt{GNU make}, 
\texttt{batch}, \texttt{GNU sed}, and \texttt{GNU awk}. Analysis is mainly performed in 
\href{https://www.python.org/}{python}, with the \href{http://matplotlib.org/}{matplotlib} 
library and the \href{http://yt-project.org/}{yt} code being used for visualization. 
If python and yt are not installed on your system, you will still be able to compile and 
run this code, but we use yt for all analysis of \amrex\ output data other than 
the ASCII files that are generated at runtime. We have some routines for this purpose 
available in Fortran and Python; some of the Fortran tools are provided in \amrex\ 
itself, in the \texttt{Tools/Postprocessing} subdirectory, and it is straightforward enough 
to modify them for basic analysis. We may be able to provide others upon request.

This software assumes that you have set several environment variables:
\begin{itemize}
  \item \texttt{AMREX\_HOME}, the location of the \amrex\ code's top-level directory
  \item \texttt{CASTRO\_DIR}, the location of the \castro\ code's top-level directory
  \item \texttt{WDMERGER\_HOME}, the location of this software's top-level directory
  \item \texttt{MICROPHYSICS\_DIR}, the location of the \texttt{Microphysics} code's top-level directory
\end{itemize}

In addition, you should include in your \texttt{PYTHONPATH} the location of the 
\texttt{analysis} subdirectory. In \texttt{bash}, include the following in your 
\texttt{.bashrc} profile file:

\texttt{export PYTHONPATH=\$PYTHONPATH:\$WDMERGER\_HOME/analysis}

A note on machine compatibility: this software is currently only developed and tested on
machines that run Linux and come with the common GNU flavors of most pieces of software
like \texttt{sed}. We do not guarantee that this software will work out of the box on OS X
or other Unix derivatives; however, we generally try to avoid GNU-specific options, so that 
trying to run this software on OS X will not be too painful. With that being said, this 
software does heavily rely on \texttt{bash} and at present there are no plans to maintain
compatibility with any other shell.

\chapter{Directory Layout}

There are several top-level directories in the \texttt{wdmerger} project. Here we will describe
the contents of each. There are also a \texttt{LICENSE.txt} file containing the software license,
and a \texttt{README.md} file describing how to get started.

The \texttt{documentation} directory contains this user's guide and is intended to provide information
on how to use and run this software.

The \texttt{analysis} directory contains the Python analysis scripts used for analyzing output
data (both in \amrex\ and plain-text format). 

The \texttt{science} directory includes problem setups that are or have been used in the papers
that have been or will be submitted based on this software.

The \texttt{job\_scripts} directory contains utilities that are used for running
problem setups on various machine architectures.

The \texttt{papers} directory includes the LaTeX source code for the journal articles
that are being submitted as a result of this project.



\chapter{Merger Problem}

This chapter describes the setup of the white dwarf merger problem. It includes descriptions of 
the physics involved, the code options and runtime parameters, the diagnostic output 
and other I/O information, and the analysis routines. A full description of the physics 
and the associated numerical algorithms is included in the \castro\ code paper \cite{castro}.

\section{Physical Description}

We model two white dwarfs in a rotating binary system. The white dwarfs are composed of 
a mixture of elements that is principally carbon and oxygen, and are individually supported 
against gravity by degenerate electron material at non-zero temperature. This matter is described well 
for arbitrary degeneracy and relativity levels by the Helmholtz equation of state \cite{timmes_swesty:2000}.
It includes ideal gas contributions from the ions, a simple blackbody recipe for the contribution from 
the radiation field, and optionally Coulomb corrections. 

\castro\ includes full self-gravity with the Poisson equation, as well as the ability to perform a
simulation in a rotating reference frame with fixed rotation period. By default the $z$ axis will 
be the rotation axis, with the stars initially aligned along the $x$ axis. We select the 
orbital period of the system, and the masses of the \textit{primary} (more massive) and 
\textit{secondary} (less massive) white dwarfs. We then use Kepler's third law to determine the 
appropriate initial orbital distance. The center of mass of the system corresponds with the center 
of the domain, which is the origin; the domain is $1.024 \times 10^{10}\ \text{cm}$ wide in each 
coordinate direction, so that each dimension runs from $-5.12 \times 10^{9}\ \text{cm}$ to 
$+5.12 \times 10^{9}\ \text{cm}$. In a rotating reference frame, we set the stars at rest. 
We can instead run a simulation in the inertial reference frame. To make it equivalent, we
start all zones in the domain with a velocity corresponding to the equivalent rotating 
reference frame velocity. This means that the stars will be synchronously rotating (though we
may relax this assumption in future work).

Though we are mainly interested in 3D simulations, it is sometimes useful to compare to 2D
simulations which are much cheaper and can be run at higher resolutions. The only physically
meaningful analogue to these mergers in lower dimensions occurs in 2D axisymmetric cylindrical
coordinates. CASTRO represents these as $(r,z,\theta)$. In this coordinate system we enforce
that the stars lie along the $z$ axis and that the center of the stars lies at $r = 0$. The
$\theta$ dimension doesn't get directly evolved hydrodynamically but that is the dimension
along which the rotational motion occurs, so in some circumstances the velocity in that
direction will change with time because we still allow the $\theta$-velocity to participate in
the Coriolis force. When rotation is enabled, the rotation is along the $z = 0$ axis in the $r$
coordinate dimension. To try this out you should use \texttt{DIM = 2} in the makefile and
use \texttt{inputs\_2d} in the \texttt{wdmerger} directory in \castro.

For the nuclear network we use by default the \texttt{aprox13} network provided by Frank Timmes, which 
we have ported for use in the \texttt{Microphysics} repository. It is an alpha-chain network 
with isotopes ranging from ${}^4$He to ${}^{56}$Ni. We have also ported \texttt{aprox19} and
\texttt{aprox21}, which include the alpha chain isotopes as well as other nuclides for hydrogen
burning and more accurate iron-group calculations. It is also possible to use the \texttt{iso7}
network, which has the alpha nuclides from helium to silicon as well as nickel. The energy generation
rate will be less accurate but at least should give some qualitatively correct answers. You can also
provide your own network following the instructions in the Microphysics repository, as long as it
contains at least ${}^4$He, ${}^{12}$C, ${}^{16}$O, ${}^{20}$Ne, and ${}^{24}$Mg. This the minimum
set of isotopes needed to construct the initial white dwarf models.

\section{Runtime Parameters}

There are a few \castro\ input parameters that you will typically want to control when running problems. 
Among the most important is whether we are doing rotation (\texttt{castro.do\_rotation}) and if so, 
what the rotational period is (\texttt{castro.rotational\_period}). (Note that the rotational period 
is used to set the orbital properties even if we're not rotating, so you should always set this appropriately.)
You'll also want to change the \texttt{stop\_time}, which is when the simulation will terminate. 
You should also avoid flooding yourself with plotfiles and checkpoints by setting \texttt{amr.plot\_files\_output} and 
\texttt{amr.checkpoint\_files\_output}, respectively; these are the simulation time differences
between successive plotfiles and checkpoints. Note that \castro\ does not adjust the timestep 
to meet this criterion, so it will generate these at the first timestep that passes this interval.

The \texttt{wdmerger} parameters of interest are all controlled through the \texttt{probin} file. 
These control the layout of the stars on the grid, the tagging options for refinement, and some other things. 
A \texttt{probin} file is composed of three Fortran namelists; a namelist is enclosed in 
\texttt{\&namelist ... /}. The \texttt{fortin} namelist controls the main system parameters. 
In the following list we describe the parameters; you can see their current defaults by 
examining \texttt{probin}. (Some parameters in there are currently beta quality and 
not recommended for use, and for a couple others there will be no practical reason to change. 
Stick to the ones in this list for now.)
\begin{itemize}
  \item \texttt{mass\_P}: mass of the primary white dwarf, in solar masses (real)
  \item \texttt{mass\_S}: mass of the secondary white dwarf, in solar masses (real; if less than zero, 
    then no secondary star will be generated, and the primary star will be loaded onto the grid at the 
    center of the domain)
  \item \texttt{central\_density\_P}: alternatively, we can specify the central density of the primary white dwarf instead of the mass (real)
  \item \texttt{central\_density\_S}: alternatively, we can specify the central density of the secondary white dwarf instead of the mass (real)
  \item \texttt{nsub}: number of sub-intervals to divide a zone into when interpolating from the initial model (integer)
  \item \texttt{no\_orbital\_kick}: whether or not to kick the stars with an orbital velocity corresponding to their 
    Keplerian rotation rate (logical; this only makes sense in the inertial reference frame with \texttt{castro.do\_rotation = 0})
  \item \texttt{collision}: whether to set the stars moving towards each other at free-fall speed for a collision (logical)
  \item \texttt{collision\_separation}: if we're doing a collision, number of WD radii to set the stars apart from each other initially (real)
  \item \texttt{damping}: whether or not to exert a constant damping force on the orbit (logical)
  \item \texttt{damping\_alpha}: strength of damping force (real)
  \item \texttt{bulk\_velx}: uniform velocity to give to all gas in the domain in the $x$-direction (real)
  \item \texttt{bulk\_vely}: uniform velocity to give to all gas in the domain in the $y$-direction (real)
  \item \texttt{bulk\_velz}: uniform velocity to give to all gas in the domain in the $z$-direction (real)
  \item \texttt{interp\_temp}: whether or not to take temperature as given when interpolating from our initial model (logical;
    if false, we will interpolate based on pressure)
  \item \texttt{ambient\_density}: density of the rarefied ambient medium outside the stars (real)
  \item \texttt{stellar\_temp}: initial temperature of the stars (and also ambient medium) (real)
  \item \texttt{stellar\_C12, stellar\_O16}: mass fractions of ${}^{12}$C and ${}^{16}$O in the stars (real)
  \item \texttt{star\_axis}: what coordinate axis the stars will be lined up along initially (integer; $x$ = 1, $y$ = 2, $z$ = 3)
  \item \texttt{maxTaggingRadius}: fractional size of the domain outside of which we won't perform any refinement (real)
  \item \texttt{sponge\_dist}: we can apply a sponge that damps the velocities starting at this fraction multiplied by the 
    distance from the center to the domain boundary (real)
  \item \texttt{sponge\_width}: smoothing width of the sponge; the sponge is fully applied at {\tt sponge\_dist} + {\tt sponge\_width},
    in the same coordinates (real)
  \item \texttt{sponge\_timescale}: the sponge decreases the velocity by a factor $1 / (1 + (\Delta t / t_s)$, where 
    $t_s$ is the {\tt sponge\_timescale} in seconds (real)
  \item \texttt{gw\_dist}: for the gravitational wave signals we calculate, this is the distance in kiloparsecs from the observer to the binary system (real)
  \item \texttt{fill\_ambient\_bc}: whether we should set ghost cells on the domain boundary to have the ambient state properties (logical)
\end{itemize}

The \texttt{tagging} namelist is described in the AMR chapter of the \castro\ user's guide. At present, we 
only tag based on absolute values of the density and temperature.

The \texttt{extern} namelist contains parameters that are used by the microphysics modules. In particular, 
the equation of state usually relies on parameters passed in this way. For the Helmholtz EOS, the relevant 
parameters are \texttt{eos\_input\_is\_constant}, which determines whether or not we update the internal 
energy on an EOS call, and \texttt{use\_eos\_coulomb}, which determines whether we include Coulomb 
interactions in the EOS evaluation.



\section{Diagnostic Output}

\texttt{wdmerger} provides two kinds of diagnostic output at runtime. One set keeps track of quantities affecting 
the entire grid (such as the total amount of mass and energy on the grid); the other tabulates properties 
of the individual stars. The former is, by default, printed to \texttt{grid\_diag.out} and the latter to 
\texttt{star\_diag.out}. This output can be used to keep track of how the run is going, but more often is 
saved for the purposes of plotting later on (since it may be too expensive to write out enough plotfiles to 
generate these types of curves at the end of the run).

The routines that generate both of these are found in \\\texttt{sum\_integrated\_quantities.cpp}. 
This is a replacement of the \castro\ function that does something similar. We use the same 
conventions; for example, if summing over the whole grid and we encounter a region that is covered by 
a fine grid, we zero out the contribution from the coarse grid and only count the fine grid quantities 
in that location.

\subsection{Grid Quantities}

The following are the quantities that involve operations over the entire grid, as well as the continuous 
integral representation of them. Discretization is typically done by setting $\int dV \rightarrow \Delta V$ and 
assuming cell-centered values for the integrands.
\begin{itemize}
  \item \texttt{TOTAL ENERGY}: Total amount of energy stored in the system.
  \begin{equation}
    \int dV\, \left(-\frac{1}{2}\rho \phi + \rho E + \rho \Phi_{\text{rot}}\right)
  \end{equation}

  \item \texttt{TOTAL E GRID}: Total amount of energy represented on the computational grid; differs from
    \texttt{total\_energy} only when rotation is enabled.
  \begin{equation}
    \int dV\, \left(\frac{1}{2}\rho \phi + \rho E\right)
  \end{equation}

  \item \texttt{GAS ENERGY}: Kinetic plus internal energy.
  \begin{equation}
    \int dV\, \rho E
  \end{equation}

  \item \texttt{KIN. ENERGY}: Kinetic energy (calculated using velocities; in general will differ from $\rho(E - e)$).
  \begin{equation}
    \int dV\, \frac{1}{2} \rho \mathbf{v}^2
  \end{equation}

  \item \texttt{ROT. ENERGY}: Energy associated with rotating reference frame.
  \begin{equation}
    \int dV\, \rho\, {\bm \omega} \cdot (\mathbf{r} \times \mathbf{p}) + \int dV\, \frac{1}{2} {\bm \omega}^T \cdot \mathbf{I} \cdot {\bm \omega}
  \end{equation}

  \item \texttt{GRAV. ENERGY}: Gravitational potential energy ($\phi < 0$).
  \begin{equation}
    \int dV\, \frac{1}{2}\rho\phi
  \end{equation}

  \item \texttt{INT. ENERGY}: Internal/thermal energy of the gas.
  \begin{equation}
    \int dV\, \rho e
  \end{equation}

  \item \texttt{XMOM, YMOM, ZMOM}: Total momentum.
  \begin{equation}
    \int dV\, \rho (\mathbf{u} + {\bm \omega} \times \mathbf{r})
  \end{equation}

  \item \texttt{XMOM GRID, YMOM GRID, ZMOM GRID}: Momentum on grid only; no rotation.
  \begin{equation}
    \int dV\, \rho \mathbf{u}
  \end{equation}

  \item \texttt{XMOM ROT., YMOM ROT., ZMOM ROT.}: Momentum due only to rotation.
  \begin{equation}
    \int dV\, \rho {\bm \omega} \times \mathbf{r}
  \end{equation}

  \item \texttt{ANG. MOM. X, ANG. MOM. Y, ANG. MOM. Z}: Total angular momentum.
  \begin{equation}
    \int dV\, \rho(\mathbf{r} \times \mathbf{u} + \int dV\, \mathbf{I} \cdot {\bm \omega}
  \end{equation}

  \item \texttt{ANG. MOM. X GRID, ANG. MOM. Y GRID, ANG. MOM. Z GRID}: Angular momentum on grid only; no rotation.
  \begin{equation}
    \int dV\, \rho\mathbf{r} \times \mathbf{u}
  \end{equation}

  \item \texttt{ANG. MOM. X ROT., ANG. MOM. Y ROT., ANG. MOM. Z ROT.}: Angular momentum due only to rotation.
  \begin{equation}
    \int dV\, \mathbf{I} \cdot {\bm \omega}
  \end{equation}

  \item \texttt{MASS}: Mass.
  \begin{equation}
    \int dV\, \rho
  \end{equation}

  \item \texttt{X COM, Y COM, Z COM}: Center of mass.
  \begin{equation}
    \int dV\, \rho \mathbf{x}
  \end{equation}

  \item \texttt{X COM VEL, Y COM VEL, Z COM VEL}: Velocity of center of mass.
  \begin{equation}
    \int dV\, \rho \mathbf{v}
  \end{equation}

  \item \texttt{h\_+, h\_x (rotation axis, star axis, motion axis)}: Gravitational wave strain in ``plus'' and ``cross'' polarization modes, observed along the rotation axis, the axis the stars were originally aligned along, and the axis the stars originally moved on. (By default, there are $z$, $x$, and $y$ respectively). See \texttt{wdmerger} paper I for details on how these are calculated.



  \subsection{Star Quantities}

  Now we consider the quantities describing the stars. First we must describe our strategy for determining what parts of the domain correspond to the primary and what parts correspond to the secondary. At the beginning of the simulation, we establish the binary so that each star's center of mass lies on the \texttt{star\_axis} and is zero in the other directions. At each subsequent timestep, we make a guess at the location of the new center of mass: $\mathbf{x}_{\text{COM}} \to \mathbf{x}_{\text{COM}} + \mathbf{v}_{\text{COM}} + \Delta t$. Here $\mathbf{v}_{\text{COM}}$ is the velocity of the center of mass at the last timestep (which at the beginning of the simulation is simply the Keplerian orbital speed). Then we refine this estimate by doing an integral on the domain. We define $\mathbf{x}^\prime_P$ and $\mathbf{x}^\prime_S$ as the tentative locations, and then do a center-of-mass calculation on all zones within the Roche radius $r_L$ of that star, which encompasses the effective gravitational domain of that star. We use the commonly employed definition of the Roche radius of \citet{eggleton:1983},
  \begin{equation}
    \frac{r_L}{a} = \frac{0.49 q^{2/3}}{0.6^{2/3} + \text{ln}(1 + q^{1/3})}.
  \end{equation}
  Here $a$ is the orbital separation (i.e. $\left|\mathbf{x}^\prime_P - \mathbf{x}^\prime_S\right|$ ), and if $q = M_S / M_P$ is the mass ratio then this yields the Roche radius of the secondary. $q$ is inverted to get the Roche radius of the primary. Once we do this calculation, we have a new $\mathbf{x}_P$ and $\mathbf{x}_S$, as well as $\mathbf{v}_P$ and $\mathbf{v}_S$ and $M_P$ and $M_S$. We also then define the distance between the stars as $|\mathbf{x}_P - \mathbf{x}_S|$. We also calculate the angle that the line joining them makes with respect to the original axis; this is a measure of how much they have rotated. With the center of masses located, we can add any diagnostics of interest we want related to the stars. At present we only have one. The \texttt{PRIMARY 1E$n$ RADIUS} and \texttt{SECONDARY 1E$n$ RADIUS} are the radii from the center of the stars at which the average density is $10^n\ \text{g}\ \text{cm}^{-3}$.

\end{itemize}



\chapter{How to Build a Problem}

This chapter describes the necessary steps you need to take to build your own problem setup,
and describes the underlying code that facilitates the job submission. If you are interested in
doing white dwarf mergers and want to use the default source code, read Section \ref{sec:standard_problems}.
If you are interested in setting up a different problem but using this software's job submission 
system, you can do so; Section \ref{sec:non-standard_problems} gives some advice on that.

\section{Standard Problems}
\label{sec:standard_problems}

At a minimum, your working directory needs a \texttt{compile/} subdirectory with a \texttt{makefile}
in it, and some \texttt{bash} script that you run to submit the jobs. If you want to build 
with the default settings, then your makefile needs only to have the line:
\begin{lstlisting}
  include $(WDMERGER_HOME)/source/GNUmakefile
\end{lstlisting}%$
This includes the main makefile from the \texttt{source/} directory in \texttt{wdmerger}. 
The main makefile sets all of our build options (compiler, number of space dimensions, 
equation of state, etc.) and then includes the \castro\ makefile, which then runs as normal.
If you want to adjust the build options, then simply add a line before this include. As an 
example, if you want to build using a gamma-law equation of state, set:
\begin{lstlisting}
  EOS_dir = gamma_law_general/

  include $(WDMERGER_HOME)/source/GNUmakefile 
\end{lstlisting}%$

The \texttt{make} variables in the main makefile are all set conditionally, so as long 
as you set your variable before including the main makefile, it will be built as you expect.

Once your makefile is setup, you can simply type \texttt{make} (or \texttt{make -j} 
to use more processors and speed up the operation) at the command line. 
If you're using Cray compilers on a supercomputer this will take several minutes;
with GNU compilers it should be quick. When everything is compiled, if it was
successful you should have an executable that looks like \texttt{Castro3d.Linux.Cray.Cray.MPI.ex}
in the \texttt{compile} directory.

Now \texttt{cd} up one level, and use your favorite text editor to create a simple 
\texttt{bash} script, which for example purposes we will call \texttt{run.sh}.
The first thing we should do is tell the script where to look to pull in 
all the helper functions we'll need to execute a job:

\begin{lstlisting}
source $WDMERGER_HOME/job_scripts/run_utils.sh
\end{lstlisting}%$

Decide what directory you want to build the problem in. It is expected 
to be a subdirectory of the \texttt{results/} directory, which will be
automatically created for you:
\begin{lstlisting}
dir=results/test_problem/
\end{lstlisting}
Now, optionally decide how many processors you want to run with, and 
how long you want the job to run:
\begin{lstlisting}
nprocs=128
walltime=4:00:00
\end{lstlisting}
If you do not, the job submission script will default to running the job
using one node for one hour. Finally, run your job:
\begin{lstlisting}
run $dir $nprocs $walltime
\end{lstlisting}%$
That is all you need to submit a job. Return to the terminal, and type \texttt{bash run.sh}
and your job will execute. The helper function \texttt{run} does several things. First, 
it creates the directory named \texttt{\$dir}. Then, it copies into \texttt{\$dir} the \texttt{inputs}
and \texttt{probin} files from the root \texttt{wdmerger} source directory; these are the runtime
files \castro\ needs to determine what your problem setup will be. They contain the default 
parameters for merger runs. It also copies any auxiliary files one might need, like the 
table for the equation of state, as well as the actual \castro\ executable you just built in the 
\texttt{compile/} directory, and a copy of the run utilities that manage the job itself (so that the 
run doesn't change if you make local edits to the job management system). Finally, it builds a script 
that you'll use for actually submitting the job to the batch submission system on the machine you're on, 
and then submits that job. Thus for a typical setup, your run directory will contain at least the following 
files when you submit:
\begin{itemize}
  \item \texttt{Castro3d*.ex}
  \item \texttt{helm\_table.dat}
  \item \texttt{inputs}
  \item \texttt{probin}
  \item \texttt{run\_script}
  \item \texttt{job\_scripts/}
\end{itemize}
Note that if you want to set up this directory as above but \textit{not} actually submit the job, 
set the \texttt{no\_submit} variable to any value in your run script. Similarly, if you want to 
set up the directory to contain the inputs and probin file but none of the machine-dependent 
scripts or the executable, set the \texttt{inputs\_only} variable to any value in your run script.

Typically you'll want to run multiple jobs at once, that vary things like the number of processors 
or the runtime parameters. For example, suppose we want to run the same problem but we want to 
explore what happens when we change the rotational period of the system. The main helper functions 
will automatically handle this for you as long as your variable is part of a valid namespace in the inputs files.
The orbital period is handled by the inputs parameter \texttt{castro.rotational\_period}. Therefore 
we simply need to update a variable named \texttt{castro\_rotational\_period}. 
Note that we use an underscore (\texttt{castro\_}) and not a period (\texttt{castro.}) after the namespace.
This is simply because \texttt{bash} does not permit periods in variable names; but the helper routine will 
automatically take care of this for you when looking for the appropriate variable in the inputs file.
A sample script follows:

\begin{lstlisting}
source $WDMERGER_HOME/job_scripts/run_utils.sh

nprocs=512
walltime=12:00:00

for period in 25.0 50.0 75.0 100.0
do
  dir=results/period$period/
  castro_rotational_period=$period
  stop_time=$period
  run $dir $nprocs $walltime
done
\end{lstlisting}%$
This will create the subdirectories \texttt{results/period25.0} through \texttt{results/period100.0}
and in each directory the inputs file will have the corresponding \texttt{castro.rotational\_period}.
Each job will stop after one rotational period has been completed. At the end, the script will automatically move all 
output files (checkpoints, plot files, diagnostics)
to the \texttt{output/} subdirectory of that problem location, and on supported HPC systems will also copy these 
files to the long-term storage system (see Section \ref{sec:archiving} for more details). 
Generally our analysis scripts expect all the output to be in the 
\texttt{output/} subdirectory. Also, if the machine crashes before your run completes, running \texttt{bash run.sh}
again will automatically restart your job from the most recent checkpoint.

\section{Non-Standard Problems}
\label{sec:non-standard_problems}

If you want to build your own problem setup, you can do that too. The job submission 
requirements are the same as in Section \ref{sec:standard_problems}, but you will need to provide 
your own source files and makefile. They can go in the \texttt{compile/} subdirectory in your problem location.
At minimum, you'll need a \texttt{Prob\_3d.f90} file (or the equivalent for lower dimensions) and
corresponding inputs and probin files. The problems in \texttt{science/wdmerger\_I/castro} are similar to this;
they are ultimately building using the source code in the CASTRO test suite, but using a
\texttt{run\_test.sh} that is very similar to the one used for merger problems. Note that for problems
in CASTRO/Exec, you'll need to append \texttt{CASTRO\_DIR=\$CASTRO\_DIR} to \texttt{make} at the
command line, which will force an override of that variable so that you can compile
outside of the root CASTRO directory.

\chapter{Test Problems}

The \texttt{science/wdmerger\_I/} directory contains a number of example problems that we use
to assess the reliability of \castro\ in performing hydrodynamics simulations 
with self-gravity. This section includes a list of the existing problems 
as well as the instructions for building and running a problem.

\begin{itemize}
  \item The \texttt{binary} subdirectory includes problems that test the binary star setup.
  \begin{itemize}
    \item The \texttt{boundary\_condition\_comparison} test checks the validity of our 
      multipole approximation for the boundary conditions on the Poisson equation. It runs
      the gravity solve for several different choices of the number of expansion coefficients
      and compares the resulting gravitational potential to the exact result.
    \item The \texttt{circular\_orbit} test loads two white dwarfs on a grid and then orbits
      them for a large number of periods, to test conservation of linear momentum,
      angular momentum and energy convergence.
    \item The \texttt{freefall} test loads two white dwarfs onto a grid and allows them to fall towards
      each other under their mutual gravitational influence. We check to see whether the timescale for them
      colliding matches the analytical expectation for two point masses.
  \end{itemize}
  
  \item The \texttt{single} subdirectory is for tests that only use a single star.
  \begin{itemize}
    \item The \texttt{single\_star\_hse} test loads a single white dwarf on a grid and checks how well 
      hydrostatic equilibrium is maintained.
  \end{itemize}

  \item The \texttt{castro} subdirectory is for running tests in the CASTRO test suite that are relevant
  to issues that the merger problem will face.
  \begin{itemize}
    \item The \texttt{uniform\_cube\_sphere} test is designed to check the convergence
        properties of the Poisson solver. It loads either a sphere or cube of uniform density
        onto the computational domain and then calculates the potential at several different
        spatial resolutions to check if the potential converges
    \item The \texttt{evrard\_collapse} test runs the test problem described originally by 
      \cite{evrard:1988} and whose results for a grid-based code can be seen in \cite{arepo}. A spherical
      distribution of gas obeying a gamma-law equation of state is loaded onto a grid with negligible 
      pressure. The gas collapses, rebounds with a shock, and then settles into hydrodynamic equilibrium.
      It is a good test of the energy conservation properties of the code.
    \item The \texttt{kelvin\_helmoltz} test runs several varieties of a 2D shearing fluid that exhibits the 
      Kelvin-Helmholtz instability. The relevant papers are \cite{robertson:2010}, \cite{arepo}, and \cite{mcnally:2012}.
  \end{itemize}

  \item The \texttt{performance} subdirectory examines the scaling and performance properties of CASTRO.
  \begin{itemize}
    \item The \texttt{strong\_scaling} test runs the same problem with a varying number of processors and 
      checks the scaling properties.
    \item The \texttt{weak\_scaling} test scales the size of the problem with the number of processors and
      checks the scaling properties.
    \item The \texttt{threading} test is useful for determining the optimal number of OpenMP threads for your machine.
    \item The \texttt{load\_balancing} test finds which load balancing strategy makes the most sense for our problem.
  \end{itemize}
\end{itemize}

Each test directory is set up with a uniform pattern. The \texttt{source/} subdirectory includes all of the source code
needed to build the test. The \texttt{compile/} directory contains the \texttt{GNUmakefile}, which builds the executable.
The \texttt{run\_test.sh} script executes the test by creating a subdirectory
for each parameter set and then submitting it with a batch submission script (for a vanilla Linux installation,
this will use the \texttt{batch} utility; for a limited number of clusters and supercomputers, 
there will be recommended options and specialized submission scripts). The \texttt{analysis.py} Python script
is used to analyze the output and generate figures. In summary, to run a test, perform the following series of commands:
\begin{itemize}
  \item \texttt{cd compile}
  \item \texttt{make}
  \item \texttt{cd ..}
  \item \texttt{bash run\_test.sh}
  \item \texttt{bash run\_test.sh} (after the execution has completed, to archive output)
  \item \texttt{python analysis.py} (after the execution has completed)
\end{itemize}



\chapter{System Configuration}

Running the wdmerger software will require a little setup in terms of configuring the 
scripts to work with the system you are currently using. The software is currently actively 
maintained only for a small number of supercomputing resources (Blue Waters at NCSA 
and Titan at OLCF, in particular). If you are using one of these machines, then 
modifying the code for your purposes is straightforward. If you are not, you'll need to do a 
small bit of ground work to get everything set up, but once you do the automated scripts 
will handle everything else for you. Feel free to contact us for advice on how to do so 
(see Section \ref{sec:contributors} for contact information), but be aware that it will be 
hard for us to debug on systems that we do not have access to. 

It is important to note that although we have attempted to 
make sure that this software builds and runs on a generic Linux workstation, most of the test 
problems are simply too large to run on anything smaller than a medium-sized cluster. To get a flavor 
of what is going on, though, you can comment out the larger runs from the various loops in the 
\texttt{run\_test.sh} file and then run the smaller problem setup.

\section{Account Details and System Options}

All of the machine-dependent functionality exists in the file \texttt{job\_scripts/run\_utils.sh}.
In particular, near the end of the file you can find a bash conditional that tests on the name of 
certain machines. Each machine definition includes a number of variables that are needed to 
properly configure all of the helper functions. If you are using a system already known to us,
then the only thing you need to change is the \text{allocation} variable: this is the account 
that jobs will be charged to on the machine. 

If you are establishing a new machine, then there are a few steps you will need to take. First, 
add a section to this conditional that tests on what you want to call the name of the machine. 
Then, make entries for the necessary variables:
\begin{itemize}
  \item \texttt{allocation}: the account jobs are charged to.
  \item \texttt{exec}: the function used to submit batch scripts. For PBS-style submission 
systems such as TORQUE, this will be \texttt{qsub}.
  \item \texttt{ppn}: the number of processors per node on the system. Be careful with how 
you define this. Many systems have nodes that have two integer cores for every floating point
unit and will report the number of integer cores as the number of processors per node, 
and for floating point-intensive applications like hydrodynamics adding as many threads/MPI tasks
as there are integer cores may actually slow you down. You will have to test to find 
the best setting for you.
  \item \texttt{run\_ext}: the file extension of the standard output file when a job is in operation. 
This is used by the run scripts to ensure that we don't submit a job in a directory where one is 
already running. For many systems this will be ``.OU''.
  \item \texttt{batch\_system}: the job submission system used on the machine. For many supercomputers 
``PBS'' will be what you should insert here, which is a catchall for any derivative such as TORQUE 
that uses normal PBS options for accepting jobs.
  \item \texttt{archive\_method} (optional): the method used by the machine for saving files to 
long-term storage. See Section \ref{sec:archiving}. This is optional, and if you don't set it then
the scripts won't attempt to copy your data off the machine.
\end{itemize}
When you submit a job through a \texttt{run\_test.sh} file using the \texttt{run} command, 
a batch script will be automatically created for you. For PBS systems you should be mostly 
good to go as-is, but some systems vary particularly in the \texttt{-l nodes} option, so 
you should make sure that the job script construction is in sync with how batch jobs 
on that machine should appear. Locate the \texttt{create\_job\_script} function and adjust it 
accordingly by changing the relevant \texttt{echo} statements. If you are using a different submission
system, create a new section of the conditional in that function, and model it off what we do for PBS
so that the relevant information is output to the newly-created job script (called \texttt{run\_script} 
by default; you can change this in the definitions section as well).

The last step is to tell the script how to know that you're using the machine you just defined. 
Locate the \texttt{get\_machine} command and use the output of \texttt{uname -n} to uniquely determine 
which system you are using.

\section{Archiving}
\label{sec:archiving}

This software has some capability to automatically handle archiving of data to long-term storage,
at least for systems we already know. Each time we run a job and we are continuing from an old checkpoint,
we move all new plotfiles and checkpoints to the \texttt{output/} subdirectory of the job submission 
location. We also make copies of the diagnostics files, and the standard output files. By default this is 
all we do. But on systems where archiving is supported, we copy these files to the long-term storage.

There are currently two methods we know about: \texttt{htar} for Titan, and Globus Online for Blue Waters.
\texttt{htar} is a command that both creates a tar file and then copies it to the high-performance storage system 
(HPSS) using the \texttt{hsi} command. It works on a file-by-file basis, so we loop over all of the files to 
archive and run \texttt{htar} on them individually. If your system uses \texttt{htar}, then you should be all set.

\href{https://www.globus.org/}{Globus Online} is an internet service that handles data transfer requests 
between certain known endpoints. This is the choice used by Blue Waters for copying and retrieving data
to their storage system. You will need to set up your account and authenticate through Globus before using it.
Once you know your Globus username, you can adjust the \texttt{globus\_username} in \texttt{run\_utils.sh}. 
We note that the Globus archive strategy is different. There is a cap on the number of simultaneous transfers 
one can have, so submitting transfers for every plotfile and checkpoint will not work. Instead, when we 
run the archiving script, we first move all the files to the output directory, and then we do a transfer 
on the entire \texttt{output/} subdirectory. Globus will automatically sync the source and destination 
directories, so no extra work is required of us. We choose the \texttt{sync\_level = 2} option, which 
overwrites existing files if the source has a newer date than the destination. This allows us to 
continually update the diagnostic files, which always have the same name.

If your system uses a different archive strategy, you will need to update the \texttt{archive\_all} 
and \texttt{archive} functions.

\chapter{Contributors}
\label{sec:contributors}

This project was started as the Ph.D. dissertation project for Max Katz at Stony Brook University. 
Michael Zingale is Max's primary thesis advisor. Correspondence and requests for information 
about this software should be directed to them; their e-mail addresses are maximilian.katz@stonybrook.edu
and michael.zingale@stonybrook.edu, respectively. Alan Calder and Doug Swesty at Stony Brook are 
also co-investigators on this project. The \castro\ code was developed primarily at the 
Lawrence Berkeley National Laboratory. Staff there that have contributed directly and 
indirectly to this project include Ann Almgren, John Bell, Weiqun Zhang, Andy Nonaka, and Vince Beckner.

We also gratefully acknowledge the assistance of Noel Scudder and Platon Karpov, 
Stony Brook University undergraduates who helped with running simulations and 
developing the visualization software based on yt. Adam Jacobs at Stony Brook also provided 
assistance in various aspects of deploying our codes on supercomputing resources.

%------------------------------------------------------------------------------
\backmatter

\bibliographystyle{../papers/aasjournal}
\bibliography{../papers/refs}

\end{document}
