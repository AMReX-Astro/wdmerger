\documentclass[twocolumn,numberedappendix]{../aastex62}

% these lines seem necessary for pdflatex to get the paper size right
\pdfpagewidth 8.5in
\pdfpageheight 11.0in

% for the red MarginPars
\usepackage{color}

% some extra math symbols
\usepackage{mathtools}

% allows Greek symbols to be bold
\usepackage{bm}

% allows us to force the location of a figure
\usepackage{float}

% allows comment sections
\usepackage{verbatim}

% Override choices in \autoref
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

% MarginPars
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{\vskip-\baselineskip\raggedright\tiny\sffamily\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

\newcommand{\msolar}{\mathrm{M}_\odot}

% Software names
\newcommand{\amrex}{\texttt{AMReX}}
\newcommand{\boxlib}{\texttt{BoxLib}}
\newcommand{\castro}{\texttt{CASTRO}}
\newcommand{\maestro}{\texttt{Maestro}}
\newcommand{\microphysics}{\texttt{Microphysics}}
\newcommand{\wdmerger}{\texttt{wdmerger}}
\newcommand{\python}{\texttt{Python}}
\newcommand{\matplotlib}{\texttt{matplotlib}}
\newcommand{\yt}{\texttt{yt}}
\newcommand{\vode}{\texttt{VODE}}
\newcommand{\isoseven}{\texttt{iso7}}
\newcommand{\aproxthirteen}{\texttt{aprox13}}
\newcommand{\aproxnineteen}{\texttt{aprox19}}
\newcommand{\aproxtwentyone}{\texttt{aprox21}}

\begin{document}

%==========================================================================
% Title
%==========================================================================
\title{Numerical Stability of Detonations in White Dwarf Simulations}

\shorttitle{Numerical Detonations}
\shortauthors{Katz et al. (2018)}

\author{Max P. Katz}
\affiliation
{
  NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, CA, 95051, USA
}

\author{Michael Zingale}
\affiliation
{
  Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY, 11794-3800, USA
}



%==========================================================================
% Abstract
%==========================================================================
\begin{abstract}
Some simulations of Type Ia supernovae feature thermonuclear detonations that
arise self-consistently during the evolution. It is not clear whether these
detonations would occur in a simulation that fully resolved the burning. In this
study we examine a test detonation problem inspired by collisions of white dwarfs.
This test problem demonstrates that achieving a converged thermonuclear ignition
requires spatial resolution finer than 1 km in the burning region, and that
the simulation as a whole likely needs a base resolution on the order of 1 km.
Current computational resource constraints place this stringent resolution requirement
out of reach for almost all multi-dimensional supernova simulations.
Consequently, contemporary simulations that self-consistently demonstrate
detonations are very likely not resolved and should not be trusted.
\end{abstract}
\keywords{supernovae: general - white dwarfs}

%==========================================================================
% Introduction
%==========================================================================
\section{Introduction}
\label{sec:introduction}

Thermonuclear detonations are common to all current likely models of Type Ia
supernovae (SNe Ia), but how they are actually generated in progenitor systems
is still an open question. Different models predict different locations for
the detonation and different mechanisms for initiating the event. Common to all
of the cases is a severe lack of numerical resolution in the location where the
detonation is expected to occur. The length and time scale at which a detonation
forms is orders of magnitude smaller than the resolution that typical multi-dimensional
hydrodynamic simulations can achieve. The mere presence of a detonation (or lack thereof)
in a simulation is therefore only weak evidence regarding whether a detonation would truly occur.

In this study we examine the challenges associated with simulating thermonuclear detonations.
The inspiration for this work comes from the literature on head-on collisions of WDs,
which can occur, for example, in certain triple star systems \citep{thompson:2011,hamers:2013}.
WD collisions rapidly convert a significant amount of kinetic energy into thermal energy and
thus set up conditions ripe for a thermonuclear detonation. Collisions are interesting
from the simulation perspective because of both this susceptibility to detonation and
the relative ease in setting them up.

Early studies on WD collisions \citep{rosswog:2009,raskin:2010,loren-aguilar:2010,
hawley:2012,garcia-senz:2013} typically had effective spatial resolutions in the
range 100--500 km, and observed detonations that convert a large amount of
carbon/oxygen material into iron-group elements. These studies varied in methodology
(Lagrangian versus Eulerian evolution, nuclear network used) and did not closely agree
on the final result of the event (see Table 4 of \cite{garcia-senz:2013} for a summary).
\cite{kushnir:2013} argued that many of these simulations featured numerically unstable
evolution, ultimately caused by the zone size being significantly larger than the length
scale over which detonations form. The detonation length scale can vary widely based
on physical conditions \citep{seitenzahl:2009,garg:2017} but is generally much
smaller than 100 km. In this paper, we attempt to find what simulation length scale
is required to achieve converged thermonuclear ignitions.



\section{Numerically Unstable Burning}
\label{sec:unstable_burning}

\citet{kushnir:2013} observe an important possible failure mode
for reacting hydrodynamics simulations. Let us define $\tau_e = e / \dot{e}$
as the nuclear energy injection timescale, and $\tau_s = \Delta x / c_s$
as the sound-crossing time in a zone (where $\Delta x$ is the grid
resolution and $c_s$ is the speed of sound).
When the sound-crossing time is too long, energy is built up in
a zone faster than it can be advected away by pressure waves.
This is of course a problem inherent only to numerically discretized
systems as the underlying fluid equations are continuous.
This can lead to a numerically seeded detonation caused by the
temperature building up too quickly in the zone; the detonation
may be spurious in this case. If $\tau_s \ll \tau_e$, we can be
confident that a numerically seeded detonation has not occurred.
In practice, we will quantify this as:
\begin{equation}
  \tau_s \leq f_{s}\, \tau_e \label{eq:burning_limiter_2}
\end{equation}
and require that $f_{s}$ is sufficiently smaller than one.
\citet{kushnir:2013} state that $f_{s} = 0.1$ is a sufficient
criterion. \citet{kushnir:2013} enforced this criterion on
their simulations by artificially limiting the magnitude of the energy
release after a burn.

To understand this choice, consider the outcome of our baseline case.
When the two WDs collide, there is a prompt detonation
at the contact point at the center of the domain. This detonation occurs
when the density is relatively low because the WDs have not had much
time to collide, and so the nucleosynthetic yield of nickel is low. If
this is the case, the colliding WDs are not a good candidate for
normal SNe Ia. However, if this detonation is delayed, material builds
up at a stalled shock in the center, and the eventual detonation occurs
on the outer edge of the stalled shock. There will then be much more
high density material to process into nickel, and the resulting
explosion creates an amount of nickel potentially sufficient to explain
normal SNe Ia. The discussion thus centers on whether the prompt detonation
is physical. \citeauthor{kushnir:2013} argues that it is not, on the basis
that $\tau_s > \tau_e$ when this detonation occurs. We find the same thing:
for the baseline case, the sound-crossing time is about five times longer
than the energy injection time. This means that an ignition is guaranteed
to occur for numerical reasons. \citeauthor{kushnir:2013} thus argue that
the suppressed burn is justifiable, as it suppresses this prompt detonation.

This argument is flawed because a physical detonation may \textit{also}
occur this way. For example, consider a region of WD material at uniformly
high temperature, say $5 \times 10^9\ \text{K}$, with an arbitrarily large size,
say a cube with side length 100 km. This region will very likely ignite,
even if it is surrounded by much cooler material. By the time the material on
the edges can advect heat away, the material in the center will have long since
started burning carbon, as the sound crossing time scale is sufficiently large
compared to the energy injection time scale. This is true regardless of whether
the size of this cube corresponds to the spatial resolution in a simulation.
Suppression of the burn in this case is unphysical: if we have a zone matching
these characteristics, the zone should detonate.

When the resolution is low enough, there is a floor on the size of a hotspot,
possibly making such a detonation more likely. This is an unavoidable consequence
of the low resolution; yet, it may be the correct result of the simulation that
was performed. That is, even if large hotspots are unphysical because in reality
the temperature distribution would be smoother, if such a large hotspot \textit{were}
to develop (which is the implicit assumption of a low resolution simulation), then
it would likely detonate. If the results do not match what occurs at higher
resolution, then the simulation is not converged and the results are not reliable.
However, it may also be the case that a higher resolution simulation will yield
similar results, for example because even at the higher resolution, the physical
size of the hotspot stays the same. Additionally, one might think that simply
adding adaptive mesh refinement in the burning region could help, because then
$\Delta x$ decreases, so the ratio of $\tau_s$ to $\tau_e$ should decrease.
We developed an AMR criterion to do just this: we tag on regions suggested by
the burning stability criterion, \autoref{eq:burning_limiter_2}. But if a low
resolution simulation develops a very large hotspot that violates the stability
criterion, adding refinement inside this region will likely not help, because the
energy trapped in the center of the region may not be able to escape before ignition
occurs. So for the simulations with resolution inside the stars that was coarser than
10 km, even adding a significant amount of refinement (say, refining by a factor of
64 or 128) did not change the propagation of a detonation.

For the above reasons, an appeal to the numerical instability criterion alone is
insufficient to understand whether a given ignition is real. We find that \textit{all}
of the detonations we have seen at any resolution violate the stability criterion,
even for the finest resolution simulation we performed (150 m, inside the burning region).
This is true even if the prompt detonation is suppressed:
the delayed detonation also is numerically unstable. For comparison purposes, we
developed an option for our burner to use a ``suppressed'' burning mode. In a
suppressed burn, we limit the changes to the state so that \autoref{eq:burning_limiter_2}
is always satisfied\footnote{To achieve this we directly multiply the
right-hand-side vector in the integration by a factor $F$
for all variables, where $F$ is the multiplicative factor needed to
be applied to $\dot{e}$ such that the equality in \autoref{eq:burning_limiter_2}
holds. (If the inequality is already satisfied, then the integration
vector is not modified.) We fix $\tau_s$ to be the value of the sound
crossing time at the beginning of the burn (that is, we do not
update it as the sound speed changes) and we fix the energy $e$
that goes into the estimate for $\tau_e$ to be the value of the
internal energy of the zone at the beginning of the burn.}. When we
evolve the baseline case with this suppressed burner, the prompt detonation
is indeed suppressed. A later detonation does occur, but this delayed
detonation also triggers the suppression. The result is that the detonation
does not get to very high temperatures, and the result is again a ``failed''
supernova that does not generate nearly enough nickel. Thus we do not
reproduce the claim of \citeauthor{kushnir:2013} that the suppressed
burning mode suppresses only the prompt detonation, but allows the
delayed detonation to occur. We could find some way to turn off the
suppression and allow the delayed detonation, but without some independent
confirmation that the delayed detonation is physical (while the prompt
detonation is not) this would be begging the question.

The upshot of this discussion is that the only way to resolve the
question is with higher resolution simulations. The semi-analytic
criterion provided by \cite{garg:2017}, and the numerical calculations
of \cite{seitenzahl:2009}, give critical radii for spontaneous detonation.
For the densities we are interested in ($\sim 10^7\ \text{g\,/\,cm}^3$),
assuming equal C/O material, the critical radii are in the range 1-10 km.
So this suggests that our simulation needs to do significantly better than
this if we are likely to make headway on this issue. Detonations observed
at higher resolution may be physically plausible because the critical
temperature gradient can be resolved. The high resolution needs to be
achieved with some combination of high resolution of the collision process
itself, and AMR based on the stability criterion as described
above, where we tag for refinement all zones above some $f_{s}$, the
ratio of the sound crossing time to the energy injection time. We
cannot solely rely on a low resolution run with AMR based on the
burning stability criterion for the reason described above, that a
detonation may be spuriously locked in by the low resolution regardless
of how much resolution we can practicably add to it.



%==========================================================================
% 1D collision test problem
%==========================================================================
\section{Numerical Test Problem}
\label{sec:collisions}

Plotfiles of relevant quantities are created every 0.01 seconds.
The timesteps are limited to ensure that every multiple of this plotfile interval is
reached exactly, which means an effective maximum timestep of 0.01 seconds. Timesteps
are by default limited only by the hydrodynamic stability constraint and this plotfile
interval constraint. For simulations with resolution better than 64 km, we use a CFL
number of 0.5. For the simulations with lower resolution, we use a CFL number of 0.01.
This is because at low resolution, the timestep is so large that the detonation can
form in a single timestep and represent such a large change that the burning integration
fails (\autoref{sec:retries}). The timestep retry mechanism can handle this gracefully,
but the subcycled timestep necessary to do so is usually so small that the full level
timestep ends up being so long that it just makes more sense to avoid this situation
entirely and use smaller level advance timesteps.



%==========================================================================
% Conclusions
%==========================================================================
\section{Conclusions and Discussion}\label{Sec:Conclusions and Discussion}
\label{sec:conclusion}



\acknowledgments

This research was supported by NSF award AST-1211563 and DOE/Office of
Nuclear Physics grant DE-FG02-87ER40317 to Stony Brook. An award of
computer time was provided by the Innovative and Novel Computational
Impact on Theory and Experiment (INCITE) program.  This research used
resources of the Oak Ridge Leadership Computing Facility located in
the Oak Ridge National Laboratory, which is supported by the Office of
Science of the Department of Energy under Contract
DE-AC05-00OR22725. Project AST106 supported use of the ORNL/Titan
resource.  This research used resources of the National Energy
Research Scientific Computing Center, which is supported by the Office
of Science of the U.S. Department of Energy under Contract
No. DE-AC02-05CH11231. The authors would like to thank Stony Brook
Research Computing and Cyberinfrastructure, and the Institute for
Advanced Computational Science at Stony Brook University for access
to the high-performance LIred and SeaWulf computing systems, the latter
of which was made possible by a \$1.4M National Science Foundation grant (\#1531492).

The authors thank Chris Malone and Don Willcox for useful discussions
on the nature of explosive burning, and Doron Kushnir for providing
clarification on the nature of the burning limiter used in \cite{kushnir:2013}.

This research has made use of NASA's Astrophysics Data System 
Bibliographic Services.

\clearpage

\bibliographystyle{../aasjournal}
\bibliography{../refs}

\end{document}
