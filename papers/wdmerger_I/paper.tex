\documentclass[12pt,preprint]{aastex}
% for \sout
\usepackage{ulem}
% makes sure \em{} is italic rather than underlined (corrects ulem from line above)
\normalem

% for the red MarginPars
\usepackage{color}

% some extra math symbols
\usepackage{mathtools}

% allows Greek symbols to be bold
\usepackage{bm}

\newcommand{\rhocutoff}{\rho_\mathrm{cutoff}}
\newcommand{\rhoanelastic}{\rho_\mathrm{anelastic}}

\newcommand{\gcc}{\mathrm{g~cm^{-3} }}
\newcommand{\Tcutoff}{T_\mathrm{cutoff}}

% MarginPars
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{\vskip-\baselineskip\raggedright\tiny\sffamily\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}


\newcommand{\evm}{{(-)}}
\newcommand{\evz}{{(\circ)}}
\newcommand{\evp}{{(+)}}
\newcommand{\enu}{{(\nu)}}



\newcommand{\msolar}{\mathrm{M}_\odot}

\begin{document}

%==========================================================================
% Title
%==========================================================================
\title{Double White Dwarf Mergers with CASTRO\\ I. Methodology and Code 
       Verification}

\shorttitle{DWD Mergers. I. Methodology}
\shortauthors{Max}

\author{TBD}
%==========================================================================
% Abstract
%==========================================================================
\begin{abstract}
We describe our numerical methodology for modeling double white dwarf
systems with the AMR hydrodynamics code Castro.

\end{abstract}
\keywords{hydrodynamics - methods: numerical - supernovae: general - white dwarfs}

%==========================================================================
% Introduction
%==========================================================================
\section{Introduction}

Type Ia supernovae (SNe Ia) are currently some of the most exciting events to study in astrophysics. These bright, brief pulses of light in the distant universe have led to a number of important discoveries in recent years, including the discovery of the accelerated expansion of the universe \citep{perlmutter1999,riess1998}. However, their origin is shrouded in mystery. It has long been expected that these events arise from the thermonuclear explosions of white dwarfs \citep{hoyle_fowler:1960}, but the cause of these explosions is uncertain. In particular, it is not clear what process causes the temperatures in these white dwarfs to become hot enough for explosive burning of its constituent nuclei. The model favored initially by the community was the so-called single-degenerate (SD) model \citep{whelan_iben:1973}. Accretion of material from a companion star such as a red giant would cause the star to approach the Chandrasekhar mass, and in doing so the temperature and density in the center would become sufficient for thermonuclear fusion to proceed. However, in recent years many alternative progenitor models have been discussed. A leading candidate for explaining some or most of these explosions is the double-degenerate (DD) model, in which two white dwarfs merge and the merged object reaches the conditions necessary for a thermonuclear ignition \citep{ibentutukov:1984,webbink:1984}. Another is the double detonation scenario, where accretion of material onto a sub-Chandrasekhar white dwarf would lead to a detonation inside the accreted envelope, and this would send a compressional wave into the core of the star that would trigger a secondary detonation. A recent review of the progenitor models can be found in \citet{hillebrandt:2013}.

There are several observational reasons why double-degenerate systems are a promising progenitor system for at least a substantial fraction of normal SNe Ia. No conclusive evidence exists for a surviving companion star of a SN Ia; this is naturally explained by the DD model because both WDs are likely to be destroyed in the merger process. Similarly, pre-explosion images of the SN Ia systems have never clearly turned up a companion star, and in some cases a large fraction of the parameter space for the nature of the companion star is excluded. Additionally, not enough progenitor systems are seen for the SD case to match the observed local SN Ia rate, whereas the number of white dwarf binaries may be sufficient to account for this rate. Finally, the DD model can naturally explain the fact that many SNe Ia are observed to occur at very long delay times after the stars were formed, since the progenitor systems only become active once both stars have evolved off the main sequence. A thorough review of the observational evidence about SNe Ia and further discussion of these ideas can be found in \cite{maoz:2014}.  

The history of double degenerate theory can broadly be described as a series of three major paradigm shifts. The first attempts to model the results of the merger process came in the 1980s. \cite{nomotoiben:1985} demonstrated that off-center carbon ignition would occur in the more massive white dwarf as it accreted mass near the Eddington rate from the less massive white dwarf overflowing its Roche lobe. \cite{saionomoto:1985} tracked the evolution of the flame and found that it propagated quiescently into the center, converting the carbon-oxygen white dwarf into an oxygen-neon-magnesium white dwarf. This would then be followed by collapse into a neutron star -- a result with significantly different observational properties compared to a SN Ia. This scenario, termed accretion-induced collapse, would be avoided only if the accretion rate were well below the Eddington rate. \cite{tutukov_yungelson:1979} observed that this could happen if the mass loss from the secondary was higher than the Eddington rate and thus the accreted material formed an accretion disc, which might rain down on the primary more slowly. The main finding was that double degenerate systems would not obviously lead to Type Ia supernovae.

The first three-dimensional simulations of double degenerate systems were performed by \citet{benz:1990}, who used the smoothed particle hydrodynamics (SPH) method to simulate the merger process. They found that if the lower-mass star (generally called the ``secondary'') was close enough to the more massive star (the ``primary'') to begin mass transfer on a dynamical time scale, the secondary completely disrupted and formed a hot envelope around the primary, with a centrifugally-supported accretion disk surrounding the core and envelope. They noted that the envelope temperature was hot enough to generate carbon fusion but neglected nuclear reactions in their simulation. Later simulations of similar setups \citep{rasio_shapiro:1995,yoon:2007,loren-aguilar:2009,raskin:2012} corroborated this result. This validation of the prediction of \cite{tutukov_yungelson:1979} did not instill confidence in the community. \cite{mochkovitch_livio:1990} and \cite{livio:2000} observed that turbulent viscosity would be sufficiently large for angular momentum to be removed from the disk at a rate high enough to generate the troublesome accretion timescales. Based on all of this evidence, the review of \cite{hillebrandtniemeyer2000} argued that the model was only viable if the accretion-induced collapse problem could be avoided. Later work by \cite{shen:2012} and \cite{schwab:2012} used a more detailed treatment of the viscous transport in the outer regions of the remnant and found that while the centrifugally supported envelope would be converted into hot envelope material on a viscous timescale, their simulations still led to off-center carbon burning. \cite{vankerkwijk:2010} argued that equal-mass mergers would lead to the conditions necessary for carbon detonation in the center of the merged object, but \cite{shen:2012} questioned this for similar reasons related to how viscous transport would convert rotational motion into pressure support. \cite{zhu:2013} followed this with an expanded parameter space study and argued that many of their carbon-oxygen systems had the potential to detonate. The study of the long-term evolution of the remnants is thus still an open subject of research.

The most recent shift in perspective on this problem started with two series of papers appearing at roughly the same time. One began with \cite{pakmor:2010} and \cite{pakmor:2011}. This group used the SPH method to study the merger of equal-mass ($0.9\ M_\odot$) carbon-oxygen white dwarfs and found that a hotspot was generated near the surface of the primary white dwarf. They argued that this region had a temperature and density sufficient to trigger a thermonuclear detonation. They propagated this detonation throughout the system and found that it would observationally appear as a subluminous Type Ia supernova. This was the first time a DD simulation successfully reproduced at least some characteristics of a SN Ia. \cite{pakmor:2011} tried a few different mass combinations and found empirically that this would hold as long as the secondary was at least 80\% as massive as the primary. These events, where the merger process resulted in the detonation of the system during the merger coalescence -- avoiding the much longer time-scale evolution -- were termed ``violent'' mergers.

Around the same time, however, \cite{guillochon:2010} and \cite{dan:2011} came to significantly different conclusions. They pointed out that the previously mentioned simulations generally all shared a significant drawback, which was that their initial conditions were not carefully constructed. \cite{motl:2002}, \cite{dsouza:2006}, and \cite{motl:2007} (the first three-dimensional grid-based simulations of mass transfer in binary white dwarf systems) pioneered the study of looking at the long-term dynamical evolution of binary white dwarf systems after constructing exact equilibrium initial conditions. In contrast to the other studies, they found that paying close attention to the accuracy of the initial conditions of the simulation has important effects. In particular, earlier work placed the stars too close together and ignored the effects of tidal forces on changing the shape of the secondary, leading to the merger happening artificially too quickly. When the initial conditions are constructed in exact equilibrium, the system can be stable for tens of orbital periods, substantially changing the character of the mass transfer phase. However, one limitation of this series of studies is that the authors used a polytropic equation of state and thus could not consider nuclear reactions. \cite{guillochon:2010} and \cite{dan:2011} improved on this using a realistic equation of state, a nuclear reaction network, and a similar approach to the equilibrium initial conditions, and found substantial agreement with the idea that mass transfer occurs in a stable manner over tens of orbital periods. They also found that, assuming the material accreted onto the surface of the primary was primarily helium, explosive surface detonations would occur as a result of accretion stream instabilities during the mass transfer phase prior to the full merger. This could trigger a double-detonation explosion and thus perhaps a SN Ia.

The very latest developments give some areas of agreement and some of remaining uncertainty. \cite{pakmor:2012} performed a merger scenario with a $1.1\ M_\odot$ and $0.9\ M_\odot$ setup, with better treatment of the initial conditions, and also found that the merger process happened over more than ten orbits. Nevertheless, they still found that a carbon-oxygen detonation would occur, in line with their earlier results. \cite{moll:2014} was also able to find a detonation in a similarly massive system. \cite{dan:2012} and \cite{dan:2014} performed a large sweep of the parameter space for merger pairs and found that pure carbon-oxygen systems would generally not lead to detonations and violent mergers except for the most massive systems. They did find that for systems containing helium, many of them would detonate and potentially lead to SNe Ia, either through the aforementioned instabilities in the accretion stream, or during the contact phase, similar to the violent carbon-oxygen mergers. \cite{pakmor:2013} added a thin helium shell on their primary white dwarf, and found that this robustly led to a detonation of the white dwarf. Thus at least a preliminary agreement may be that systems containing helium could robustly lead to events resembling SNe Ia, as well as very massive carbon-oxygen binaries.

Given the above, why is another approach using a different simulation code warranted? First and foremost, reproducibility of the results across simulation codes and algorithms is important for gauging confidence in this result. With only a couple of exceptions, most of the existing results that study the viability of the DD progenitor as an explanation for Type Ia supernovae (that is, including a temperature-dependent equation of state and nuclear reactions) have used the SPH method. SPH codes have a number of features which do aid them in the study of these systems, such as excellent conservation of angular momentum. However, the question of whether a prompt detonation in a merger actually happens depends in detail on the nature of the gas at the interface between the two stars, which is at much lower density than the rest of the stellar material. The SPH codes use uniform mass particles, so their effective resolution is \textit{lowest} at the stellar surface. In contrast, a grid-based code with adaptive mesh refinement can zoom in on the regions where hotspots will develop, while also maintaining high enough resolution in the high-density regions to adequately capture the large-scale mass transfer dynamics. However, there are also outstanding questions of convergence in SPH (e.g. \cite{zhu_SPH:2014}) and whether the method correctly captures fluid instabilities. This is an important question for white dwarf mergers because of the likely importance small-scale instabilities will have on the evolution of the low-density gas at the primary's surface. The pioneering work of \cite{agertz:2007} compared grid and SPH codes and found some important differences. Most relevant for this discussion is that the SPH codes could not adequately handle mixing from the Kelvin-Helmholtz instability in the test they propose. As pointed out by \cite{price:2008}, this is not a result of SPH being inherently unable to model this instability, but instead it is attributed to the fact that the standard SPH evolution equations do not have a mechanism for capturing discontinuities in internal energy. \citeauthor{price:2008} showed that the addition of an artificial thermal conductivity can dramatically improve the ability of the SPH codes to exhibit this instability. There have since been a number of other papers discussing this issue, but to our knowledge none of these improvements have yet been incorporated into an SPH model of a WD merger. Another reason to be skeptical is that the detonation itself was inserted manually in \cite{pakmor:2010} based on small-scale detonation simulations. A self-consistent evolution of a thermonuclear detonation, if it existed, would significantly build confidence in the progenitor model.

This is the first in a series of papers designed to address these outstanding theoretical issues for white dwarf mergers. This work will discuss the verification of our hydrodynamics code for simulating these events. Later efforts will look at the initial conditions of the system, the robustness with which a hotspot is found from which a detonation could occur, and the importance of the initial white dwarf models, which should be more sophisticated than simple carbon-oxygen mixtures and in principle should use results from modern stellar evolution calculations. Section \ref{sec:Numerical Methodology} describes our code and why it can provide useful results compared to other methodologies used for this problem. Section \ref{sec:initial_models} describes the method we use for setting up a binary white dwarf simulation. Section \ref{sec:Tests} discusses a few test problems that we use to demonstrate that our code accurately solves the equations of fluid dynamics. Section \ref{sec:Performance} demonstrates that the software scales well for supercomputer applications. Finally, Section \ref{sec:Conclusions and Discussion} recaps what we have shown and highlights some of the future work we plan to do.

%==========================================================================
% Numerical Methodology
%==========================================================================
\section{Numerical Methodology}\label{sec:Numerical Methodology}

To study the white dwarf merger problem, we use the grid-based hydrodynamics code CASTRO \citep{castro}. CASTRO solves the Euler equations, along with the inclusion of optional modules for gravity, nuclear reactions and thermodynamics. CASTRO is based on the BoxLib adaptive-mesh refinement (AMR) framework \citep{rendleman:2000}, which represents fluid data on a mesh where regions of interest have higher spatial resolution. CASTRO is highly parallel and is designed for large-scale use on modern supercomputers; see Section \ref{sec:Performance} for information on how CASTRO performs for our problem. The next few subsections describe our approach to each of the physics components used in this work. We direct the reader to the original code paper for a full description of CASTRO's approach to solving the equations of hydrodynamics. In this work, we report mainly on the changes we have made to the code since its original release, for the purpose of approaching this problem. 

\subsection{Hydrodynamics}

The Euler equations for hydrodynamics (in the absence of source terms) in conservative form are:
\begin{align}
  \frac{\partial \rho}{\partial t} &= -\bm{\nabla} \cdot (\rho \mathbf{u}) \label{eq:euler_density}\\
  \frac{\partial \rho \mathbf{u}}{\partial t} &= -\bm{\nabla} \cdot (\rho \mathbf{u}\mathbf{u}) - \bm{\nabla}p \label{eq:euler_momentum}\\
  \frac{\partial \rho E}{\partial t} &= -\bm{\nabla}\cdot(\rho\mathbf{u}E + p\mathbf{u}). \label{eq:euler_energy}
\end{align}
Here $\rho$ is the mass density, $\mathbf{u}$ is the fluid velocity vector, $p$ is the pressure, and $E = \mathbf{u}^2 / 2 + e$ is the total energy density where $e$ is the internal (thermal) energy density.

We use the unsplit piecewise-parabolic method (PPM) solver in CASTRO to advance the hydrodynamics system in time \citep{ppmunsplit}.  A number of changes were made to the solver, which are detailed in the Appendix.  These changes bring the algorithm more in line with that of \cite{ppm}. CASTRO as originally released featured a slightly modified version of the higher resolution limiters of \cite{colella_sekora:2008}, which can be accessed in the code using \texttt{castro.ppm\_type = 2}. The advantage of this limiter is that it preserves physical extrema rather than clipping them off as in the original approach of \cite{ppm}. However, we found these limiters to be unsatisfactory for our problem. There are many regions in our problem with large density gradients (such as the interface between the star's atmosphere and the ambient gas outside of it) and in these regions the algorithm can yield negative densities. This often results from the limiters interpreting these gradients as being true minima. As a result, we use the original limiter, which is strictly monotonicity preserving in the parabolic profiles it generates; this is activated with \texttt{castro.ppm\_type = 1}.

A related issue that required a code improvement was that in cases of large density gradients such as the edge of a star, it is possible to generate negative densities in zones even with the more strongly limited PPM. This can occur if a region of large density is moving away from an ambient zone at relatively large speeds; then the net density flux in the ambient zones can be large enough to unphysically drag the density below zero. In practice, this will occur at the trailing edge of a star that is moving across a grid. In such a situation, there are two main approaches one could take: either explicitly introduce a positivity-guaranteeing diffusive flux, or reset the characteristics of the affected zone. We choose the latter approach. Even though it is non-conservative, it preserves a characteristic we value, which is to keep the edge of the stars relatively sharp, as they physically should be. Since the mass of the affected zones is typically already fairly low, this should not seriously affect the energy conservation properties of our simulation. Similarly, it is also possible in this physical situation for very large velocities to be generated in the material behind the star. The momentum transferred to the ambient zone is close to the momentum of the stellar material, but this is of much higher density and so the velocity of the ambient material must become very large to compensate. While this may be physically viable in certain circumstances, it poses a problem for our code, which uses subcycling in time (see below). The timestep is fixed at the beginning of the set of subcycled timesteps, and if in the middle of the cycle a velocity is generated that is so large that it violates the CFL criterion for this timestep, we cannot guarantee the stability of the resulting solution since we cannot alter the timestep. (Nor would we want to, because in some cases because the velocity becomes spuriously large and would drag down the global timestep.) Therefore we also insert a check at the end of every hydrodynamics timestep that resets the velocity and/or sound-speed (indirectly, using the temperature) of any zone that violates the CFL criterion given the timestep we chose at the beginning of the cycle. In practice, we find that this is mainly necessary only for simulations in the inertial reference frame of the white dwarf merger, which we do not normally do for the reasons stated in Section \ref{sec:kepler}.

CASTRO's approach to adaptive mesh refinement, based on its underlying BoxLib framework, is to refine zones based on certain user-specified criteria that tag regions of interest for higher spatial resolution. Data is represented on one of a number of AMR levels, where each level corresponds to a set of zones at the same resolution, which covers a subset of the domain covered by the level immediately below it. We typically call the level 0 grid the \textit{coarse} grid, which has the lowest spatial resolution. Each finer, higher-level grid has a higher resolution than the grid below it by some integer factor $N$, which is restricted to be $N = 2\ \text{or}\ 4$ in the code. The zones are strictly contained within the rectangular extent of the underlying coarser zones (the code is not restricted to representing only Cartesian geometries, but we use a Cartesian mesh with uniform spacing in each dimension for the present study). For the time evolution of the AMR system we use subcycling, where each AMR level is advanced at a different timestep and a correction step is applied at the end to synchronize the various levels. Generally the number of subcycled timesteps is equal to the jump in refinement between levels, so for example on a grid with three levels and two jumps of four in refinement, the level 2 zones will have 16 times higher spatial resolution than the coarse grid and there will be 16 level 2 timesteps per level 0 timestep.

The boundary conditions on the hyperbolic system are simply zero-gradient zones that allow material to flow directly out of the domain. Using AMR, we make the coarse grid large enough that the boundaries are relatively far from the region of interest. This ensures that any boundary effects do not pollute the inner region where the stars will eventually make contact.  We further make the restriction that refined grids cannot reach the domain boundary.

\subsection{Microphysics}

The equation of state (EOS) for our simulations is the Helmholtz EOS \citep{timmes_swesty:2000}. This models an electron-positron gas of arbitrary relativity and degeneracy over a wide range of temperatures and densities. Thermodynamic quantities are calculated as derivatives of the Helmholtz free energy, and the values are interpolated from a table. The natural variables of the Helmholtz free energy are temperature and density, and calling the EOS is simplest in this form. However, in hydrodynamics we often have the density, and internal energy as independent variables, and we want to obtain the temperature, pressure, and other quantities. To do this, we employ a Newton-Raphson iteration over the temperature (given some sufficient starting guess) until we find the temperature that corresponds to the desired internal energy. Sometimes this process fails to converge and the iterative value approaches zero. In these cases we employ a ``floor'' that limits how low the temperature can go (typically $10^4$ or $10^5$ K). There is a choice here how to proceed: we can either assign this floor value to the temperature and let that zone be thermodynamically inconsistent (the original behavior in Castro), or we can adjust the internal energy to be thermodynamically consistent with the temperature, at the cost of violating energy conservation. We have found in test problems of one-dimensional shocks that the latter yields more accurate results, so we employ the latter method.

Reactions?

\subsection{Gravity}
\label{sec:gravity}

We solve the Poisson equation for self-gravity for our problem,
\begin{equation}
  \nabla^2 \Phi(\mathbf{x}) = -4\pi G\, \rho(\mathbf{x}),
\end{equation}
where $\Phi$ is the gravitational potential, $G$ is the gravitational constant, and $\rho$ is the mass density. We note that the sign convention used in CASTRO is opposite to that commonly seen in the physics literature, so that $\Phi$ is positive everywhere. The solution of this equation in CASTRO is described in \cite{castro}, and consists of both level and composite solves, and a final synchronization at the end.

\subsubsection{Coupling to Hydrodynamics}\label{sec:gravity_hydro_coupling}

The effect of gravity on the hydrodynamical evolution is typically incorporated by the use of a source term for the momentum and energy equations. The integral form of the momentum source term is
\begin{equation}
  \int \rho \mathbf{g}\, dV
\end{equation}
and for the energy source term it is
\begin{equation}
  \int \rho \mathbf{u}\cdot\mathbf{g}\, dV \label{eq:cell_center_gravity_source}.
\end{equation}
In most hydrodynamics codes these are discretized as $\rho\, \mathbf{g}\, \Delta V$ and $\rho\, \mathbf{u}\,\cdot\mathbf{g}\, \Delta V$, respectively, where $\Delta V$ is the zone volume and $\rho$, $\mathbf{u}$, and $\mathbf{g}$ are evaluated at the zone center. As observed by \cite{arepo}, the essential problem with this approach is that fluid movements in a grid code are calculated using fluxes at zone boundaries, while the standard gravity source term is an estimate of the flux using the zone center quantities. This can lead to a striking mismatch when the gradient across a zone is large. We have found that this is especially important at the edge of a star, where the large density gradient means that the momentum and energy fluxes can transition sharply even across half of a zone width. Another problem is that in a merger calculation in which two stars are moving on a grid, large quantities of material are being advected, so even a slight error in the calculation for the high-density material can manifest itself as a long-term error in energy conservation. This becomes apparent in Section \ref{sec:kepler}. So while a closer look at this may be unwarranted for situations like a stationary single star central explosion calculation, a detailed discussion is certainly called for in the case of an orbiting binary system where getting the gravity right is absolutely necessary for fully trusting the long-term evolution of the system.

\citet[Chapter 4]{shu:1992} observes that it is possible to describe the source term for the momentum equation by taking the divergence of a gravitational stress tensor,
\begin{equation}
  G_{ij} = -\frac{1}{4\pi G}\left(g_i g_j - \frac{1}{2}|\mathbf{g}|^2\delta_{ij}\right).
\end{equation}
The momentum equations are then written explicitly in conservative form when discretized: just as with the momentum stress tensor, divergences are calculated using discretized fluxes at zone boundaries, and the flux at any zone boundary is added to one cell and subtracted from another, so that the total momentum stays constant to within numerical roundoff error (with the possible exception of domain boundaries). This method works because, from the Poisson equation, $\rho$ can be written as proportional to the divergence of $\mathbf{g}$. We note that in order for the method to be reliable, the expression for all components of $\mathbf{g}$ must be correctly calculated from $\phi$ on zone boundaries so that the total divergence does indeed recover the density. This is straightforward for the component normal to the zone interface, but requires averaging over zones in the perpendicular direction to obtain the transverse components.

Dealing with the energy equation is more difficult. The central challenge is to write down a form of the discretized energy equation that explicitly conserves total energy when coupled to gravity. When gravity is included, the total energy is
\begin{equation}
  \rho E_{\text{tot}} = E_G - \frac{1}{2}\rho\phi = \frac{1}{2}\rho \mathbf{u}^2 + \rho e - \frac{1}{2}\rho\phi,
\end{equation}
where $E_G$ is the gas energy from the pure hydrodynamics equation, and the factor of $1/2$ in the gravitational energy term is necessary for simulations with self-gravity to prevent double-counting of interactions. Recently, three different approaches have been proposed for achieving this. We will briefly examine these and then describe the approach that makes the most sense for us.

\citet{marcello:2012} use the mass continuity equation to rewrite the standard source term from Equation \ref{eq:cell_center_gravity_source}. When this is done, the resulting evolution equation for the total energy is:
\begin{equation}
  \frac{\partial (\rho E_{\text{tot}})}{\partial t} = -\bm{\nabla}\cdot\left(\rho \mathbf{u} E_{\text{tot}} + p\mathbf{u} - \frac{1}{2}\rho\mathbf{u}\phi\right) + \frac{1}{2}\phi\frac{\partial \rho}{\partial t} - \frac{1}{2}\rho\frac{\partial \phi}{\partial t}.
\end{equation}
In practice, we would still actually evolve the total gas energy $E_G$, and so when discretized to second order in time from time level $n$ to time level $n+1$ the gas energy equation would look like
\begin{align}
  (\rho E_G)^{n+1} &= (\rho E_G)^{n} + \left(\frac{1}{2} (\rho \phi)^{n+1} - \frac{1}{2} (\rho \phi)^{n}\right) - \Delta t\, \bm{\nabla}\cdot\left(\rho \mathbf{u} E_{\text{tot}} + p\mathbf{u} - \frac{1}{2}\rho\mathbf{u}\phi\right) \notag \\
   &+ \frac{\Delta t}{2}\left(\phi \frac{\partial \rho}{\partial t}\right)^{n+1/2} - \frac{\Delta t}{2} \left(\rho \frac{\partial \phi}{\partial t}\right)^{n+1/2}.
\end{align}
It is not immediately obvious that this is explicitly conservative for the total energy, due to the last two terms on the right-hand side, but written in discrete form the potential is linked to the density in such a way that there is indeed a perfect cancellation of terms between adjacent cells (see Equation 88 in \citeauthor{marcello:2012}). Doing it this way requires a prescription for calculating $\phi^{n+1/2}$, the potential at the time halfway between the new and old state, as well as $\dot{\phi}^{n+1/2}$, the time derivative of the potential at that time. To second order in time, these can be calculated as $(\phi^{n+1} + \phi^{n})/2$ and $(\phi^{n+1}-\phi^{n})/\Delta t$ respectively. So, in practice what we do is to first solve the Poisson equation using $\rho^n$ at the beginning of a time step; then, evolve the pure hydrodynamics equations without the gravitational source to obtain $\rho^{n+1}$; then, solve the Poisson equation for $\phi^{n+1}$; and then to finally calculate the gravitational source terms and add them in. An alternative approach is to use the predictor-corrector scheme that CASTRO features, where the time level $n$ source term is included in the hydrodynamics update and then left out of the final update. In either case, the time level $n$ calculation of $\mathbf{g}$ is included the PPM reconstruction of the edge states.

\citet{jiang:2013} start from the motivation of extending to the energy equation the flux-based formalism that allows explicit conservation for the momentum equation. They determine that the energy is perfectly conserved if we define a gravitational flux $\mathbf{F_g}$ and take its divergence for the energy equation:
\begin{align}
  \mathbf{F_g} &= \frac{1}{8\pi G}\left(\phi\bm{\nabla}\dot{\phi} - \dot{\phi}\bm{\nabla}\phi\right) + \rho\mathbf{u}\phi \label{eq:jiang_flux}\\
  \frac{\partial (\rho E_G)}{\partial t} &= -\bm{\nabla} \cdot \left(\rho\mathbf{u}E_G + p\mathbf{u} + \mathbf{F_g}\right)
\end{align}
This is analytically equivalent to the approach by \citet{marcello:2012}, which can be seen by taking the divergence of the flux and using the Poisson equation. It would be justified to use this form simply because it is cleaner to write down, but there is a more important benefit to using an explicit flux based formula: in AMR, conservation between levels can only happen if all fluid movements are written down using fluxes, because it is those fluxes that are adjusted at coarse-fine interfaces to ensure conservation. In this form we can simply add the gravitational source term to the hydrodynamical fluxes and use the existing AMR algorithm to guarantee conservation between levels. We note that \citeauthor{jiang:2013} say that this method requires the solution of an additional Poisson equation to obtain the intermediate time gravitational potential, but as we pointed out above, this can be done to second order simply by taking a centered difference using the existing Poisson solves, so no extra Poisson solve is necessary.

This flux-based formalism has a significant drawback for our problem: unlike \citeauthor{jiang:2013}, we use open hydrodynamic boundary conditions, not periodic boundary conditions. Mass, momentum and energy can be lost (or gained) across the domain boundary. This is not a serious source of error in the standard cell-centered discretization, because the source term is proportional to $\rho$ and the density near the domain boundary is generally filled with a very low density ambient medium, so the size of the source term is always small. However, the gravitational flux in Equation \ref{eq:jiang_flux} has terms that are not explicitly proportional to density. Consequently, the flux lost across the domain boundaries can be significant (though this effect diminishes with increasing resolution). A related problem is that at the edge of a star, the gravitational flux can be so large that the gas energy becomes negative after an update. This effect is greatly diminished if the flux is instead proportional to density, as in the standard method. This is not a major problem for ideal or polytropic gases in pure hydrodynamics problems, but in our simulations we want the internal energy (and thus the temperature) to always be reliable so that nuclear reactions occur at the correct rates. This is especially sensitive at the edges of the stars, where the burning is expected to occur in a violent merger scneario.

A possible compromise approach is discussed in \cite{arepo}. We start with the integral formulation of the energy source term (Equation \ref{eq:cell_center_gravity_source}), but instead of discretizing at the cell center, we use Gauss' theorem to evaluate the source term at the zone boundaries (see Equations 91--96 of that paper; we exclude the term related to the motion of the zones since our grids are fixed). This results in a gravitational update for zone $i$ written as
\begin{equation}
  \Delta E^{\text{grav}}_{i} = -\frac{1}{2}\sum_{j} \Delta m_{ij}\, \mathbf{r}_{ij} \cdot \bm{\nabla}\phi_i \approx -\frac{1}{2}\sum_{j} \Delta m_{ij}\left(\phi_i - \phi_j\right),
\end{equation}
where the sum is over all $j$ zones that share a boundary with zone $i$. Here $\mathbf{r}_{ij}$ is the radius vector joining zones $i$ and $j$ and $\Delta m_{ij}$ is the mass exchanged between zones $i$ and $j$ in the hydrodynamics update, and is equal to $\Delta t A_{ij} F_\rho^{ij}$, where $A_{ij}$ is the area of the zone boundary and $F_{\rho}^{ij}$ is the mass flux from the mass continuity equation. It is straightforward to see that the second version is explicitly conservative. The benefit of this form is that the mass exchanged between zones near the domain boundary will always be small in absolute terms since these zones are at very low density. Thus the energy lost through the boundary will be of comparable size to the standard method, but the gravity update should be more accurate since the change in gravitational energy now tracks the explicit motion of mass on the grid.

% Now rewrite this so that it is in flux form

\subsubsection{Boundary Conditions}\label{sec:gravity_boundary_conditions}

Analytic solutions to the Poisson equation customarily assume that the potential vanishes at large distances from the region of non-zero density. However, on a finite computational domain it is usually not possible to have the edges of the domain be far enough away that the potential can be taken to be zero there. Solving the Poisson equation therefore requires knowledge of the values of the potential on the edges of the computational domain. In principle, they can be computed by doing a direct sum over the mass distribution inside the domain, where the mass in each zone is treated as a point source:
\begin{equation}
  \Phi_{\text{lmn}} = G\, \sum_{\text{i, j, k}} \frac{G \rho_{\text{ijk}}}{|\mathbf{x}_{\text{lmn}} - \mathbf{x}_{\text{ijk}}|}\, \Delta V_{\text{ijk}}.\label{direct_sum}
\end{equation}
Here (i, j, k) are the indices of cells inside the domain, and (l, m, n) are the indices of boundary locations. $\Delta V$ is the volume of the zone. We have implemented this as an option \footnote{It is controlled with the \texttt{gravity.direct\_sum\_bcs} input parameter.} in CASTRO. If there are $N$ zones per spatial dimension, then there are $6 N^2$ boundary zones, and each boundary zone requires a sum over $N^3$ zones, so the direct computation of the boundary conditions scales as $N^5$.  This method is expensive enough that it is not used for hydrodynamics simulations (though it is useful for comparison to approximate solutions).

In a typical simulation we place the boundaries of the domain far enough away from the region containing most of the mass that some method of approximation to this direct summation is justified. Many approaches exist in the literature. The original release of CASTRO featured the crudest possible approximation: a monopole prescription, where the boundary values were computed by summing up all the mass on the domain and treating it as a point source at the domain center. This is exactly correct only for a spherically symmetric mass distribution, and therefore is best suited for problems like single-star Type Ia supernova simulations that employ self-gravity. However, for a problem like that of a binary star system with significant departures from spherical symmetry, this assumption fails to produce accurate boundary values. This results in a significant drift of the center of the mass of the system over time, which will be discussed in Section \ref{sec:kepler}.

The most natural extension of the monopole prescription is to include higher-order multipole moments. If the entire mass distribution is enclosed, then the potential can be expanded in a series of spherical harmonics $Y_{lm}$:
\begin{equation}
  \Phi(\mathbf{x}) = \sum_{l=0}^{\infty}\sum_{m=-l}^{l} \frac{4\pi}{2l + 1} q_{lm} \frac{Y_{lm}(\theta,\phi)}{r^{l+1}}, \label{spherical_harmonic_expansion}
\end{equation}
where $q_{lm}$ are the so-called multipole moments. The origin of the coordinate system is taken to be the center of the computational domain, and $r$ is the distance to the origin. The multipole moments can be calculated by expanding the Green's function for the Poisson equation as a series of spherical harmonics.
\begin{comment}
, which yields
\begin{equation}
  q_{lm} = \int Y^*_{lm}(\theta^\prime, \phi^\prime)\, {r^\prime}^l \rho(\mathbf{x}^\prime)\, d^3x^\prime. \label{multipole_moments_original}
\end{equation}
\end{comment}
After some algebraic simplification of Equation \ref{spherical_harmonic_expansion} using the addition theorem for spherical harmonics,
\begin{comment}
\begin{align}
  &\frac{4\pi}{2l+1} \sum_{m=-l}^{l} Y^*_{lm}(\theta^\prime,\phi^\prime)\, Y_{lm}(\theta, \phi) = P_l(\text{cos}\, \theta) P_l(\text{cos}\, \theta^\prime) \notag \\
  &\ \ + 2 \sum_{m=1}^{l} \frac{(l-m)!}{(l+m)!} P_{l}^{m}(\text{cos}\, \theta)\, P_{l}^{m}(\text{cos}\, \theta^\prime)\, \left[\text{cos}(m\phi)\, \text{cos}(m\phi^\prime) + \text{sin}(m\phi)\, \text{sin}(m\phi^\prime)\right].
\end{align}
\end{comment}
the potential outside of the mass distribution can be written as:
\begin{equation}
  \Phi(\mathbf{x}) = \sum_{l=0}^{\infty} \left[Q_l^{(0)} \frac{P_l(\text{cos}\, \theta)}{r^{l+1}} + \sum_{m = 1}^{l}\left[ Q_{lm}^{(C)}\, \text{cos}(m\phi) + Q_{lm}^{(S)}\, \text{sin}(m\phi)\right] \frac{P_{l}^{m}(\text{cos}\, \theta)}{r^{l+1}} \right].\label{multipole_potential}
\end{equation}
$P_l(x)$ are the Legendre polynomials and $P_{l}^{m}(x)$ are the associated Legendre polynomials. $Q_l^{(0)}$ and $Q_{lm}^{(C,S)}$ are variants of the multipole moments that involve integrals of $P_l$ and $P_l^m$, respectively, over the computational domain.
\begin{comment}
\begin{align}
  Q_l^{(0)}   &= \int P_l(\text{cos}\, \theta^\prime)\, {r^{\prime}}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_{lm}^{(C)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{cos}(m\phi^\prime)\, {r^\prime}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_{lm}^{(S)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{sin}(m\phi^\prime)\, {r^\prime}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime.
\end{align}
\end{comment}
This approach becomes computationally feasible when we cut off the outer summation in Equation \ref{multipole_potential} at some finite value of $l_{\text{max}}$. If it is of sufficiently high order, we will accurately capture the distribution of mass on the grid. In practice we first evaluate the discretized analog of the modified multipole moments for $0 \leq l \leq l_{\text{max}}$ and $1 \leq m \leq l$, an operation that scales as $N^3$. We then directly compute the value of the potential on all of the $6N^2$ boundary zones. Since the multipole moments only need to be calculated once per Poisson solve, the full operation scales only as $N^3$. The amount of time required to calculate the boundary conditions will be directly related to the chosen value of $l_{\text{max}}$, so there is a trade-off between computational expense and accuracy of the result.

% Insert multipole vs. exact calculation here

\subsubsection{Convergence Testing}\label{sec:gravity_convergence_testing}

Since the results of a merger simulation depend strongly on gravity, it is important to check whether proper numerical convergence is achieved for the Poisson solver. To do so, we created a simple test that inserts a sphere of radius $R$ and uniform mass density $\rho$ onto our grid, and used CASTRO to calculate the gravitational potential $\phi$ of this setup. We ensure that $R$ is an integer multiple of the grid spacing, and the center of the sphere is at the origin. The problem domain for our simulations is $[-1.6, 1.6]^3$, and we take $R = 0.5$ and $\rho = 10^3$. The zones with $r > R$ are filled with an ambient material of very low density. We run this problem at multiple resolutions corresponding to jumps by a factor of two. For comparison, at each grid point we evaluate the analytical potential of a uniform sphere, which can be easily determined using Gauss' law:
\begin{equation}
  \phi_{\text{sphere}}(r) = \frac{GM}{r} \times \begin{cases} (3R^2 - r^2)/(2 r^2) & r \leq R \\ 1 & r > R \end{cases},\label{eq:sphere-analytical}
\end{equation}
where $M = 4\pi R^3 / 3$ is the mass of the sphere. Defining the $L^2$ norm of a field $f$ as
\begin{equation}
  L^2(f) = \left(\sum_{i,j,k} dx\, dy\, dz\, f_{ijk}^2\right)^{1/2},
\end{equation}
we measure the numerical error by calculating the $L^2$ norm of the error and normalizing it by the $L^2$ norm of the analytical solution:
\begin{equation}
  \text{Error} = \frac{L^2(\phi - \phi_{\text{sphere}})}{L^2(\phi_{\text{sphere}})}.
\end{equation}

\begin{figure}
  \label{fig:gravity_convergence}
\end{figure}

The results of this test are plotted in Figure \ref{fig:gravity_convergence}. We find that convergence is actually substantially better than second-order, and that the difference is largest at low resolutions. The explanation for this is that we are attempting to model a spherical object on a rectangular grid. At very low resolution, the object does not look very spherical. As the resolution is increased, the total amount of mass on the grid, as well as its location, will change as the sphere fills out. This means we are combining the true accuracy bonus from increased resolution with the artificial accuracy bonus from getting closer to solving the problem we are supposed to be solving.

We can solve one of these two sources of error by evaluating Equation \ref{eq:sphere-analytical} with a mass $M$ that corresponds to the amount of mass actually on the grid, defined as the sum of the density multiplied by the volume of each cell for all cells that have greater than ambient density. The resolution study for this case is also plotted in Figure \ref{fig:gravity_convergence}. We still obtain convergence better than second-order, indicating that we still have the geometrical problem.

The only way to fully eliminate this effect is to test a problem that does not change with resolution. The obvious companion is a cube of uniform density $\rho$, where now $R$ is half of the side length of the cube. At each resolution we use the same $R$ as for the sphere, which ensures that the cube always fills exactly the same fraction of the domain and thus has the same mass, so the only improvement comes from better sampling at higher resolution. The potential for this object has been worked out analytically by \cite{waldvogel:1976} (see also a similar result in \cite{hummer:1996}). The potential is in Equation 15 of that paper, though the last term is missing a factor of $1/2$, which destroys the symmetry. Inserting this missing factor and performing a simple coordinate transformation so that the center of the cube is at the origin, the potential is
\begin{align}
  \phi_{\text{cube}}(x,y,z) &= G\rho\sum_{i,j,k=0}^1\left(x_i y_j\, \text{tanh}^{-1}\left(\frac{z_k}{r_{ijk}}\right) + y_j z_k\, \text{tanh}^{-1}\left(\frac{x_i}{r_{ijk}}\right) + z_k x_i\, \text{tanh}^{-1}\left(\frac{y_j}{r_{ijk}}\right) \right.\notag \\
  &\left. - \frac{x_i^2}{2}\,\text{tan}^{-1}\left(\frac{y_j z_k}{x_i r_{ijk}}\right) - \frac{y_j^2}{2}\,\text{tan}^{-1}\left(\frac{z_k x_i}{y_j r_{ijk}}\right) - \frac{z_k^2}{2}\,\text{tan}^{-1}\left(\frac{x_i y_j}{z_k r_{ijk}}\right)\right)
\end{align}
where $x_0 = R + x$, $x_1 = R - x$, $y_0 = R + y$, $y_1 = R-y$, $z_0 = R+z$, $z_1 = R-z$, and $r_{ijk} = \sqrt{x_i^2 + y_j^2 + z_k^2}$. In \texttt{FORTRAN} and \texttt{C}, the inverse hyperbolic tangent is \texttt{atanh} and the inverse tangent is \texttt{atan} (\textit{not} \texttt{atan2}). This formula is valid both inside and outside the cube. The normalized $L^2$ error norm for this problem is also shown in Figure \ref{fig:gravity_convergence}, and indeed we see near-perfect second-order scaling.

The main lesson here is that in a convergence study, it is important to ensure that the physical problem does not change with resolution. Since in the case of spherical objects on rectangular grids the effect is to artificially boost convergence with resolution, in a simulation with spherical objects like stars one can envision a scenario of being fooled into believing apparently good convergence results that are simply a convolution of artificially high gravitational convergence and poor convergence in the hydrodynamics. A convergence study in this case is only fully valid if there is reason to be confident that this effect is negligible compared to other factors.

Another factor that may affect convergence testing is easily seen in Figure \ref{fig:gravity_convergence}, where at the highest resolution the error flattens out. This occurs because of a limitation in the accuracy of the multigrid solver. For problems of this size we find empirically that the multigrid solver does not converge for error tolerances smaller than about $10^{-11}$, which enforces a lower limit on the total error. This is also something to be aware of in assessing convergence for our full simulations.

\subsection{Rotation}\label{sec:rotation}

For the evolution of binary systems, it is most natural to evolve the two stars in a frame that is co-rotating at the same period as the orbital period. CASTRO has the ability to evolve systems in a rotating reference frame, as discussed in the original code paper. Source terms corresponding to the Coriolis and centrifugal force terms are added to the momentum and energy equations. In this frame, the stars essentially remain stationary in their original positions due to the centrifugal force supporting against the gravitational attraction, and will remain this way as long as significant mass transfer does not occur. \cite{swc:2000} demonstrated (in the context of neutron star mergers) that conservation of angular momentum is much easier to obtain in the rotating reference frame than in an inertial frame in which stars advect large amounts of material around the domain. We wish to emphasize that although it is commonly stated in the literature that grid-based codes poorly conserve angular momentum, this is not generally true. When the resolution is sufficiently high and a rotating reference frame is employed, excellent conservation properties can result, as demonstrated in Section \ref{sec:kepler}. We note that as the stars begin to coalesce, the rotating reference frame will no longer provide a good approximation to the spatial motion of the stars and then they will begin to significantly move around the domain. This is not necessarily problematic because the most important feature of the rotating frame is that it helps ensure that the initial coalescence is not the result of spurious numerical loss of angular momentum. When significant mass transfer sets in and evolution proceeds on a dynamical timescale, the conservation properties may be slightly worse but angular momentum conservation is also less important.

%==========================================================================
% Initial Models and Problem Setup
%==========================================================================

\section{Initial Models and Problem Setup}
\label{sec:initial_models}

At the start of any full simulation, we generate initial model white dwarfs by integrating the equation of hydrostatic equilibrium, taking the temperature and composition to be constant, and using the general stellar equation of state.  This results in a single non-linear equation to find the density in a zone given the conditions in the zone beneath it:
\begin{equation}
\frac{p_{i+1} - p_i}{\Delta x} = \frac{1}{2} (\rho_i + \rho_{i+1}) g_{i+1/2}.
\end{equation}
This equation is a function of $\rho_{i+1}$ only since the pressure is uniquely determined by the density in this case. $\rho_i$ and $p_i$ are known,  $g_{i+1/2}$ is the gravitational acceleration at the interface between zones $i$ and $i+1$, found by simply adding up all the mass from zones $1, \ldots, i$ to get the enclosed mass, $M_{i+1/2}$, and then setting $g_{i+1/2} = -GM_{i+1/2}/r_{i+1/2}^2$. We solve this equation for $\rho_{i+1}$ using a Newton-Raphson iteration.

We desire to specify the mass of the white dwarf, as well as its temperature and composition. To start the integration off, we therefore need to guess at a central density.  We then do a secant iteration over the entire integration procedure to find the central density needed to yield the desired total mass.  We note that the grid spacing $\Delta x$ is chosen such that it is equal to the zone size on the finest level of our problem, which is known in advance since we specify the domain size and the maximum number of possible levels of refinement.

We map the 1D model onto the 3D Cartesian grid by taking density, temperature, and composition as the independent variables, interpolating these to the cell centers, and then calling the equation of state to initialize the remaining terms.  The interpolation process divides each zone into $n_{\text{sub}}$ sub-zones of equal volume for the purpose of sampling the 1D model, and the sub-zones are added together to obtain the full zone's characteristics. This sub-grid-scale interpolation is useful near the edge of the star, where the density falls off rapidly with radius.

For a single star simulation, the star is simply placed at the center of the computational domain, which we take to be the origin. For a binary star simulation, we take as parameters the mass of the two white dwarfs and the initial orbital period $T$. Using Kepler's third law and assuming a circular orbit, we can then work out the orbital separation $a$:
\begin{equation}
  a = \left(\frac{GM T^2}{4\pi^2}\right)^{1/3}.
\end{equation}
Here $M = M_P + M_S$ is the total mass of the system, where $M_P$ is the specified \textit{primary} mass and $M_S$ is the specified \textit{secondary} mass. The primary WD will always be on the left side of the computational domain for our simulations, and is more massive than the secondary. This reflects the usual terminology in the literature where the primary WD is the accretor and the secondary is the donor. The center of mass is located at the center of the computational domain, and the stars lie along the $x$ axis, so that the primary's center of mass is located at $x = -(M_S / M)\, a$ and the secondary's center of mass is located at $x = (M_P / M)\, a$.

The initial velocity is taken to be zero in if we are in the reference frame that rotates with the WDs, and if we are in the inertial frame the velocity is set equal to the rigid rotation rate corresponding to the specified period $T$.

We do not attempt to enforce equilibrium with an additional relaxation step. This will be an important part of future work in this series, as numerous groups working on binary evolution \citep{swc:2000,motl:2002,rosswog:2004,dan:2011,pakmor:2012:gadget} have commented on the importance of equilibrium initial conditions in determining the evolution of the system. In particular, we plan to compare the self-consistent field method (which finds an equilibrium solution \textit{a priori}) and the method of \cite{rosswog:2004}, which starts with our method and then attempts to guide the evolution toward the equilibrium state using suitably defined damping terms. However, these issues are not considered for the present study, as we are not yet attempting to study the merger process.

%==========================================================================
% Numerical Test Problems
%==========================================================================
\section{Numerical Test Problems}\label{sec:Tests}

Merger simulations face a number of numerical difficulties that are not present in single-degenerate Type Ia and core-collapse supernova simulations. In Section \ref{sec:gravity}, we discussed how the lack of spherical symmetry necessitates a careful look at the gravity solver. There are also hydrodynamical issues: the merger process will result in substantial motion of stellar material across the grid. This bulk motion presents an opportunity for advection errors to build up, and is only partially mitigated by evolving the white dwarfs in a co-rotating frame. It is therefore important to be be aware of the behavior of the code in such circumstances. The behavior of CASTRO for many standard hydrodynamics test problems was detailed in the original code paper \citep{castro}, and in the interest of brevity we will not repeat them here. Instead, we focus on a subset of problems that highlight the special difficulties introduced in merger simulations. These problems couple the hydrodynamics, gravity and equation of state modules. We observe that while in most non-trivial three-dimensional problems this creates a complexity that makes it impossible to determine exact analytical solutions, it is straightforward to devise problems for which certain global properties should obey simple, expected behaviors. Where possible, these should be quantified and a convergence study performed. We recommend that all simulation codes should check their performance on such problems to ensure their reliability when coupling the various physics modules necessary to build a complete and realistic simulation.

\subsection{Maintaining Hydrostatic Equilibrium}\label{sec:HSE}

In Section \ref{sec:initial_models} we describe the process by which we generate initial stellar models. While the 1D models are in hydrostatic equilibrium to within a small error, interpolation onto the 3D Cartesian grid will introduce perturbations into the solution \citep{zingale:2002}. Although we ensure that the initial models are generated with the same equation of state and are as well resolved as our finest grid, there will still be a hydrodynamical error associated with the fact that the rectangular grid cannot faithfully represent a spherical star. Additionally, the gravitational potential obtained by the multigrid solver will differ slightly from the one assumed by the initial model, and the operator splitting between the gravity and hydrodynamics should also result in small errors. As a result, we expect that the star will oscillate slightly about an equilibrium point, but that the amplitude of this oscillation should decrease with increasing resolution.

This problem was studied in the first CASTRO paper, but is worth revisiting here. A single star explosion simulation may only last a couple of seconds, and the CASTRO paper studied the behavior of the star after one second of evolution. However, the dynamical timescale of a typical carbon-oxygen white dwarf is on the order of 1--10 seconds. Additionally, a binary orbit is typically on the order of 10--100 seconds when a merger simulation starts, and with equilibrium initial conditions the system may survive for tens of orbits before the secondary is disrupted. When this does happen, we want to be confident that it was because of the dynamics of the merger process and not because of an instability in an individual star. Our goal here is thus to populate a single star onto our three-dimensional coordinate grid and evolve it for a period of time long enough to assess whether the star is truly stable, and to probe how the size of deviation from equilibrium is affected by grid resolution.

\subsection{Gravitational Free Fall}\label{sec:Gravitational Free Fall}

A simple test to verify the Poisson solver implemented by CASTRO is
the case of gravitational free fall. In this setup, two stars, each
individually in an equilibrium state, are placed on the computational
grid, separated by an initial distance $r_0$ along the $x$ axis with
zero initial velocity. We choose stars of masses $0.6$ and $0.8\,
M_\odot$, with the lower mass star on the left side (e.g. $x < 0$) of
the domain and the higher mass star on the right, such that their
center of mass coincides with the center of the
domain. Gravitationally, the stars may be treated as point masses
until the point of contact, so the equation of motion governing the
distance $r$ between their centers of mass is that of simple free
fall:
\begin{equation}
  \ddot{r}(t) = - \frac{GM}{r},
\end{equation}
where $G$ is the gravitational constant and $M$ is the total mass of
the system. This differential equation has a closed-form solution for
the evolution time as a function of separation:
\begin{equation}
  t(r) = \sqrt{\frac{r_0^3}{2GM}} \left[ \text{arccos}\left(\sqrt{\frac{r}{r_0}}\,\right) + \sqrt{\frac{r}{r_0} \left(1 - \frac{r}{r_0}\right)}\ \right]. \label{analyticalFreeFall}
\end{equation}\MarginPar{This result is derived in the freefall directory.}
This result can be derived by recognizing that
\[
  t(r) = \int_{r_0}^r \frac{dr}{v(r)}
\]
and inserting the velocity as a function of radial separation,
\[
  v(r) = \sqrt{\frac{2GM}{r_0}\left(\frac{r_0}{r} - 1\right)}.
\]
We determine the initial separation to be consistent with the
simulation performed in Section \ref{sec:kepler}; that is, we select
an initial orbital period and use Kepler's third law to calculate the
radius of a circular orbit with that period. We select an initial
orbital period of $T = 100$ s for this simulation, so that the separation is
\[
  r_0 = 3.61 \times 10^{9}\ \text{cm}.
\]
We chose a relatively low resolution simulation to demonstrate the
capabilities of CASTRO even while using modest resources. The
computational grid is covered by a coarse grid of $48^3$ zones, with
two levels of refinement above the coarse grid. Each refined grid
carries an increase in resolution by a factor of 4 relative to the
coarser grid below it. CASTRO initially assigns $93\%$ of the domain
to be covered by the intermediate resolution grids, and $0.05\%$ of
the domain to be covered by the finest resolution grids.

The analytical result in Equation \ref{analyticalFreeFall} determines
the total elapsed free-fall time,
\[
  t_{\text{ff}} = \frac{\pi}{2} \sqrt{\frac{r_0^3}{2GM}} = \frac{T}{4\sqrt{2}}.
\]
The physical radius of each white dwarf is roughly $10\%$ of the
initial separation, so we consider the evolution terminated when the
radial separation reaches that value (at that point, the stars will no
longer be in free-fall due to physical contact). Since the elapsed
time goes roughly as the square root of the distance for small $r$, we
consider the evolution terminated when $t = 0.99\, t_{\text{ff}}$. The
results of our simulation are shown in Figure \ref{Fig:Free Fall}. The
positions of the two stars are determined by calculating the center of
mass of the right $(x > 0)$ and left $(x < 0)$ sides of the domain at 
the end of each time step, and treating the centers of mass as the 
respective location of the two stars.

\subsection{Galilean Invariance}\label{sec:galileo}

It is often stated in the literature that Eulerian methods for hydrodynamics with grids fixed in space do not obey the Galilean invariance of the underlying Euler equations, so that simulations moving at a uniform bulk velocity will appear different than an equivalent stationary simulation. If true, this means that codes such as CASTRO may be fundamentally inappropriate for merger problems (though any errors should be greatly diminished in a reference frame that rotates with the initial binary orbit). Recently, this has come up in two ways which are of note for us in the present study. We will explain these situations and run tests to determine whether this actually is a significant concern.

\citet{arepo} performed a Kelvin-Helmholtz instability test and showed that (at low resolution) a fixed-grid code failed to develop the expected fluid instability when the whole fluid was moving at a strongly supersonic uniform velocity. This contrasted with the results of the moving-mesh code AREPO being presented in that study, which demonstrated Galilean invariance even at large bulk velocities. If verified, this claim would have important consequences on how much we can trust the ability of CASTRO to test the violent merger progenitor model. Shearing between the material flowing out of the secondary and material near the surface of the primary may trigger fluid instabilities that play an important role in the evolution of that gas, which is the site of the initial detonation in the prompt explosion model. \citet{guillochon:2010} showed for their simulation that Kelvin-Helmholtz instabilities produced this way may raise the temperature of the accreting material enough to ignite a detonation. Therefore if we are not correctly reproducing the characteristics of the Kelvin-Helmholtz instability in the case where there is significant mass motion on the grid, we cannot be confident that a detonation (or lack thereof) is not numerically seeded. However, \citet{robertson:2010} observe that Galilean invariance of simulation results for the Euler equations occurs only because of truncation error in the discretization of the fluid equations. This takes the form of a numerical diffusion term which is dependent on velocity (and also resolution). The advantage of a moving-mesh code is that the mesh everywhere moves with the local flow velocity, which substantially reduces the numerical diffusion. \citeauthor{robertson:2010} argue that the differences seen between the moving-mesh and fixed-grid code are caused by the interaction of this numerical diffusion with small-scale instabilities (which may be physical or numerical) which couple with and fundamentally alter the large-scale modes. Small-scale instabilities are seeded in this problem by the choice of a sharp initial discontinuity between the fluids. Crucially though, \citeauthor{robertson:2010} point out that this problem does not converge with resolution and so it is not possible to know the correct behavior of this problem. As such, we do not know whether the small-scale modes found in AREPO are real, so the problem is not useful in formally discriminating between methodologies. They instead propose an alternate test with a smoother initial contact. This converges to the same solution qualitatively in both the stationary and bulk velocity cases, indicating that the code does generally maintain Galilean invariance (to some specified error that depends on resolution and the uniform flow speed).  We will see whether we can reproduce this result, as well as a result by \cite{wadsley:2008}, who used the FLASH code to simulate a hot bubble subject to mixing by the Kelvin-Helmholtz instability, and also found that the mixing was affected by a uniform bulk velocity.

A related question is whether our code reliably simulates the bulk motion of the stars across the grid, and whether such bulk motion affects the stability of the star. This concern is prompted by the study of \cite{tasker:2008}, who studied the effect of uniform translation on the stability of a spherically symmetric model for a galaxy cluster. They compared the radial profile of the cluster at initialization and after a period of time evolution. Using FLASH and ENZO, they found that a static cluster retains its shape at high enough resolution, while uniform translation of the cluster causes mixing of the core material due to numerical diffusion which results in an underestimation of the core's true density. The SPH codes they used did a better job maintaining the core density. We will perform a variant of this test using white dwarf models.

\subsubsection{Kelvin-Helmholtz Instability}\label{sec:khi}

Following \cite{robertson:2010}, we set up a Kelvin-Helmholtz test in the following way. The problem domain runs from 0 to 1 in both the $x$ and $y$ directions. Though this is a two-dimensional test, we run CASTRO in 3D and replicate the problem along 8 cells in the $z$ direction; the physical size of these zones is determined so that $dz = dx = dy$, a requirement at present in CASTRO. The problem involves a fluid slab of density $\rho_1 = 2.0$ traveling rightward in the $x$-direction at velocity $v_1 = 0.5$, sandwiched by a fluid of density $\rho_2 = 1.0$ traveling leftward at velocity $v_2 = -0.5$. The density gradient is in the $y$ direction, so this creates a velocity shear along the interface between the fluids. The density and velocity distribution on the computational domain are given by:
\begin{align}
  \rho &= \rho_1 + R(y)\left[\rho_2 - \rho_1\right] \\
  v_x  &= v_1 + R(y)\left[v_2 - v_1\right] \\
  v_y  &= v_{\text{bulk}} + v^\prime
\end{align}
Here $R(y)$ is a ramp function that describes the transition between the two fluids, while $v_{\text{bulk}}$ is the bulk motion of the fluid in the $y$ direction and $v^\prime$ is the velocity perturbation that seeds the instability. The problem will be established for two sets of initial conditions (ICs), which we follow \citeauthor{robertson:2010} in calling ICs A and B. They differ in their ramp function ($R_A$ and $R_B$ respectively), as well as the initial perturbation ($v^\prime_A$ and $v^\prime_B$ respectively), and the frequency of the perturbation ($n_A = 4$ and $n_B = 2$):
\begin{align}
  R_A &= \begin{cases} 0 & |y - 0.5| > 0.25 \\ 1 & |y - 0.5| < 0.25 \end{cases} \\
  R_B &= \Big\{\left[1 + \text{exp}(-2(y-0.25)/\Delta_y)\right]\left[1 + \text{exp}(2(y-0.75)/\Delta_y)\right]\Big\}^{-1} \\
  v^\prime_A &= w_0\, \text{sin}\left(n_A\, \pi\, x\right) \left\{\text{exp}\left[-\frac{(y-0.25)^2}{2\sigma^2}\right] + \text{exp}\left[-\frac{(y-0.75)^2}{2\sigma^2}\right]\right\} \\
  v^\prime_B &= w_0\, \text{sin}\left(n_B\, \pi\, x\right).
\end{align}
Here $w_0 = 0.1$ is the scale of the velocity perturbation, $\sigma = 0.05/\sqrt{2}$ controls the width of the Gaussian for IC A, and $\Delta_y$ is the transition distance scale for the smooth ramp of IC B. The pressure everywhere is set to $p = 2.5$, and we run this with a gamma-law equation of state set to $\gamma = 5/3$. Plotfiles are generated every 0.01 seconds, and the problem is run until $t = 2$.

We run the problem for $v_\text{bulk} = [0, 1, 3, 10, 30, 100]$, and for each set of initial conditions run the problem at resolutions of $64^2$, $128^2$, $256^2$, $512^2$. In addition, for each initial condition we run a simulation at the very high resolution of $2560^2$ for the stationary problem only. This serves as a reference solution to gauge the extent to which the bulk flow affects the development of the fluid instability.

\subsubsection{Hot Bubble}\label{sec:hot_bubble}

\subsubsection{Moving Star}\label{sed:moving_star}

\subsection{Keplerian Orbit}\label{sec:kepler}

should show:

effect of resolution

inertial vs. rotating frame

effect of ppm\_reference and gravity update type

effect of boundary conditions (size of domain and possibly multipole dependence)


%==========================================================================
% Performance
%==========================================================================
\section{Performance}\label{sec:Performance}

strong scaling plot

space filling curve?

OMP vs. MPI?

breakdown by component?


%==========================================================================
% Conclusions
%==========================================================================
\section{Conclusions and Discussion}\label{sec:Conclusions and Discussion}


\acknowledgments

This research was supported by NSF award AST-1211563.  This research
used resources of the National Energy Research Scientific Computing
Center, which is supported by the Office of Science of the
U.S. Department of Energy under Contract No. DE-AC02-05CH11231.  An
award of computer time was provided by the Innovative and Novel
Computational Impact on Theory and Experiment (INCITE) program.  This
research used resources of the Oak Ridge Leadership Computing Facility
located in the Oak Ridge National Laboratory, which is supported by
the Office of Science of the Department of Energy under Contract
DE-AC05-00OR22725. Project AST006 supported use of the ORNL/Titan resource. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575. Project AST100037 supported use of the resources NICS/Kraken and NICS/Darter.

%TODO: add blue waters citation

\clearpage

\bibliographystyle{apj}
\bibliography{refs}


\clearpage
\appendix

\section{Castro hydrodynamics changes}

Summary of changes:
\begin{itemize}
\item New reference state/flattening fix: helps energy conservation a
  lot, also fixes some undershoot/overshoots

\item New gravity update type: fixes some energy conservation

\item temperature-based PPM: fixes temperature floors

\item changing I's to use limit of the parabola instead of the
  cell-center: not sure if this does much

\item Colella \& Glaz Riemann solver: does a much better job with
  temperature in strong shocks

\item Tracing of gravity: seems to help with the material at the edge
  of the star

\item implicit update of the rotation terms?  (I coded this up once, but
  it is not currently in the code)
\end{itemize}

We use the Castro code as described in \citet{castro}.  For all the
runs, the PPM reconstruction is done, using the original limiters for
the parabolic profiles \citep{ppm}.  We modify the prediction of the
interface states slightly.  The original implementation of the PPM
prediction in Castro takes the form:
\begin{equation}
q_{i+1/2,L}^{n+1/2} = q_i -
   \sum_{\nu;\lambda_i^{(\nu)}\ge 0} l_i^{(\nu)} \cdot \left [
        q_i - \mathcal{I}_+^{(\nu)}(q_i)
       \right ] r_i^{(\nu)}
\end{equation}
where $q$ is the vector of primitive variables, with $q_i$
representing the average in the cell, $l^{(\nu)}$ and $r^{(\nu)}$ are
the left and right eigenvectors with eigenvalue $\lambda^{(\nu)}$,
with $\nu$ the index of the characteristic wave of the system.  The
sum is over all the waves that result from the characteristic
structure of the problem, but designed such that only waves moving
toward the interface contribute to the interface value,
$q_{i+1/2,L}^{n+1/2}$.  Finally, $\mathcal{I}_+^{(\nu)}(q)$ is the
average under the parabolic profile of quantity $q$ of all the information
that can reach the right interface of the zone $i$ as carried by the wave
$\nu$.   The reader is referred to
\citet{ppmunsplit} for further defaults.

\subsection{Reference States}

The presence of $q_i$ in this expression serves as a reference state.
The idea is that the error in the characteristic projection of the
jump (due to the nonlinearity) is minimized if we pick a suitable
reference state, since we only care about the jumps carried to the
interface over the timestep \citep{colellaglaz1985}.  The original
Castro implementation used the cell-average quantity.  Experiments
found that this is ill-behaved in the presence of strong shocks
(especially at low resolution). For the present work, we switch the
reference state to
\begin{equation}
\label{eq:refchoice}
\tilde{q}_L = \left \{ \begin{array}{cc}
       \mathcal{I}_+^{(+)}(q_i) & \mathrm{if~} u + c > 0 \\
       q_i                    & \mathrm{otherwise}
\end{array}
\right .
\end{equation}
where the $(+)$ superscript here means the fastest wave moving to the right
(the $u+c$ eigenvalue).   This is simply the average under the largest
portion of the parabolic profile that could possible reach the interface 
over the timestep.  This is
in agreement with \citet{ppmunsplit} (eq. 90).  This makes our
expression appear as:
\begin{equation}
\label{eq:ppmstatel}
q_{i+1/2,L}^{n+1/2} = \tilde{q}_L -
   \sum_{\nu;\lambda_i^{(\nu)}\ge 0} l_i^{(\nu)} \cdot \left [
        \tilde{q}_L  - \mathcal{I}^{(\nu)}_+(q_i)
       \right ] r_i^{(\nu)}
\end{equation}
The final change is that now we can no longer simply multiply
$\tilde{q_i} - \mathcal{I}^{(\nu)}_+(q_i)$ by the flattening
parameter, $\chi$, but instead blend the traced state with the cell
centered state as:
  \begin{equation}
  q_{i+1/2,\{L,R\}}^{n+1/2} \leftarrow (1 - \chi) q_i + \chi q_{i+1/2,\{L,R\}}^{n+1/2}
  \end{equation}
This is the same prescription used in the \cite{ppm}.

This change is now the default in the public version of Castro,
and can be controlled with the parameter {\tt castro.ppm\_reference}.
Figure~\ref{Fig:sod} shows the
solution to the Sod problem using the old and new reference states.
We see that the shock and contact are slightly sharper with this new
reference state, but also there is a slight dip at the tail of the
rarefaction with the new method.

It is instructive to look at the effect of the reference state choice.
First, consider one of the variables present in one-dimensional flow
(density, velocity in the normal direction, and pressure).  Here we
our reference state is as in Eq.~\ref{eq:refchoice} and we have.
Ignoring flattening, if there are no waves moving toward our
interface, then Eq.~\ref{eq:ppmstatel} reduces to:
\begin{equation}
q_{i+1/2,L}^{n+1/2} = \tilde{q}_L = q_i
\end{equation}
If instead only the fastest wave is moving toward the interface, then
only the term corresponding to the fastest wave in the sum will be
added in Eq.~\ref{eq:ppmstatel}, but our choice of reference state makes
that term 0 by design, and our interface state is:
\begin{equation}
q_{i+1/2,L}^{n+1/2} = \tilde{q}_L = \mathcal{I}_+^{+}(q_i)
\end{equation}
This is the desired behavior for each of these cases. 

The details of the reference state for passively advected quantities 
(which includes the transverse velocities in the 1-d tracing) is not
typically discussed.  If we use the same idea of the reference state as
in Eq.~\ref{eq:refchoice}, and consider a quantity $\xi$ which should only
jump across the contact, then our interface state becomes:
\begin{equation}
\xi_{i+1/2,L}^{n+1/2} = \tilde{\xi}_L -
  \underbrace{l_i^\evz \cdot \left [
        \tilde{\xi}_L  - \mathcal{I}^\evz_+(\xi_i)
       \right ] r_i^\evz}_{\text{only if~$u \ge 0$}}
\end{equation}
Again, ignoring flattening, if $u \ge 0$, then we have
\begin{equation}
\xi_{i+1/2,L}^{n+1/2} = \tilde{\xi}_L -
  \left (\tilde{\xi}_L  - \mathcal{I}^\evz_+(\xi_i) \right ) = \mathcal{I}^\evz_+(\xi_i)
\end{equation}
(where we used the fact that the eigenvectors are normalized to 1 and
don't mix in any other states when dealing with passive terms).  This
is the expected behavior---we see a state that is traced only by the
contact wave.  However, if $u < 0$ but $u + c \ge 0$, then we instead get:
\begin{equation}
\xi_{i+1/2,L}^{n+1/2} = \tilde{\xi}_L = \mathcal{I}^\evp_+(\xi_i)
\end{equation}
Here we used the same definition of the reference state and see that our
interface state sees the profile traced under the fastest wave, not the
contact.  This is not the correct behavior for a passively-advected
quantity.  

The fix for passively-advected quantities is to simply ignore the 
idea of a reference state and just test on the speed of the contact
itself, setting:
\begin{equation}
\xi_{i+1/2,L}^{n+1/2} = \left \{ \begin{array}{cc}
       \mathcal{I}_+^\evz(\xi_i) & \mathrm{if~} u  > 0 \\
       \xi_i                    & \mathrm{otherwise}
\end{array}
\right .
\end{equation}



\subsection{Gravity Tracing}

We note a few additional differences between the original PPM
implementation of \citet{ppm} and Castro.  In the original PPM
implementation, the gravitational was reconstructed as a parabola, and
this was traced under to find the forcing that affects the interface
for each wave.  Castro followed \citet{ppmunsplit} which instead adds
$(\Delta t/2)g$ to the interface states at the end of the
reconstruction.  In the current implementation, we do the original
parabolic reconstruction and characteristic tracing of the gravitation
source.  Our system with the source appears as:
\begin{equation}
q_t + A(q) q_x = G
\end{equation}
where $G = (0, g, 0)^T$---i.e. the gravitational source only affects
$u$, not $\rho$ or $p$.  Note that in the PPM paper, they put $G$ on 
the lefthand side of the primitive variable equation, so our signs are
opposite.  Our projections are now:
\begin{equation}
\sum_{\nu; \lambda^\enu \ge 0}l^\enu \cdot (\tilde{q} - \mathcal{I}^\enu_+(q) - \tfrac{\Delta t}{2} G) r^\enu
\end{equation}
for the left state, and
\begin{equation}
\sum_{\nu; \lambda^\enu \le 0} l^\enu \cdot (\tilde{q} - \mathcal{I}^\enu_-(q) - \tfrac{\Delta t}{2} G) r^\enu 
\end{equation}
for the right state.  Since $G$ is only non-zero for velocity, only
the velocity changes.  Writing out the sum (and performing the vector products), we
get:
\begin{eqnarray}
u_{i+1/2,L}^{n+1/2} =
   \tilde{u}_+ 
  &-& \frac{1}{2} \left [
      \left (\tilde{u}_+ - \mathcal{I}_+^\evm(u) - \frac{\Delta t}{2} \mathcal{I}^\evm_+(g) \right ) - 
       \frac{\tilde{p}_+ - \mathcal{I}_+^\evm(p)}{C} \right ] \nonumber \\
  &-& \frac{1}{2} \left [
      \left (\tilde{u}_+ - \mathcal{I}_+^\evp(u) - \frac{\Delta t}{2} \mathcal{I}^\evp_+(g) \right ) +
       \frac{\tilde{p}_+ - \mathcal{I}_+^\evp(p)}{C} \right ]
\end{eqnarray}
These differ from the expression in the PPM paper, where $\Delta t G$,
not $\Delta t/2 G$ is used in the projection, however we believe that
the factor of $1/2$ is correct.  To see this, notice that if both
waves are moving toward the interface, then the source term that is
added to the interface state is $(\Delta t/4) (\mathcal{I}_+^\evm(g) +
\mathcal{I}_+^\evp(g))$ for the left state, which reduces to $(\Delta
t/2) g$ for constant g---this matches the result from Taylor
expanding to the interface at the half-time (as in \citealt{ppmunsplit}).

There is one additional effect of this change---now the gravitational
source is seen by all Riemann solves (including the transverse solves)
whereas previously it was only added to the final unsplit interface
states.  Both methods are second-order accurate.

The tracing of gravity can be controlled in the public version of Castro
with the parmeter {\tt castro.ppm\_trace\_grav}.



\subsection{Gravity Source Correction}

We also modify slightly the correction done to the gravitational
source terms after integrating the system in time.  In the original
Castro paper (Eqs. 19 and 20), the state after the conservative update
due to fluxes and external sources (other than gravity) was
represented with the superscript `$(2,\star)$'.  The gravitational
sources were then corrected, making them centered in time, yielding
the final state at the end of the hydrodynamics step, indicated with
the superscript `$(2)$'.  This appeared as:
\begin{eqnarray}
(\rho {\bf U})^{(2)} &=& (\rho {\bf U})^{(2,\star)} +
   \frac{\Delta t}{2} \left [ (\rho {\bf g})^{(2,\star)} - 
                              (\rho {\bf g})^{(1)} \right ]\\
(\rho E)^{(2)} &=& (\rho E)^{(2,\star)} +
   \frac{\Delta t}{2} \left [ (\rho {\bf U\cdot g})^{(2,\star)} - 
                              (\rho {\bf U\cdot g})^{(1)} \right ]
\end{eqnarray}
Here we make the subtle change of using the corrected ${\bf u}$ in the
correction to the energy.  So our new energy correction equation appears as:
\begin{equation}
(\rho E)^{(2)} = (\rho E)^{(2,\star)} +
   \frac{\Delta t}{2} \left [ (\rho {\bf U\cdot g})^{(2)} - 
                              (\rho {\bf U\cdot g})^{(1)} \right ]
\end{equation}
Both of these are second-order accurate, but we've found that the
latter is slightly better for conserving energy.  This change is now
the default in the public version of Castro (it can be controlled by
{\tt castro.grav\_source\_type}.

Rotation is handled similarly to gravity during the conservative update.
The momentum equation, with rotation sources appears as:
\begin{equation}
\frac{\partial \rho {\bf U}}{\partial t} + \nabla \cdot (\rho {\bf U U} + p) = 
   \rho {\bf g} - 2 \rho {\bf \Omega \times U} 
                - \rho {\bf \Omega \times (\Omega \times U)}
\end{equation}
we take the rotation axis to be in the $z$-direction, ${\bf \Omega} =
\Omega \hat{\bf k}$.  Working out the cross-products, the rotation
forces appear as
\begin{equation}
{\bf F}_\mathrm{rot} = \rho \left (
    \begin{array}{c}
     \phantom{-}2 \Omega v + \Omega^2 x \\
              - 2 \Omega u + \Omega^2 y \\
              0 
    \end{array}
  \right )
\end{equation}
We want to time-center this force in the final conservative update, giving
updates for the $u$ and $v$ components of the velocity as:
\begin{eqnarray}
\frac{(\rho u)^{n+1} - (\rho u)^n}{\Delta t} + A^{(x),n+1/2} &=&
   \frac{1}{2} \left [ (\rho {\bf g}\cdot \hat{\bf i})^n
                      + (\rho {\bf g}\cdot \hat{\bf i})^{n+1} \right ] \nonumber\\&+&
   \frac{1}{2} \left [ (2\rho \Omega v)^n + (2\rho \Omega v)^{n+1} \right ] \nonumber \\&+&
   \frac{1}{2} \left [ (\rho \Omega^2 x)^n + (\rho \Omega^2 x)^{n+1} \right ] \\
\frac{(\rho v)^{n+1} - (\rho v)^n}{\Delta t} + A^{(y),n+1/2} &=&
   \frac{1}{2} \left [ (\rho {\bf g}\cdot \hat{\bf j})^n +
                       (\rho {\bf g}\cdot \hat{\bf j})^{n+1} \right ] \nonumber \\ &+&
   \frac{1}{2} \left [ (-2\rho \Omega u)^n + (-2\rho \Omega u)^{n+1} \right ] \nonumber \\ &+&
   \frac{1}{2} \left [ (\rho \Omega^2 y)^n + (\rho \Omega^2 y)^{n+1} \right ]
\end{eqnarray}
where here the explicit advective term from the Godunov method is represented as $A$ and is already time-centered.  In the Castro algorithm, a full $\Delta t$ of the old-time-level source is first added and later we correct this by subtracting off $\Delta t/2$ of the old-time-level term and adding $\Delta t/2$ of the new-time-level source.  This correction appears as:
\begin{eqnarray}
(\rho u)^{n+1} = (\rho u)^\prime 
   &+& \underbrace{
      \frac{\Delta t}{2} \left [ (\rho {\bf g}\cdot \hat{\bf i})^{n+1}
                               - (\rho {\bf g}\cdot \hat{\bf i})^{n} \right ]
    }_{\equiv \Delta^{g,x}} \nonumber \\
   &+& \frac{\Delta t}{2} \left [ (2\rho \Omega v)^{n+1} 
                                - (2\rho \Omega v)^{n} \right ] \nonumber \\
   &+& \underbrace{
      \frac{\Delta t}{2} \left [ (\rho \Omega^2 x)^{n+1} 
                               - (\rho \Omega^2 x)^{n} \right ]
    }_{\equiv \Delta^{c,x}} \\
%
(\rho v)^{n+1} = (\rho v)^\prime 
   &+& \underbrace{
      \frac{\Delta t}{2} \left [ (\rho {\bf g}\cdot \hat{\bf j})^{n+1}
                               - (\rho {\bf g}\cdot \hat{\bf j})^{n} \right ]
    }_{\equiv \Delta^{g,y}} \nonumber \\
   &+& \frac{\Delta t}{2} \left [ (-2\rho \Omega u)^{n+1} 
                                - (-2\rho \Omega u)^{n} \right ] \nonumber \\
   &+& \underbrace{
      \frac{\Delta t}{2} \left [ (\rho \Omega^2 y)^{n+1} 
                               - (\rho \Omega^2 y)^{n} \right ]
    }_{\equiv \Delta^{c,y}} \\
\end{eqnarray}
For compactness, we define the following:
\begin{eqnarray}
S_x &=& (\rho u)^\prime + \Delta^{g,x} + \Delta^{c,x} 
        - \frac{\Delta t}{2} (2 \rho \Omega v)^n \\
%
S_y &=& (\rho v)^\prime + \Delta^{g,y} + \Delta^{c,y} 
        + \frac{\Delta t}{2} (2 \rho \Omega u)^n 
\end{eqnarray}
and our updates are now a coupled implicit system for the updates of
$u$ and $v$:
\begin{eqnarray}
(\rho u)^{n+1} = S_x + \frac{\Delta t}{2}(2\rho \Omega v)^{n+1} \label{eq:rhouupdate}\\
(\rho v)^{n+1} = S_y - \frac{\Delta t}{2}(2\rho \Omega u)^{n+1} 
\end{eqnarray}
This can be solved analytically:
\begin{equation}
(\rho v)^{n+1} = \frac{S_y - \Delta t \Omega S_x}{1 + \Delta t^2 \Omega^2}
\end{equation}
and then we can find $(\rho u)^{n+1}$ from Eq.~\ref{eq:rhouupdate}.
\clearpage

\begin{figure}
  \centering
  \includegraphics[scale=1.0]{reference}
  \caption{ \label{Fig:sod} Solution to Sod's problem with the original
    reference state and the new reference state, as compared to the
    exact solution.  We note that the new reference state shows a
    slightly sharper shock and contact, but also has a dip at the tail
    of the rarefaction.}
\end{figure}

\clearpage

%\begin{figure}
%  \centering
%  \includegraphics[width=3.1in]{nested2}\hspace{1em}
%  \includegraphics[width=3.1in]{nested3}
%  \caption{\label{fig:2levgrav} Schematic showing the solve for $\phi$ for a 2-level grid.}
%\end{figure}

\clearpage

\begin{figure}
  \centering
  \includegraphics[scale=2.0]{freefall/plot_freefall}
  \caption{Time evolution of two initially stationary white dwarfs,
    mutually attracted to each other by the gravitational force. The
    horizontal axis gives the separation of the white dwarfs, scaled
    to the initial separation, and the vertical axis gives the elapsed
    time of the simulation, scaled to the total elapsed time
    necessarily. The solid curve shows the analytical result,
    calculated from Newtonian mechanics, and the circles show the
    samples from the time evolution with CASTRO. The agreement between
    theory and simulation is excellent, even for a simulation with
    modest resolution.}
  \label{Fig:Free Fall}
\end{figure}


\end{document}

