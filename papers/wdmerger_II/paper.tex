\documentclass[twocolumn,numberedappendix]{../aastex6}

% these lines seem necessary for pdflatex to get the paper size right
\pdfpagewidth 8.5in
\pdfpageheight 11.0in

% for the red MarginPars
\usepackage{color}

% some extra math symbols
\usepackage{mathtools}

% allows Greek symbols to be bold
\usepackage{bm}

% allows us to force the location of a figure
\usepackage{float}

% allows comment sections
\usepackage{verbatim}

% Override choices in \autoref
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

% MarginPars
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{\vskip-\baselineskip\raggedright\tiny\sffamily\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

\newcommand{\msolar}{\mathrm{M}_\odot}

% Software names
\newcommand{\boxlib}{\texttt{BoxLib}}
\newcommand{\castro}{\texttt{CASTRO}}
\newcommand{\microphysics}{\texttt{Microphysics}}
\newcommand{\wdmerger}{\texttt{wdmerger}}
\newcommand{\python}{\texttt{Python}}
\newcommand{\matplotlib}{\texttt{matplotlib}}
\newcommand{\yt}{\texttt{yt}}
\newcommand{\vode}{\texttt{VODE}}
\newcommand{\isoseven}{\texttt{iso7}}
\newcommand{\aproxthirteen}{\texttt{aprox13}}
\newcommand{\aproxnineteen}{\texttt{aprox19}}
\newcommand{\aproxtwentyone}{\texttt{aprox21}}

\begin{document}

%==========================================================================
% Title
%==========================================================================
\title{White Dwarf Mergers on Adaptive Meshes\\ II. Collisions}

\shorttitle{WD Mergers. II. Collisions}
\shortauthors{Katz et al. (2016)}

\author{Max P. Katz\altaffilmark{1}}
\author{Michael Zingale\altaffilmark{1}}
\author{Alan C. Calder\altaffilmark{1,2}}
\author{F. Douglas Swesty\altaffilmark{1}}
\author{Ann S. Almgren\altaffilmark{3}}
\author{Weiqun Zhang\altaffilmark{3}}

\altaffiltext{1}
{
  Department of Physics and Astronomy,
  Stony Brook University, Stony Brook, NY, 11794-3800, USA
}

\altaffiltext{2}
{
  Institute for Advanced Computational Sciences,
  Stony Brook University, Stony Brook, NY, 11794-5250, USA
}

\altaffiltext{3}
{
  Center for Computational Sciences and Engineering,
  Lawrence Berkeley National Laboratory, Berkeley, CA 94720
}

%==========================================================================
% Abstract
%==========================================================================
\begin{abstract}
We consider the collisions of white dwarfs as possible progenitors of Type Ia 
supernovae.

\end{abstract}
\keywords{supernovae: general - white dwarfs}

%==========================================================================
% Introduction
%==========================================================================
\section{Introduction}
\label{sec:introduction}

We consider the collisions of white dwarfs as possible progenitors of Type Ia 
supernovae, using the code \castro\ \cite{castro}.


%==========================================================================
% Numerical Implementation
%==========================================================================
\section{Numerical Methods}
\label{sec:numericalmethods}

The hydrodynamical equations solved in this paper were presented in \citet{wdmergerI}.
In this paper we focus our discussion on our implementation of the nuclear
reaction network and integrator. There are three main areas of concern: first,
specifying what nuclides are in the network and what reaction rates link the
various nuclides; second, once the system of ODEs governing the evolution of
the nuclear species has been written down, how to couple them to ODEs governing
the evolution of the thermodynamics, in particular the temperature; third,
coupling the nuclear reaction updates to the evolution of the hydrodynamical
system. We will deal with each of these in turn in the following subsections.

\subsection{Nuclear Network}
\label{sec:network}

White dwarfs are mainly composed of $\alpha$-chain particles, primarily ${}^4$He,
${}^{12}$C, ${}^{16}$O, ${}^{20}$Ne, and ${}^{24}$ Mg. Therefore the core of
any network appropriate for modeling nuclear burning in white dwarfs will be
these alpha chain nuclides, with the idea being that links up the $\alpha$-chain
will eventually get us to ${}^{56}$Ni, the nuclide responsible for the
energy output of Type Ia supernovae. In this paper we consider four networks
to do this, presented in order of increasing complexity. The most simple is
\isoseven\ \citep{timmes:2000}, which includes all of the aforementioned isotopes and
${}^{28}$Si (see also \citet{hix:1998}). ${}^{28}$Si effectively measures the
equilibrium state of silicon-group elements, and ${}^{56}$Ni effectively measures
the equilibrium state of iron-group elements, with the link between them governed
by the effective loss or gain of seven $\alpha$-particles. This type of network
was used by \citet{rosswog:2009} for their collision calculations in SPH.

Next is \aproxthirteen\ \citep{timmes:1999,timmes:2000}. This includes
all of the isotopes of \isoseven, and all of the $alpha$-chain particles between
silicon and nickel (${}^{32}$S, ${}^{36}$Ar, ${}^{40}$Ca, ${}^{44}$Ti, ${}^{48}$Cr,
and ${}^{52}$Fe). This network was used by \citet{hawley:2012} and \citet{raskin:2010}.
\citet{loren-aguilar:2010} and \citet{garcia-senz:2013} used a very similar network
that included additionally ${}^{60}$Zn. The \aproxnineteen\ network \citep{timmes:1999}
builds on \aproxthirteen\ by including isotopes for hydrogen burning and explicit
tracking of photodisintegration into ${}^{54}$Fe. This network was used by
\citet{kushnir:2013}, \citet{kushnir:2014}, \citet{rosswog:2009} for their
calculations with FLASH, and \citet{papish:2015}. Finally we will also
consider \aproxtwentyone, which includes all of the above plus ${}^{56}$Cr
and ${}^{56}$Fe and related reaction links. The primary virtue of using
the latter two networks is that they allow us to track changes away from
$Y_e = 0.5$.

All four of these networks have been ported into a form that is consistent
with the \boxlib\ codes, in the freely available \microphysics\ code
repository\footnote{\microphysics\ can be obtained at \url{https://github.com/BoxLib-Codes/Microphysics}.},
a collection of microphysical routines that are designed to be used in our
hydrodynamics codes. These can be easily swapped at compile time by using the 
appropriate makefile variable.

\subsection{Nuclear Burning}
\label{sec:burner}

Given a set of nuclides and the reaction links between them, we now consider
how a burning step is performed in our software. 
provided with \castro. To \vode\ we provide an integration state containing
the molar abundances $Y_{n} = X_{n} / A_{n}$, where $X$ is the mass fraction
of the abundance in the zone and $A$ is the number of nucleons. The integration
of
\begin{equation}
  \frac{d\bm{Y}}{dt} = f(\mathbf{Y})
\end{equation}
is determined using a right-hand-side provided by the nuclear burning network.
When it comes to the thermodynamical evolution, the internal energy $e$ of the zone
will change when the nuclear abundances evolve. During the integration we track
the energy release from the system using
\begin{equation}
  \frac{\partial e}{\partial t} = N_A \sum_{n} \frac{\partial Y_{n}}{\partial t} m_{n} c^2,
\end{equation}
where $c$ is the speed of light and $m_n$ is the mass of each nuclide.

In a hydrostatic burn, we keep $\rho$ and $T$ fixed throughout, and use
the energy released at the end to compute a final temperature that is
thermodynamically consistent with the new internal energy. By contrast,
in a self-heating burn, we allow the temperature to evolve in response
to the burning (see\footnote{In the cited paper, a term based on the
thermodynamic chemical potential was included; we now believe
that it is incorrect to include such a term in the burn, since it
automatically sums to zero analytically.} \citet{maestro3}):
\begin{equation}
  \frac{dT}{dt} = \frac{1}{c_V}\frac{\partial e}{\partial t}
\end{equation}
(Although $T$ evolves during the burn so that the integration is physically
accurate, as for the hydrostatic method we discard the final value
for $T$ at the end of the burn and recompute a temperature for the zone that is
consistent with its new internal energy.) Here $c_V$ is the specific heat at
constant volume, which is provided by the equation of state.  During this burn,
we can keep $c_V$ constant using its initial value, or at each step we
can choose to re-evaluate the equation of state using the latest value of $(\rho, T)$.
The latter is more expensive but also more accurate, and we use it in this paper.
In practice we find that the cost is negligible in comparison to the more expensive
parts of the calculation, and it can significantly speed up convergence near NSE.
A third option presented by \citet{raskin:2010} is a so-called ``hybrid'' mode.
In this mode, by default we do a hydrostatic burn. If that burn fails, or if the net
energy change is negative, we do the burn again in self-heating mode. All three options
are implemented in our burner software. The simulations shown in this work all
use the self-heating mode unless otherwise specified.

In our \microphysics\ repository we provide several software options for
solving a set of coupled stiff ODEs. For this work we use an implementation of
the well known variable-order Richardson extrapolation method presented by Stoer and
Bulirsch \cite{stoer:1980}, that is similar to the integrator which ships with
the original versions of the networks mentioned above. Previous work using the \boxlib\
codes typically used the classic \vode\ integrator \cite{vode}. We do maintain a version
of the software that is compatible with our software interfaces in the \microphysics\
repository, but we have largely shifted to integrators which are written in modern
Fortran as a consequence of our efforts to run our codes on GPUs. The Stoer and Bulirsch
integrator we provide satisfies this criterion; we also provide a rewrite of the VODE
BDF algorithm that uses modern Fortran features. The Stoer and Bulirsch algorithm
relies on a uniform relative error tolerance for all of the ODEs in the system, which
we set at $10^{-6}$ for all simulations described here.

\subsection{Coupling to the Hydrodynamics}
\label{sec:hydrocoupling}

In \castro, the reactions are coupled to the hydrodynamics using Strang splitting.
In a given timestep advance $\Delta t$, we first evolve the reactions alone through
a time interval $\Delta t / 2$. Then, we evolve the hydrodynamics for $\Delta t$,
and we evolve the reactions again for a further $\Delta t / 2$. The principal
drawback of this approach is that the reactions and the hydrodynamics can become
decoupled from each other. A common solution to this problem presented in
the literature has been to limit the size of the timestep and thereby limit the
extent of this decoupling \citep{raskin:2010,hawley:2012}, which we adopt here 
and have implemented in \castro. Defining the nuclear energy injection timescale 
$\tau_e$, and the species evolution timescale $\tau_{X_k}$,
\begin{align}
  \tau_e &\equiv \frac{e}{|\dot{e}|} \\
  \tau_{X_k} &\equiv \frac{X_k}{|\dot{X_k}|},
\end{align}
where $\dot{e}$ is an estimate of the time rate of change of the internal energy
from nuclear burning, and $\dot{X_k}$ is an estimate of the time rate of change 
of the mass fraction of the species with index $k$, we define burning-limited 
timesteps $\Delta t_{be}$ and $\Delta t_{bX_k}$:
\begin{align}
  \Delta t_{be} &= f_{e}\, \tau_e \\
  \Delta t_{bX_k} &= f_{X}\, \tau_{X_k}.
\end{align}
Given an estimate for $\dot{e}$, the factor $f_{e}$ determines by what 
fraction we would like to allow the internal energy to change
in the current timestep, under the assumption that $\dot{e}$ does not change from
timestep to timestep. Similarly, given an estimate for $\dot{X_k}$, the factor $f_{X}$ 
determines the maximum change in the mass fraction of any species. By making 
$f_{e}$ and $f_{X}$ smaller, we can control the magnitude of the decoupling 
between the reactions and the hydro. We choose by default $f_{X} = f_{e} = 0.1$, 
a choice which is slightly smaller than others in the literature. (Note that previous 
collision papers we are aware of only limited based on changes in internal energy, 
not on cahnges in species.)  The sensitivity of results to this choice will be discussed in 
\autoref{sec:2D:timestep}. The factors $f_{e}$ and $f_{X}$ can be set at runtime in \castro.

At the start of each advance, we limit the size of the timestep to be the smaller
of the minimum hydro timestep (limited by the CFL condition), and the minimum of all the
burning timesteps across all zones. To do this, we need a method for determining 
$\dot{e}$ and $\dot{X_k}$. A typical choice in the literature has been to set, for example,
\begin{equation}
  \dot{e} = \frac{e^{n} - e^{n-1}}{\Delta t^{n-1}}, \label{eq:burning_mode_4}
\end{equation}
where is $e^n$ is the internal energy at the start of the current timestep and
$e^{n-1}$ is the internal energy at the start of the previous timestep. 
The obvious analogue is used for constructing the species rate of change.
However, there are alternative methods of constructing this derivative estimate, 
and we have found that these different methods have measurable consequences.
We define four separate methods for calculating the time derivative, with 
the above being method 4. Method 3 is similar to method 4 but replaces the
denominator in \autoref{eq:burning_mode_4} with the change in 
internal energy over the last timestep only from the nuclear reactions.
Method 2 is the same as method 3 but we only use the change in internal 
energy from the most recent nuclear burning step, that is, the second-half
of the Strang-split burning from the last timestep (the denominator 
then becomes $\Delta t / 2$). In method 1, the most accurate option and 
the current default in \castro, we evaluate the right-hand-side of the 
burning network given the current state to explicitly obtain the 
instantaneous value of $\dot{e}$ and $\dot{X_k}$. 

To understand the consequences of this choice, and more broadly to 
understand the limitations of Strang splitting, we consider the 
basic outline of a single-level advance in an advection-reaction system:
\begin{enumerate}
  \item Evaluate timestep $\Delta t$ for the current advance
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Advance the hydrodynamics by $\Delta t$
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Return to Step 1
\end{enumerate}
Now, consider that during a head-on collision, initial nuclear burning 
will occur at the contact point between the two stars. Because of 
the staggered updates from splitting, the evolution effectively progresses 
as a cycle between burning for $\Delta t$ and getting fresh material 
advected into the contact point by the hydro update for $\Delta t$. 
When the collision begins, $\Delta t$ is controlled by the hydrodynamic 
stability criterion, and may be large enough that it is possible for 
the burning advance in Step 4 to completely burn the freshly advected 
material all the way to NSE. Consequently the evolution is no longer 
a good approximation to smooth burning of the in-falling material but
rather separate discrete burning and hydro steps, and the nature of 
the burning evolution will be quite different. Furthermore, by the 
time we return to Step 1 and estimate the next timestep size, all 
of the burning rates will be small again, and the instantaneous 
timestep limiter of method 1 may actually substantially overestimate 
the needed timestep. The other methods will see that the energy/species  
substantially changed over the last timestep, but will still 
overestimate the needed timestep because the burning was quiescent
for at least some portion of the last advance. In particular, our 
experience has shown that using only the energy change criterion
is particularly susceptible to this phenomenon; silicon-group 
material can build up without changing the internal energy by a 
large fraction, so the timestep limiter is never triggered, and 
then in a single step a substantial amount of iron-group elements 
can be be generated. This can have non-trivial effects on the 
total amount of iron-group elements generated over the course of
the simulation. We have found that the addition of the 
limiter based on changes in species functionally precludes this.

With the timestep limited the way we advocate in this paper, 
the timesteps are generally short enough so that the errors 
due to splitting are small. Other approaches to the coupling 
between reactions and hydrodynamics have been proposed in the 
broader literature, especially iterative methods such as 
deferred corrections that allow each of these operators to 
feel the lagged effects of the other operators. For example,
in the context of low Mach number flows, \cite{nonaka:2012} have
used the method of spectral deferred corrections \citep{SDC} to
couple their advection-diffusion-reaction equation set. In our
context this would involve using the advection as a source term
for the burning network, and symmetrically including a source
term to the hydrodynamics corresponding to the energy release
from the burning. We are presently investigating such a method,
and it may form the basis of further work on this subject.

Now we return to a point we hinted at above. The timestep
will only actually satisfy the energy criterion
$\Delta t \leq f_e \tau_e$ and species criterion
$\Delta t \leq f_{X_k} \tau_{X_k}$ when the estimates for
$\dot{e}$ and $\dot{X_k}$ we generate are at least as large
as the actual rate of change of energy and mass fractions
over the timestep. However, this can assumption can fail
during periods of runaway burning when the rate of change
of these quantities is highly nonlinear. We may not want
to neglect the errors caused by this approximation
because they may build up over an extended period of nonlinear
evolution and perhaps substantially change the final results.
To this end, we have implemented a timestep retry option in
\castro, which re-computes an advance if it violated the
stability criteria as judged from the end of the timestep.
However we have found that the benefits for this problem are
small and thus we do not use it for the simulations here.

One other point worth noting for the coupling of the reactions
and the hydrodynamics relates to the dual-energy formalism used
by \castro\ and many other hydrodynamics codes \cite{bryan:1995}.
\castro\ evolves separately equations for the total energy $(\rho E)$ and
for the internal energy $(\rho e)$. The former is conservative while the latter
is not, so for accurate hydrodynamics we prefer to use the total
energy when possible and to use the evolved internal energy variable
only in situations where the kinetic energy dominates the contribution
to the total energy. Indeed, for the cosmological purpose for which
this was originally developed, \cite{ENZO} choose a value $\eta_2 = 0.1$
so that $e$ is only updated to be equal to $E - \mathbf{v}^2/2$ if
$e > \eta_2\, E$. In other cases the evolved value of the internal
energy variable is preserved. However, this choice does not work
for our application, because the ultimate cause of the nuclear
burning in a white dwarf collision is the rapid conversion of
kinetic energy from the white dwarf bulk motion to thermal energy
as the white dwarfs slam into each other. Keeping $\eta_2$ large
prevents this from happening, and consequently the temperature
will never reach values high enough to generate significant
amounts of nickel. For this paper we choose $\eta_2 = 10^{-4}$,
which is low enough for the correct conversion of kinetic energy
but not so low that we need to be concerned about roundoff issues
caused by subtracting kinetic from total energy. This is also
the value that was used by \cite{hawley:2012}.

\subsection{Numerically Unstable Burning}

\citet{kushnir:2013} point out that an inappropriate timestep is 
not the only way for the numerical discretization to cause 
severe errors in the burning. Another failure mode is when
the energy injection timescale
$t_e$ is shorter than the sound-crossing time $t_s$ in a zone.
The goal is to ensure that the following condition holds:
\begin{equation}
  t_s \leq f_{s} t_e \label{eq:burning_limiter_2}
\end{equation}
The sound crossing time, $t_s$, is given by $\Delta x / c_s$, 
where $c_s$ is the sound speed and $\Delta x$ is the (minimum) 
zone width. The parameter $f_{s}$ then determines the minimum
ratio of the nuclear energy generation timescale to the 
sound-crossing time. \citet{kushnir:2013} choose $f_{s} = 0.1$ 
for their simulations, and we do too (this parameter can be set 
at runtime in \castro).

\citet{kushnir:2013} implemented this criterion by artificially 
limiting the magnitude of the energy release after a burn. As 
this is not physical, we take a different approach. Since we 
cannot directly control the energy injection timescale, we 
must find a way to alter the sound-crossing timescale. 
We can achieve this by adding levels of refinement in 
regions that do not satisfy \autoref{eq:burning_limiter_2},
which effectively lowers the $\Delta x$ and thus the
sound-crossing time. We keep tagging zones for refinement
based on this criterion until the criterion is satisfied
on the finest level. Since the concern is regions that 
may detonate, we also tag nearby zones in a buffer region
which do not themselves satisfy the criterion,
so that a detonation in a single timestep cannot 
escape into non-refined regions. The width of the buffer 
region should thus be at least as large as the number of 
timesteps before a regridding procedure is performed.
We choose a value of two for both the number of zones in the 
buffer region and the number of steps in between regrids,
for all simulations in this paper.

While this approach may add several AMR levels 
to a simulation, we agree with \citet{kushnir:2013} that 
solving this numerical instability is crucial to avoiding
early, unphysical detonations, and so it is unavoidable
if a correct evaluation of the burning phase is desired.
A simulation that does not solve this problem in this way
(or some analogous manner) will not obtain the correct amount 
of burning, and will not converge properly with resolution. 
Fortunately, the regions where this criterion is not satisfied 
are fairly localized on the grid, so the amount of additional 
computational work is not insurmountable for a single simulation.
However, this level of accuracy may make a parameter study of
the problem challenging given current computational resources.
When resources are limited, one can still use this method and set a
limit on the number of levels of refinement possible in the simulation,
so that the refinement will still go in the right places but may not
refine all of the way to the regime of guaranteed stability. 



%==========================================================================
% Problem Setup
%==========================================================================
\section{Problem Setup}
\label{sec:problemsetup}

We implement the white dwarf collision problem in \castro\ using our \wdmerger\
package. The white dwarf centers of mass are separated by a distance of four times
the (secondary) white dwarf radius. Their initial velocity is that of
two point-masses in free-fall towards each other, at the appropriate distance.


%==========================================================================
% 2D simulations
%==========================================================================
\section{Two-Dimensional Simulations}
\label{sec:2D}

Though the main focus of this work is fully three-dimensional simulations,
there is much to be learned from simpler, and much computationally cheaper,
two-dimensional axisymmetric collision simulations. These are implemented
in a cylindrical ($R-z$) coordinate system. We use these for three purposes.
First, in the regime where the 2D and 3D simulations can be usefully compared, 
zero-impact parameter simulations, to understand to what extent the
reduced-dimensionality approach can reproduce the 3D results and thus to
conclude whether 2D simulations can be used for obtaining scientific data
such as nucleosynthesis yields. Second, for testing the effect of various
code choices such as the number of isotopes in the nuclear network. A third
effect which comes for free with the others is that we can benchmark existing
and new code tools such as our nuclear postprocessing analysis. Prepared
with these choices and a better understanding of how reliable our approximations
are, we can then proceed to performing 3D simulations with our code choices
already made and with confidence in our analysis software.

\subsection{Dynamics}
\label{sec:2D:dynamics}

First we briefly consider whether the dynamical evolution of the system
is consistent with simple physics we understand. We ask two questions.
First, is the time it takes for the white dwarfs to collide consistent with
basic kinematics. Second, is the amount of energy released in the
collision by the nuclear reactions comparable to the amount of kinetic
energy we infer from observations of Type Ia supernovae?

\subsection{Timestep Limiting}
\label{sec:2D:timestep}

Here we discuss the effect of limiting the hydrodynamic timestep based on
changes in the internal energy in the nuclear reaction network.

\subsection{Nuclear Network}
\label{sec:2D:network}

Now we move to a discussion of the effect of including various isotopes
in the nuclear reaction network. In particular we compare the \aproxthirteen,
\aproxnineteen, and \aproxtwentyone reaction networks.

\subsection{Nucleosynthetic Postprocessing}
\label{sec:2D:postprocessing}

We consider also the results of our postprocessing step for nucleosynthesis
yields, to understand whether the results for the isotopes not included in
our networks make sense.



%==========================================================================
% 3D simulations
%==========================================================================
\section{Three-Dimensional Simulations}
\label{sec:3D}

Now that we have analyzed the various parts of our simulation software and
discussed our code choices, we consider full 3D simulations to understand
the potential for white dwarf collisions to be progenitors of Type Ia
supernovae. We focus on two main quantities of interest: the total amount of
nickel produced in the explosion, which directly powers the light curve
observed in SNe Ia; and, the gravitational wave signature produced by
these events.


%==========================================================================
% Conclusions
%==========================================================================
\section{Conclusions and Discussion}\label{Sec:Conclusions and Discussion}
\label{sec:conclusion}


\acknowledgments

This research was supported by NSF award AST-1211563 and DOE/Office of
Nuclear Physics grant DE-FG02-87ER40317 to Stony Brook. An award of
computer time was provided by the Innovative and Novel Computational
Impact on Theory and Experiment (INCITE) program.  This research used
resources of the Oak Ridge Leadership Computing Facility located in
the Oak Ridge National Laboratory, which is supported by the Office of
Science of the Department of Energy under Contract
DE-AC05-00OR22725. Project AST106 supported use of the ORNL/Titan
resource.  This research used resources of the National Energy
Research Scientific Computing Center, which is supported by the Office
of Science of the U.S. Department of Energy under Contract
No. DE-AC02-05CH11231.  Results in this paper were obtained using the
high-performance LIred computing system at the Institute for Advanced
Computational Science at Stony Brook University, which was obtained
through the Empire State Development grant NYS \#28451.

This research has made use of NASA's Astrophysics Data System 
Bibliographic Services. In addition, this research has made use
of the AstroBetter blog and wiki.

\clearpage

\bibliographystyle{../aasjournal}
\bibliography{../refs}

\end{document}

