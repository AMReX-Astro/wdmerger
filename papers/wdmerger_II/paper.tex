\documentclass[twocolumn,numberedappendix]{../aastex6}

% these lines seem necessary for pdflatex to get the paper size right
\pdfpagewidth 8.5in
\pdfpageheight 11.0in

% for the red MarginPars
\usepackage{color}

% some extra math symbols
\usepackage{mathtools}

% allows Greek symbols to be bold
\usepackage{bm}

% allows us to force the location of a figure
\usepackage{float}

% allows comment sections
\usepackage{verbatim}

% Override choices in \autoref
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

% MarginPars
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{\vskip-\baselineskip\raggedright\tiny\sffamily\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

\newcommand{\msolar}{\mathrm{M}_\odot}

% Software names
\newcommand{\boxlib}{\texttt{BoxLib}}
\newcommand{\castro}{\texttt{CASTRO}}
\newcommand{\microphysics}{\texttt{Microphysics}}
\newcommand{\wdmerger}{\texttt{wdmerger}}
\newcommand{\python}{\texttt{Python}}
\newcommand{\matplotlib}{\texttt{matplotlib}}
\newcommand{\yt}{\texttt{yt}}
\newcommand{\vode}{\texttt{VODE}}
\newcommand{\isoseven}{\texttt{iso7}}
\newcommand{\aproxthirteen}{\texttt{aprox13}}
\newcommand{\aproxnineteen}{\texttt{aprox19}}
\newcommand{\aproxtwentyone}{\texttt{aprox21}}

\begin{document}

%==========================================================================
% Title
%==========================================================================
\title{White Dwarf Mergers on Adaptive Meshes\\ II. Collisions}

\shorttitle{WD Mergers. II. Collisions}
\shortauthors{Katz et al. (2016)}

\author{Max P. Katz\altaffilmark{1}}
\author{Michael Zingale\altaffilmark{1}}
\author{Alan C. Calder\altaffilmark{1,2}}
\author{F. Douglas Swesty\altaffilmark{1}}
\author{Ann S. Almgren\altaffilmark{3}}
\author{Weiqun Zhang\altaffilmark{3}}

\altaffiltext{1}
{
  Department of Physics and Astronomy,
  Stony Brook University, Stony Brook, NY, 11794-3800, USA
}

\altaffiltext{2}
{
  Institute for Advanced Computational Sciences,
  Stony Brook University, Stony Brook, NY, 11794-5250, USA
}

\altaffiltext{3}
{
  Center for Computational Sciences and Engineering,
  Lawrence Berkeley National Laboratory, Berkeley, CA 94720
}

%==========================================================================
% Abstract
%==========================================================================
\begin{abstract}
We consider the collisions of white dwarfs as possible progenitors of Type Ia 
supernovae.

\end{abstract}
\keywords{supernovae: general - white dwarfs}

%==========================================================================
% Introduction
%==========================================================================
\section{Introduction}
\label{sec:introduction}

We consider the collisions of white dwarfs as possible progenitors of Type Ia 
supernovae, using the code \castro\ \cite{castro}.


%==========================================================================
% Numerical Implementation
%==========================================================================
\section{Numerical Methods}
\label{sec:numericalmethods}

The hydrodynamical equations solved in this paper were presented in \citet{wdmergerI} (hereafter, Paper I).
In this paper we focus our discussion on our implementation of the nuclear
reaction network and integrator. There are three main areas of concern: first,
specifying what nuclides are in the network and what reaction rates link the
various nuclides; second, once the system of ODEs governing the evolution of
the nuclear species has been written down, how to couple them to ODEs governing
the evolution of the thermodynamics, in particular the temperature; third,
coupling the nuclear reaction updates to the evolution of the hydrodynamical
system. We will deal with each of these in turn in the following subsections.

\subsection{Nuclear Network}
\label{sec:network}

White dwarfs are mainly composed of $\alpha$-chain particles, primarily ${}^4$He,
${}^{12}$C, ${}^{16}$O, ${}^{20}$Ne, and ${}^{24}$ Mg. Therefore the core of
any network appropriate for modeling nuclear burning in white dwarfs will be
these alpha chain nuclides, with the idea being that links up the $\alpha$-chain
will eventually get us to ${}^{56}$Ni, the nuclide responsible for the
energy output of Type Ia supernovae. In this paper we consider four networks
to do this, presented in order of increasing complexity. The most simple is
\isoseven\ \citep{timmes:2000}, which includes all of the aforementioned isotopes and
${}^{28}$Si (see also \citet{hix:1998}). ${}^{28}$Si effectively measures the
equilibrium state of silicon-group elements, and ${}^{56}$Ni effectively measures
the equilibrium state of iron-group elements, with the link between them governed
by the effective loss or gain of seven $\alpha$-particles. This type of network
was used by \citet{rosswog:2009} for their collision calculations in SPH.

Next is \aproxthirteen\ \citep{timmes:1999,timmes:2000}. This includes
all of the isotopes of \isoseven, and all of the $alpha$-chain particles between
silicon and nickel (${}^{32}$S, ${}^{36}$Ar, ${}^{40}$Ca, ${}^{44}$Ti, ${}^{48}$Cr,
and ${}^{52}$Fe). This network was used by \citet{hawley:2012} and \citet{raskin:2010}.
\citet{loren-aguilar:2010} and \citet{garcia-senz:2013} used a very similar network
that included additionally ${}^{60}$Zn. The \aproxnineteen\ network \citep{timmes:1999}
builds on \aproxthirteen\ by including isotopes for hydrogen burning and explicit
tracking of photodisintegration into ${}^{54}$Fe. This network was used by
\citet{kushnir:2013}, \citet{kushnir:2014}, \citet{rosswog:2009} for their
calculations with FLASH, and \citet{papish:2015}. Finally we will also
consider \aproxtwentyone, which includes all of the above plus ${}^{56}$Cr
and ${}^{56}$Fe and related reaction links. The primary virtue of using
the latter two networks is that they allow us to track changes away from
$Y_e = 0.5$.

All four of these networks have been ported into a form that is consistent
with the \boxlib\ codes, in the freely available \microphysics\ code
repository\footnote{\microphysics\ can be obtained at \url{https://github.com/BoxLib-Codes/Microphysics}.},
a collection of microphysical routines that are designed to be used in our
hydrodynamics codes. These can be easily swapped at compile time by using the 
appropriate makefile variable.

\subsection{Nuclear Burning}
\label{sec:burner}

Given a set of nuclides and the reaction links between them, we now consider
how a burning step is performed in our software. The goal is to integrate the
vector ${\bm{Y}} = (Y_1, Y_2, \ldots, Y_n, e, T)$, where $Y_{n} = X_{n} / A_{n}$
is the molar fraction of species $n$, with $X_n$ the mass fraction and $A_n$ the
mass number of that species, $e$ is the energy released during the burn, and
$T$ is the temperature. The equation describing its evolution is given by
\begin{equation}
  \frac{d\bm{Y}}{dt} = f(\mathbf{Y}),
\end{equation}
where the components of the right-hand-side for the species come from the particular
nuclear burning network we are using. The energy release $e$ of the zone's burn
will change when the nuclear abundances evolve, according to
\begin{equation}
  \frac{\partial e}{\partial t} = N_A \sum_{n} \frac{\partial Y_{n}}{\partial t} m_{n} c^2,
\end{equation}
where $c$ is the speed of light and $m_n$ is the mass of each nuclide.

In a hydrostatic burn, we keep $\rho$ and $T$ fixed throughout, and use
the energy released at the end to compute a final temperature that is
thermodynamically consistent with the new internal energy. By contrast,
in a self-heating burn, we allow the temperature to evolve in response
to the burning (see\footnote{In the cited paper, a term based on the
thermodynamic chemical potential was included; we now believe
that it is incorrect to include such a term in the burn, since it
automatically sums to zero analytically.} \citet{maestro3}):
\begin{equation}
  \frac{dT}{dt} = \frac{1}{c_V}\frac{\partial e}{\partial t}
\end{equation}
(Although $T$ evolves during the burn so that the integration is physically
accurate, as for the hydrostatic method we discard the final value
for $T$ at the end of the burn and recompute a temperature for the zone that is
consistent with its new internal energy.) Here $c_V$ is the specific heat at
constant volume, which is provided by the equation of state.  During this burn,
we can keep $c_V$ constant using its initial value, or at each step we
can choose to re-evaluate the equation of state using the latest value of $(\rho, T)$.
The latter is more expensive but also more accurate, and we use it in this paper.
In practice we find that the cost is negligible in comparison to the more expensive
parts of the calculation, and it can significantly speed up convergence near NSE.
A third option presented by \citet{raskin:2010} is a so-called ``hybrid'' mode.
In this mode, by default we do a hydrostatic burn. If that burn fails, or if the net
energy change is negative, we do the burn again in self-heating mode. A final option
is a burn that limits the changes due to a burn to avoid numerically unstable burning.
This mode is discussed in \autoref{sec:unstable_burning} and which we will call a
``suppressed'' burn for the remainder of this work. All four options
are implemented in our burner software. The simulations shown in this work all
use the self-heating mode unless otherwise specified.

In our \microphysics\ repository we provide several software options for
solving a set of coupled stiff ODEs. For this work we use an implementation of
the well known variable-order Richardson extrapolation method presented by Stoer and
Bulirsch \cite{stoer:1980}, that is similar to the integrator which ships with
the original versions of the networks mentioned above. Previous work using the \boxlib\
codes typically used the classic \vode\ integrator \cite{vode}. We do maintain a version
of the software that is compatible with our software interfaces in the \microphysics\
repository, but we have largely shifted to integrators which are written in modern
Fortran as a consequence of our efforts to run our codes on GPUs. The Stoer and Bulirsch
integrator we provide satisfies this criterion; we also provide a rewrite of the VODE
BDF algorithm that uses modern Fortran features. The Stoer and Bulirsch algorithm
relies on a uniform relative error tolerance for all of the ODEs in the system, which
we set at $10^{-6}$ for all simulations described here.

\subsection{Coupling to the Hydrodynamics}
\label{sec:hydrocoupling}

In \castro, the reactions are coupled to the hydrodynamics using Strang splitting.
In a given timestep advance $\Delta t$, we first evolve the reactions alone through
a time interval $\Delta t / 2$. Then, we evolve the hydrodynamics for $\Delta t$,
and we evolve the reactions again for a further $\Delta t / 2$. The principal
drawback of this approach is that the reactions and the hydrodynamics can become
decoupled from each other. A common solution to this problem presented in
the literature has been to limit the size of the timestep and thereby limit the
extent of this decoupling \citep{raskin:2010,hawley:2012}, which we adopt here 
and have implemented in \castro. Defining the nuclear energy injection timescale 
$\tau_e$, and the species evolution timescale $\tau_{X_k}$,
\begin{align}
  \tau_e &\equiv \frac{e}{|\dot{e}|} \\
  \tau_{X_k} &\equiv \frac{X_k}{|\dot{X_k}|},
\end{align}
where $\dot{e}$ is an estimate of the time rate of change of the internal energy
from nuclear burning, and $\dot{X_k}$ is an estimate of the time rate of change 
of the mass fraction of the species with index $k$, we define burning-limited 
timesteps $\Delta t_{be}$ and $\Delta t_{bX_k}$:
\begin{align}
  \Delta t_{be} &= f_{e}\, \tau_e \\
  \Delta t_{bX_k} &= f_{X}\, \tau_{X_k}.
\end{align}
Given an estimate for $\dot{e}$, the factor $f_{e}$ determines by what 
fraction we would like to allow the internal energy to change
in the current timestep, under the assumption that $\dot{e}$ does not change from
timestep to timestep. Similarly, given an estimate for $\dot{X_k}$, the factor $f_{X}$ 
determines the maximum change in the mass fraction of any species. By making 
$f_{e}$ and $f_{X}$ smaller, we can control the magnitude of the decoupling 
between the reactions and the hydro. We choose by default $f_{X} = f_{e} = 0.1$, 
a choice which is slightly smaller than others in the literature. (Note that previous 
collision papers we are aware of only limited based on changes in internal energy, 
not on cahnges in species.)  The sensitivity of results to this choice will be discussed in 
\autoref{sec:2D:timestep}. The factors $f_{e}$ and $f_{X}$ can be set at runtime in \castro.

At the start of each advance, we limit the size of the timestep to be the smaller
of the minimum hydro timestep (limited by the CFL condition), and the minimum of all the
burning timesteps across all zones. To do this, we need a method for determining 
$\dot{e}$ and $\dot{X_k}$. A typical choice in the literature has been to set, for example,
\begin{equation}
  \dot{e} = \frac{e^{n} - e^{n-1}}{\Delta t^{n-1}}, \label{eq:burning_mode_4}
\end{equation}
where is $e^n$ is the internal energy at the start of the current timestep and
$e^{n-1}$ is the internal energy at the start of the previous timestep. 
The obvious analogue is used for constructing the species rate of change.
However, there are alternative methods of constructing this derivative estimate, 
and we have found that these different methods have measurable consequences.
We define four separate methods for calculating the time derivative, with 
the above being method 4. Method 3 is similar to method 4 but replaces the
denominator in \autoref{eq:burning_mode_4} with the change in 
internal energy over the last timestep only from the nuclear reactions.
Method 2 is the same as method 3 but we only use the change in internal 
energy from the most recent nuclear burning step, that is, the second-half
of the Strang-split burning from the last timestep (the denominator 
then becomes $\Delta t / 2$). In method 1, the most accurate option and 
the current default in \castro, we evaluate the right-hand-side of the 
burning network given the current state to explicitly obtain the 
instantaneous value of $\dot{e}$ and $\dot{X_k}$. 

To understand the consequences of this choice, and more broadly to 
understand the limitations of Strang splitting, we consider the 
basic outline of a single-level advance in an advection-reaction system:
\begin{enumerate}
  \item Evaluate timestep $\Delta t$ for the current advance
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Advance the hydrodynamics by $\Delta t$
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Return to Step 1
\end{enumerate}
Now, consider that during a head-on collision, initial nuclear burning 
will occur at the contact point between the two stars. Because of 
the staggered updates from splitting, the evolution effectively progresses 
as a cycle between burning for $\Delta t$ and getting fresh material 
advected into the contact point by the hydro update for $\Delta t$. 
When the collision begins, $\Delta t$ is controlled by the hydrodynamic 
stability criterion, and may be large enough that it is possible for 
the burning advance in Step 4 to completely burn the freshly advected 
material all the way to NSE. Consequently the evolution is no longer 
a good approximation to smooth burning of the in-falling material but
rather separate discrete burning and hydro steps, and the nature of 
the burning evolution will be quite different. Furthermore, by the 
time we return to Step 1 and estimate the next timestep size, all 
of the burning rates will be small again, and the instantaneous 
timestep limiter of method 1 may actually substantially overestimate 
the needed timestep. The other methods will see that the energy/species  
substantially changed over the last timestep, but will still 
overestimate the needed timestep because the burning was quiescent
for at least some portion of the last advance. In particular, our 
experience has shown that using only the energy change criterion
is particularly susceptible to this phenomenon; silicon-group 
material can build up without changing the internal energy by a 
large fraction, so the timestep limiter is never triggered, and 
then in a single step a substantial amount of iron-group elements 
can be be generated. This can have non-trivial effects on the 
total amount of iron-group elements generated over the course of
the simulation. We have found that the addition of the 
limiter based on changes in species functionally precludes this.

With the timestep limited the way we advocate in this paper, 
the timesteps are generally short enough so that the errors 
due to splitting are small. Other approaches to the coupling 
between reactions and hydrodynamics have been proposed in the 
broader literature, especially iterative methods such as 
deferred corrections that allow each of these operators to 
feel the lagged effects of the other operators. For example,
in the context of low Mach number flows, \cite{nonaka:2012} have
used the method of spectral deferred corrections \citep{SDC} to
couple their advection-diffusion-reaction equation set. In our
context this would involve using the advection as a source term
for the burning network, and symmetrically including a source
term to the hydrodynamics corresponding to the energy release
from the burning. We are presently investigating such a method,
and it may form the basis of further work on this subject.

Now we return to a point we hinted at above. The timestep
will only actually satisfy the energy criterion
$\Delta t \leq f_e \tau_e$ and species criterion
$\Delta t \leq f_{X_k} \tau_{X_k}$ when the estimates for
$\dot{e}$ and $\dot{X_k}$ we generate are at least as large
as the actual rate of change of energy and mass fractions
over the timestep. However, this can assumption can fail
during periods of runaway burning when the rate of change
of these quantities is highly nonlinear. We may not want
to neglect the errors caused by this approximation
because they may build up over an extended period of nonlinear
evolution and perhaps substantially change the final results.
To this end, we have implemented a timestep retry option in
\castro, which re-computes an advance if it violated the
stability criteria as judged from the end of the timestep.
However we have found that the benefits for this problem are
small and thus we do not use it for the simulations here.

One other point worth noting for the coupling of the reactions
and the hydrodynamics relates to the dual-energy formalism used
by \castro\ and many other hydrodynamics codes \cite{bryan:1995}.
\castro\ evolves separately equations for the total energy $(\rho E)$ and
for the internal energy $(\rho e)$. The former is conservative while the latter
is not, so for accurate hydrodynamics we prefer to use the total
energy when possible and to use the evolved internal energy variable
only in situations where the kinetic energy dominates the contribution
to the total energy. Indeed, for the cosmological purpose for which
this was originally developed, \cite{ENZO} choose a value $\eta_2 = 0.1$
so that $e$ is only updated to be equal to $E - \mathbf{v}^2/2$ if
$e > \eta_2\, E$. In other cases the evolved value of the internal
energy variable is preserved. However, this choice does not work
for our application, because the ultimate cause of the nuclear
burning in a white dwarf collision is the rapid conversion of
kinetic energy from the white dwarf bulk motion to thermal energy
as the white dwarfs slam into each other. Keeping $\eta_2$ large
prevents this from happening, and consequently the temperature
will never reach values high enough to generate significant
amounts of nickel. For this paper we choose $\eta_2 = 10^{-4}$,
which is low enough for the correct conversion of kinetic energy
but not so low that we need to be concerned about roundoff issues
caused by subtracting kinetic from total energy. This is also
the value that was used by \cite{hawley:2012}.

\subsection{Numerically Unstable Burning}
\label{sec:unstable_burning}

\citet{kushnir:2013} point out that an inappropriate timestep is 
not the only way for the numerical discretization to cause 
severe errors in the burning. Another failure mode is when
the energy injection timescale
$t_e$ is shorter than the sound-crossing time $t_s$ in a zone.
When the sound-crossing time is too long, energy is built up in
a zone faster than it can be advected away by pressure waves.
This is obviously a problem inherent to numerically discretized
systems as the underlying fluid equations are continuous.
This can lead to a numerically seeded detonation caused by the
temperature building up too quickly in the zone; the detonation
is spurious in this case and should be avoided if possible.
The goal is to ensure that the following condition holds:
\begin{equation}
  t_s \leq f_{s} t_e \label{eq:burning_limiter_2}
\end{equation}
The sound crossing time, $t_s$, is given by $\Delta x / c_s$, 
where $c_s$ is the sound speed and $\Delta x$ is the (minimum) 
zone width. The parameter $f_{s}$ then determines the minimum
ratio of the nuclear energy generation timescale to the 
sound-crossing time. \citet{kushnir:2013} choose $f_{s} = 0.1$ 
for their simulations, and we do too (this parameter can be set 
at runtime in \castro).

\citet{kushnir:2013} implemented this criterion by artificially 
limiting the magnitude of the energy release after a burn. We
too have developed an option for our burner to do this,
the ``suppressed'' burning mode. In a suppressed burn, we limit
the changes to the state so that \autoref{eq:burning_limiter_2}
is always satisfied. To achieve this we directly multiply the
right-hand-side vector in the integration by a constant factor $F$
for all variables, where $F$ is the multiplicative factor needed to
be applied to $\dot{e}$ such that the equality in \autoref{eq:burning_limiter_2}
holds. (If the inequality is already satisfied, then the integration
vector is not modified.) We fix $t_s$ to be the value of the sound
crossing time at the beginning of the burn (that is, we do not
update it as the sound speed changes) and we fix the energy $e$
that goes into the estimate for $t_e$ to be the value of the
internal energy of the zone at the beginning of the burn. If
instead one allowed $c_s$ and $e$ to evolve with the burn, one
would obtain a less conservative limiter in the case of explosive
burning, as $c_s$ and $e$ are both increasing in this case.
As the point of the limiter is to ensure that the changes to the
\textit{original} energy are small enough so that the following
hydrodynamics update can advect away newly generated energy
quickly enough to avoid a nuerically seeded detonation,
we desire the most conservative version of the limiter. We discuss
our results with the suppressed burning mode in \autoref{sec:2D}.

Regardless of whether the suppressed burning mode works for this
particular problem, it is not physical, so in general we have
taken a different approach. If we insist that we cannot
cannot directly control the energy injection timescale, we 
must find a way to alter the sound-crossing timescale. 
We can achieve this by adding levels of refinement in 
regions that do not satisfy \autoref{eq:burning_limiter_2},
which effectively lowers the $\Delta x$ and thus the
sound-crossing time. We keep tagging zones for refinement
based on this criterion until the criterion is satisfied
on the finest level. Since the concern is regions that 
may detonate, we also tag nearby zones in a buffer region
which do not themselves satisfy the criterion,
so that a detonation in a single timestep cannot 
escape into non-refined regions. The width of the buffer 
region should thus be at least as large as the number of 
timesteps before a regridding procedure is performed.
We choose a value of two for both the number of zones in the 
buffer region and the number of steps in between regrids,
for all simulations in this paper.

While this approach may add several AMR levels 
to a simulation, we agree with \citet{kushnir:2013} that 
solving this numerical instability is crucial to avoiding
early, unphysical detonations, and so it is unavoidable
if a correct evaluation of the burning phase is desired.
A simulation that does not solve this problem in this way
(or some analogous manner) will not obtain the correct amount 
of burning, and will not converge properly with resolution. 
Fortunately, the regions where this criterion is not satisfied 
are fairly localized on the grid, so the amount of additional 
computational work is not insurmountable for a single simulation.
However, this level of accuracy may make a parameter study of
the problem challenging given current computational resources.
When resources are limited, one can still use this method and set a
limit on the number of levels of refinement possible in the simulation,
so that the refinement will still go in the right places but may not
refine all of the way to the regime of guaranteed stability. 



%==========================================================================
% Problem Setup
%==========================================================================
\section{Problem Setup}
\label{sec:problemsetup}

We implement the white dwarf collision problem in \castro\ using our \wdmerger\
package. We load the grid in the way most previous papers have done so:
the white dwarf centers of mass are initially separated by a distance of four times
the (secondary) white dwarf radius. Note that the radius of the white dwarf, and
consequently the initial distance, depends on the equation of state used, and this
can vary somewhat depending on the relevant physics used, such as Coulomb corrections
in the Helmholtz equation of state. However, the results do not really depend on the
exact initial distance as long as there is enough time for the WDs to distort in
response to tidal forces as they approach. Their initial velocity is that of
two point-masses in free-fall towards each other coming in from infinity, such that
the contact point is at the origin, and they approach each other on the $x$-axis.
The composition of the WDs is as described in Paper I: low-mass WDs are helium-dominated,
intermediate-mass WDs are carbon and oxygen dominated, and high-mass WDs contain
oxygen, neon, and magnesium. By default the WDs approach each other head-on, with
their respective centers lying on the $x$-axis, but we have also added an option
to have the WDs approach each other at a non-zero impact parameter $b$, where the
offset is perpendicular to the $x$-axis in the $y$ direction. We measure
$b$ in units of the secondary WD's radius, so that the default head-on case
corresponds to $b = 0$ and the case where the two WDs just graze past each other
corresponds to $b = 1$ (in practice it happens slightly differently because
of the WDs responding to each other's gravitational fields).

For all the 3D simulations to follow, we use the same coarse grid as in Paper I:
there are 256 zones per dimension, and the width of each axis is $1.024 \times 10^{10}$
cm, yielding a base resolution of 400 km. When we use adaptive-mesh refinement, we will
use the refinement scheme described in \autoref{sec:unstable_burning}. This is
acceptable because the outcome of the collision problem is chiefly a function
of the propagation of burning front through the WDs, so for this problem one
can take some liberties in other parts of the algorithm for the purpose of
computational efficiency, which can always be turned back on later if a more
accurate answer is desired. So, for example, we do not need to add refinement
everywhere in the WDs in the interest of getting the collision dynamics to be
slightly more accurate. Additionally, we choose to solve the Poisson equation
for the self-gravity of the system only on the coarse grid. The gravitational
potential and acceleration on the finer levels are interpolated from the coarse
grid.

We have also added to \wdmerger\ the ability to use the 2D cylindrical ($R-z$)
coordinate system evolution in \castro. In this coordinate system, we align
the WDs along the $z$-axis (which is analogous to our $x$-axis in the Cartesian
evolution) at the usual distance, with the center of the WDs at $R = 0$. The
un-simulated $\phi$ dimension then would extend the WDs through a $2\pi$ revolution.
The nature of the axisymmetry inherent to the cylindrical coordinate system
means that we can only run head-on collisions with $b = 0$. The coarse grid
is the same resolution as the 3D case: the width of the $z$ axis is $1.024 \times 10^{10}$
cm and the width of the $R$ axis is $5.012 \times 10^{10}$ cm, with twice as
many zones along the $z$ axis as the $R$ axis so that the equal 400 km
resolution is maintained.

The simulation is terminated when the total energy is positive, indicating that
the system has become unbound due to nuclear energy release, and has been
constant or decreasing for the last five timesteps, indicating that no further
meaningful nuclear energy generation is occurring and material is now beginning
to stream off the simulation domain. As all simulations in this paper are of
WD collisions that generate enough energy to unbind the system, this stopping
criterion is applied throughout all simulations shown here.



%==========================================================================
% 2D simulations
%==========================================================================
\section{Two-Dimensional Simulations}
\label{sec:2D}

While white dwarf collisions are three-dimensional events, there is much to
be learned from simpler, and much computationally cheaper, two-dimensional
axisymmetric collision simulations. These are implemented in the cylindrical
coordinate system described in \autoref{sec:problemsetup}.
We simulate collisions in 2D cylindrical coordinates for three purposes.
First, we can quickly get up to speed on the nature of the problem and then
benchmark existing and new code tools such as our nuclear postprocessing analysis.
Second, in the regime where the 2D and 3D simulations can be usefully compared, 
zero-impact parameter head-on simulations ($b = 0$), we can understand to
what extent the reduced-dimensionality approach can reproduce the 3D results
and thus determine whether 2D simulations can be used for obtaining scientific
predictions, including nucleosynthesis yields and observable quantities.
Third, we can cheaply test the effect of various code choices such as the number
of isotopes in the nuclear network; we can afford to run the same problem many
times while tweaking parameters one by one to learn their effects. 
Prepared with these results and a better understanding of how reliable our
approximations are, we can then proceed to performing 3D simulations with
our code choices already made and with confidence that we understand the
uncertainty resulting from these choices.

\subsection{Dynamics}
\label{sec:2D:dynamics}

First we briefly consider whether the dynamical evolution of the system
is consistent with simple physics we understand. We ask two questions.
First, is the time it takes for the white dwarfs to collide consistent with
basic kinematics? Second, is the amount of energy released in the
collision by the nuclear reactions comparable to the amount of kinetic
energy we infer from observations of Type Ia supernovae? We must ensure
that we are getting these basics right before we can advance to more
complicated questions. This is not necessarily a trivial matter. We have
observed, for example, that at low resolution, the WDs take longer to
collide than at higher resolution, because the gravitational field is
much less accurate.

\subsection{Timestep Limiting}
\label{sec:2D:timestep}

Here we discuss the effect of limiting the hydrodynamic timestep based on
changes in the internal energy in the nuclear reaction network.

\subsection{Nuclear Network}
\label{sec:2D:network}

Now we move to a discussion of the effect of including various isotopes
in the nuclear reaction network. In particular we compare the \aproxthirteen,
\aproxnineteen, and \aproxtwentyone reaction networks.

\subsection{Nucleosynthetic Postprocessing}
\label{sec:2D:postprocessing}

We consider also the results of our postprocessing step for nucleosynthesis
yields, to understand whether the results for the isotopes not included in
our networks make sense.



%==========================================================================
% 3D simulations
%==========================================================================
\section{Three-Dimensional Simulations}
\label{sec:3D}

Now that we have analyzed the various parts of our simulation software and
discussed our code choices, we consider full 3D simulations to understand
the potential for white dwarf collisions to be progenitors of Type Ia
supernovae. We focus on two main quantities of interest: the total amount of
nickel produced in the explosion, which directly powers the light curve
observed in SNe Ia; and, the gravitational wave signature produced by
these events.


%==========================================================================
% Conclusions
%==========================================================================
\section{Conclusions and Discussion}\label{Sec:Conclusions and Discussion}
\label{sec:conclusion}


\acknowledgments

This research was supported by NSF award AST-1211563 and DOE/Office of
Nuclear Physics grant DE-FG02-87ER40317 to Stony Brook. An award of
computer time was provided by the Innovative and Novel Computational
Impact on Theory and Experiment (INCITE) program.  This research used
resources of the Oak Ridge Leadership Computing Facility located in
the Oak Ridge National Laboratory, which is supported by the Office of
Science of the Department of Energy under Contract
DE-AC05-00OR22725. Project AST106 supported use of the ORNL/Titan
resource.  This research used resources of the National Energy
Research Scientific Computing Center, which is supported by the Office
of Science of the U.S. Department of Energy under Contract
No. DE-AC02-05CH11231.  Results in this paper were obtained using the
high-performance LIred computing system at the Institute for Advanced
Computational Science at Stony Brook University, which was obtained
through the Empire State Development grant NYS \#28451.

This research has made use of NASA's Astrophysics Data System 
Bibliographic Services. In addition, this research has made use
of the AstroBetter blog and wiki.

\clearpage

\bibliographystyle{../aasjournal}
\bibliography{../refs}

\end{document}

